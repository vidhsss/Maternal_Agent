{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import time\n",
    "import json\n",
    "from typing import List, Dict, Any, Optional, Tuple, Callable\n",
    "import re\n",
    "import requests\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "class MultilingualRAGEvaluator:\n",
    "    \"\"\"\n",
    "    Evaluator for multilingual RAG systems without ground truth.\n",
    "    Supports evaluation in multiple languages for maternal healthcare domain.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        judge_llm: Any = None,\n",
    "        embedding_model: str = \"google/muril-base-cased\",\n",
    "        languages: List[str] = [\"english\", \"hindi\", \"assamese\", \"hinglish\"],\n",
    "        rag_system: Any = None,\n",
    "        non_rag_system: Any = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the evaluator.\n",
    "        \n",
    "        Args:\n",
    "            judge_llm: LLM used for evaluating responses (if None, only automated metrics are used)\n",
    "            embedding_model: Multilingual embedding model for semantic similarity\n",
    "            languages: List of languages to evaluate\n",
    "            rag_system: RAG system to evaluate\n",
    "            non_rag_system: Non-RAG system to compare against\n",
    "        \"\"\"\n",
    "        self.judge_llm = judge_llm\n",
    "        self.languages = languages\n",
    "        self.rag_system = rag_system\n",
    "        self.non_rag_system = non_rag_system\n",
    "        \n",
    "        # Load embedding model for semantic evaluations\n",
    "        print(f\"Loading embedding model: {embedding_model}\")\n",
    "        self.embedder = SentenceTransformer(embedding_model)\n",
    "        \n",
    "        # Define medical keywords for different languages\n",
    "        self.medical_keywords = self._load_medical_keywords()\n",
    "        \n",
    "        # Create metric trackers\n",
    "        self.results = {lang: {} for lang in languages}\n",
    "    \n",
    "    def _load_medical_keywords(self) -> Dict[str, List[str]]:\n",
    "        \"\"\"Load medical keywords for each language.\"\"\"\n",
    "        # In a real implementation, load from a file or database\n",
    "        # This is a minimal example with a few maternal healthcare terms\n",
    "        return {\n",
    "            \"english\": [\n",
    "                \"pregnancy\", \"prenatal\", \"postnatal\", \"birth\", \"labor\", \"delivery\",\n",
    "                \"ultrasound\", \"trimester\", \"fetus\", \"breastfeeding\", \"complication\",\n",
    "                \"eclampsia\", \"caesarean\", \"midwife\", \"obstetrician\", \"maternal\"\n",
    "            ],\n",
    "            \"hindi\": [\n",
    "                \"गर्भावस्था\", \"प्रसवपूर्व\", \"प्रसवोत्तर\", \"जन्म\", \"प्रसव\", \"प्रसव\",\n",
    "                \"अल्ट्रासाउंड\", \"तिमाही\", \"भ्रूण\", \"स्तनपान\", \"जटिलता\",\n",
    "                \"एक्लेम्पसिया\", \"सिजेरियन\", \"दाई\", \"प्रसूति विशेषज्ञ\", \"मातृ\"\n",
    "            ],\n",
    "            \"assamese\": [\n",
    "                \"গর্ভাৱস্থা\", \"প্ৰসৱ পূর্বৱর্তী\", \"প্ৰসৱৰ পিছত\", \"জন্ম\", \"প্ৰসৱ\", \"প্ৰসৱ\",\n",
    "                \"আল্ট্ৰাছাউণ্ড\", \"ত্ৰৈমাসিক\", \"ভ্ৰূণ\", \"স্তন্যপান\", \"জটিলতা\",\n",
    "                \"এক্লেম্পছিয়া\", \"ছিজেৰিয়ান\", \"ধাত্ৰী\", \"প্ৰসূতি বিশেষজ্ঞ\", \"মাতৃ\"\n",
    "            ],\n",
    "            \"hinglish\": [\n",
    "                \"pregnancy\", \"prenatal\", \"postnatal\", \"birth\", \"labor\", \"delivery\",\n",
    "                \"ultrasound\", \"trimester\", \"garbh\", \"breastfeeding\", \"complication\",\n",
    "                \"eclampsia\", \"caesarean\", \"dai\", \"doctor\", \"maternity\"\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    def evaluate_all(\n",
    "        self,\n",
    "        test_queries: Dict[str, List[str]],\n",
    "        rag_answers: Optional[Dict[str, List[str]]] = None,\n",
    "        non_rag_answers: Optional[Dict[str, List[str]]] = None,\n",
    "        retrieval_contexts: Optional[Dict[str, List[str]]] = None\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        Run comprehensive evaluation for all languages.\n",
    "        \n",
    "        Args:\n",
    "            test_queries: Dictionary of queries by language\n",
    "            rag_answers: Pre-generated RAG answers (optional)\n",
    "            non_rag_answers: Pre-generated non-RAG answers (optional)\n",
    "            retrieval_contexts: Retrieved contexts for RAG (optional)\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with evaluation results\n",
    "        \"\"\"\n",
    "        # Generate answers if not provided\n",
    "        if rag_answers is None and self.rag_system is not None:\n",
    "            rag_answers = self._generate_answers(test_queries, self.rag_system, \"rag\")\n",
    "            \n",
    "        if non_rag_answers is None and self.non_rag_system is not None:\n",
    "            non_rag_answers = self._generate_answers(test_queries, self.non_rag_system, \"non_rag\")\n",
    "        \n",
    "        # Get retrieval contexts if not provided\n",
    "        if retrieval_contexts is None and self.rag_system is not None and hasattr(self.rag_system, \"retrieve\"):\n",
    "            retrieval_contexts = self._get_retrieval_contexts(test_queries)\n",
    "        \n",
    "        # Run evaluations for each language\n",
    "        for lang in self.languages:\n",
    "            if lang not in test_queries:\n",
    "                print(f\"Skipping {lang} - no test queries provided\")\n",
    "                continue\n",
    "                \n",
    "            print(f\"Evaluating {lang}...\")\n",
    "            \n",
    "            # Evaluate RAG answers\n",
    "            if rag_answers and lang in rag_answers:\n",
    "                self.results[lang][\"rag\"] = self._evaluate_language_responses(\n",
    "                    queries=test_queries[lang],\n",
    "                    responses=rag_answers[lang],\n",
    "                    system_type=\"rag\",\n",
    "                    language=lang,\n",
    "                    contexts=retrieval_contexts.get(lang) if retrieval_contexts else None\n",
    "                )\n",
    "            \n",
    "            # Evaluate non-RAG answers\n",
    "            if non_rag_answers and lang in non_rag_answers:\n",
    "                self.results[lang][\"non_rag\"] = self._evaluate_language_responses(\n",
    "                    queries=test_queries[lang],\n",
    "                    responses=non_rag_answers[lang],\n",
    "                    system_type=\"non_rag\",\n",
    "                    language=lang\n",
    "                )\n",
    "                \n",
    "            # Comparative analysis if both systems evaluated\n",
    "            if \"rag\" in self.results[lang] and \"non_rag\" in self.results[lang]:\n",
    "                self.results[lang][\"comparative\"] = self._compare_systems(\n",
    "                    rag_results=self.results[lang][\"rag\"],\n",
    "                    non_rag_results=self.results[lang][\"non_rag\"],\n",
    "                    rag_responses=rag_answers[lang],\n",
    "                    non_rag_responses=non_rag_answers[lang],\n",
    "                    queries=test_queries[lang],\n",
    "                    language=lang\n",
    "                )\n",
    "        \n",
    "        # Cross-lingual analysis\n",
    "        if len(self.languages) > 1:\n",
    "            self._cross_lingual_analysis()\n",
    "            \n",
    "        return self.results\n",
    "    \n",
    "    def _generate_answers(self, test_queries, system, system_type):\n",
    "        \"\"\"Generate answers using the specified system.\"\"\"\n",
    "        answers = {}\n",
    "        \n",
    "        for lang in self.languages:\n",
    "            if lang not in test_queries:\n",
    "                continue\n",
    "                \n",
    "            print(f\"Generating {system_type} answers for {lang}...\")\n",
    "            answers[lang] = []\n",
    "            \n",
    "            for query in tqdm(test_queries[lang]):\n",
    "                try:\n",
    "                    if system_type == \"rag\":\n",
    "                        answer = system.answer(query, language=lang)\n",
    "                    else:\n",
    "                        answer = system.generate(query, language=lang)\n",
    "                    answers[lang].append(answer)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error generating answer for '{query}': {e}\")\n",
    "                    answers[lang].append(\"\")\n",
    "        \n",
    "        return answers\n",
    "    \n",
    "    def _get_retrieval_contexts(self, test_queries):\n",
    "        \"\"\"Get retrieval contexts for RAG system.\"\"\"\n",
    "        contexts = {}\n",
    "        \n",
    "        for lang in self.languages:\n",
    "            if lang not in test_queries:\n",
    "                continue\n",
    "                \n",
    "            print(f\"Getting retrieval contexts for {lang}...\")\n",
    "            contexts[lang] = []\n",
    "            \n",
    "            for query in tqdm(test_queries[lang]):\n",
    "                try:\n",
    "                    # This method depends on your RAG system's API\n",
    "                    retrieved_docs = self.rag_system.retrieve(query, language=lang)\n",
    "                    contexts[lang].append(retrieved_docs)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error retrieving context for '{query}': {e}\")\n",
    "                    contexts[lang].append([])\n",
    "        \n",
    "        return contexts\n",
    "    \n",
    "    def _evaluate_language_responses(\n",
    "        self,\n",
    "        queries: List[str],\n",
    "        responses: List[str],\n",
    "        system_type: str,\n",
    "        language: str,\n",
    "        contexts: Optional[List[List[str]]] = None\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        Evaluate responses for a specific language.\n",
    "        \n",
    "        Args:\n",
    "            queries: List of queries\n",
    "            responses: List of responses\n",
    "            system_type: 'rag' or 'non_rag'\n",
    "            language: Language being evaluated\n",
    "            contexts: Retrieved contexts (for RAG only)\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with evaluation metrics\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        # 1. Evaluate uncertainty handling\n",
    "        results[\"uncertainty\"] = self._evaluate_uncertainty(responses, language)\n",
    "        \n",
    "        # 2. Medical terminology usage\n",
    "        results[\"medical_terminology\"] = self._evaluate_medical_terminology(responses, language)\n",
    "        \n",
    "        # 3. Response length and detail\n",
    "        results[\"response_detail\"] = self._evaluate_response_detail(responses)\n",
    "        \n",
    "        # 4. Self-consistency\n",
    "        results[\"self_consistency\"] = self._evaluate_self_consistency(responses)\n",
    "        \n",
    "        # 5. Query-response relevance\n",
    "        results[\"query_relevance\"] = self._evaluate_query_relevance(queries, responses)\n",
    "        \n",
    "        # 6. For RAG systems, evaluate context utilization\n",
    "        if system_type == \"rag\" and contexts:\n",
    "            results[\"context_utilization\"] = self._evaluate_context_utilization(responses, contexts)\n",
    "            results[\"attribution\"] = self._evaluate_attribution(responses)\n",
    "        \n",
    "        # 7. LLM judge evaluation if available\n",
    "        if self.judge_llm:\n",
    "            results[\"llm_evaluation\"] = self._llm_judge_evaluation(\n",
    "                queries, responses, language, system_type, contexts\n",
    "            )\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _evaluate_uncertainty(self, responses: List[str], language: str) -> Dict:\n",
    "        \"\"\"Evaluate appropriate expression of uncertainty in responses.\"\"\"\n",
    "        uncertainty_phrases = {\n",
    "            \"english\": [\"may\", \"might\", \"possible\", \"suggest\", \"consider\", \"consult\", \"uncertain\", \"not clear\", \"limited information\"],\n",
    "            \"hindi\": [\"शायद\", \"हो सकता है\", \"संभव\", \"सुझाव\", \"विचार\", \"परामर्श\", \"अनिश्चित\", \"स्पष्ट नहीं\", \"सीमित जानकारी\"],\n",
    "            \"assamese\": [\"হয়তো\", \"সম্ভৱ\", \"পৰামৰ্শ\", \"বিবেচনা\", \"অনিশ্চিত\", \"স্পষ্ট নহয়\", \"সীমিত তথ্য\"],\n",
    "            \"hinglish\": [\"may\", \"might\", \"ho sakta hai\", \"possible\", \"suggest\", \"consider\", \"consult\", \"uncertain\", \"not clear\", \"limited information\"]\n",
    "        }\n",
    "        \n",
    "        lang_phrases = uncertainty_phrases.get(language, uncertainty_phrases[\"english\"])\n",
    "        \n",
    "        scores = []\n",
    "        for response in responses:\n",
    "            # Count uncertainty phrases\n",
    "            count = sum(1 for phrase in lang_phrases if phrase.lower() in response.lower())\n",
    "            \n",
    "            # Normalize by response length\n",
    "            norm_count = count / max(1, len(response.split()))\n",
    "            \n",
    "            # We want some uncertainty but not too much\n",
    "            # Score is highest at appropriate levels (e.g., 1-3 phrases in a medium response)\n",
    "            if count == 0:\n",
    "                # No uncertainty expressions at all\n",
    "                score = 0.0\n",
    "            elif norm_count > 0.1:\n",
    "                # Too much uncertainty\n",
    "                score = 0.5\n",
    "            else:\n",
    "                # Appropriate level of uncertainty\n",
    "                score = 1.0\n",
    "                \n",
    "            scores.append(score)\n",
    "            \n",
    "        return {\n",
    "            \"scores\": scores,\n",
    "            \"average\": np.mean(scores),\n",
    "            \"appropriate_uncertainty_rate\": sum(1 for s in scores if s > 0.7) / len(scores)\n",
    "        }\n",
    "    \n",
    "    def _evaluate_medical_terminology(self, responses: List[str], language: str) -> Dict:\n",
    "        \"\"\"Evaluate use of appropriate medical terminology for maternal healthcare.\"\"\"\n",
    "        # Get medical keywords for this language\n",
    "        keywords = self.medical_keywords.get(language, [])\n",
    "        \n",
    "        scores = []\n",
    "        keyword_counts = []\n",
    "        \n",
    "        for response in responses:\n",
    "            response_lower = response.lower()\n",
    "            \n",
    "            # Count medical keywords\n",
    "            count = sum(1 for keyword in keywords if keyword.lower() in response_lower)\n",
    "            keyword_counts.append(count)\n",
    "            \n",
    "            # Normalize by response length\n",
    "            words = len(response.split())\n",
    "            norm_count = count / max(1, words / 50)  # Expect ~1 keyword per 50 words\n",
    "            \n",
    "            # Score based on normalized count (more is better up to a point)\n",
    "            score = min(1.0, norm_count)\n",
    "            scores.append(score)\n",
    "            \n",
    "        return {\n",
    "            \"scores\": scores,\n",
    "            \"average\": np.mean(scores),\n",
    "            \"keyword_density\": np.mean([count / max(1, len(resp.split())) for count, resp in zip(keyword_counts, responses)])\n",
    "        }\n",
    "    \n",
    "    def _evaluate_response_detail(self, responses: List[str]) -> Dict:\n",
    "        \"\"\"Evaluate response length and detail level.\"\"\"\n",
    "        # Get word counts\n",
    "        word_counts = [len(response.split()) for response in responses]\n",
    "        \n",
    "        # Score based on word count (more detailed is better, up to a point)\n",
    "        detail_scores = []\n",
    "        for count in word_counts:\n",
    "            if count < 30:\n",
    "                # Too brief\n",
    "                score = count / 30\n",
    "            elif count > 500:\n",
    "                # Excessively long\n",
    "                score = 500 / count\n",
    "            else:\n",
    "                # Appropriate length\n",
    "                score = 1.0\n",
    "                \n",
    "            detail_scores.append(score)\n",
    "            \n",
    "        return {\n",
    "            \"word_counts\": word_counts,\n",
    "            \"average_words\": np.mean(word_counts),\n",
    "            \"detail_scores\": detail_scores,\n",
    "            \"average_detail_score\": np.mean(detail_scores)\n",
    "        }\n",
    "    \n",
    "    def _evaluate_self_consistency(self, responses: List[str]) -> Dict:\n",
    "        \"\"\"Evaluate consistency between different responses.\"\"\"\n",
    "        if len(responses) < 2:\n",
    "            return {\"average_similarity\": 0.0, \"consistency_score\": 0.0}\n",
    "            \n",
    "        # Generate embeddings for all responses\n",
    "        embeddings = self.embedder.encode(responses)\n",
    "        \n",
    "        # Calculate pairwise cosine similarities\n",
    "        similarities = cosine_similarity(embeddings)\n",
    "        \n",
    "        # Get average similarity (excluding self-similarity)\n",
    "        np.fill_diagonal(similarities, 0)\n",
    "        avg_similarity = similarities.sum() / (similarities.shape[0] * (similarities.shape[0] - 1))\n",
    "        \n",
    "        # Convert to a score (too low or too high similarity is bad)\n",
    "        if avg_similarity < 0.3:\n",
    "            # Too inconsistent\n",
    "            consistency_score = avg_similarity / 0.3\n",
    "        elif avg_similarity > 0.9:\n",
    "            # Too similar (might be repetitive/templated)\n",
    "            consistency_score = 1 - (avg_similarity - 0.9) * 10\n",
    "        else:\n",
    "            # Healthy diversity with consistency\n",
    "            consistency_score = 1.0\n",
    "            \n",
    "        return {\n",
    "            \"average_similarity\": float(avg_similarity),\n",
    "            \"consistency_score\": float(consistency_score)\n",
    "        }\n",
    "    \n",
    "    def _evaluate_query_relevance(self, queries: List[str], responses: List[str]) -> Dict:\n",
    "        \"\"\"Evaluate semantic relevance of responses to queries.\"\"\"\n",
    "        if len(queries) != len(responses):\n",
    "            raise ValueError(\"Number of queries and responses must match\")\n",
    "            \n",
    "        relevance_scores = []\n",
    "        \n",
    "        for query, response in zip(queries, responses):\n",
    "            # Get embeddings\n",
    "            query_embedding = self.embedder.encode([query])[0]\n",
    "            response_embedding = self.embedder.encode([response])[0]\n",
    "            \n",
    "            # Calculate cosine similarity\n",
    "            similarity = cosine_similarity([query_embedding], [response_embedding])[0][0]\n",
    "            \n",
    "            # Convert to a score (higher is better)\n",
    "            relevance_scores.append(float(similarity))\n",
    "            \n",
    "        return {\n",
    "            \"relevance_scores\": relevance_scores,\n",
    "            \"average_relevance\": float(np.mean(relevance_scores))\n",
    "        }\n",
    "    \n",
    "    def _evaluate_context_utilization(self, responses: List[str], contexts: List[List[str]]) -> Dict:\n",
    "        \"\"\"Evaluate how well responses utilize retrieved contexts.\"\"\"\n",
    "        if len(responses) != len(contexts):\n",
    "            raise ValueError(\"Number of responses and contexts must match\")\n",
    "            \n",
    "        utilization_scores = []\n",
    "        \n",
    "        for response, context_list in zip(responses, contexts):\n",
    "            if not context_list:\n",
    "                utilization_scores.append(0.0)\n",
    "                continue\n",
    "                \n",
    "            # Combine all context documents\n",
    "            combined_context = \" \".join(context_list)\n",
    "            \n",
    "            # Get embeddings\n",
    "            context_embedding = self.embedder.encode([combined_context])[0]\n",
    "            response_embedding = self.embedder.encode([response])[0]\n",
    "            \n",
    "            # Calculate cosine similarity\n",
    "            similarity = cosine_similarity([context_embedding], [response_embedding])[0][0]\n",
    "            \n",
    "            # Content overlap analysis (more sophisticated than embedding similarity)\n",
    "            # Check if significant n-grams from context appear in response\n",
    "            context_words = set(combined_context.lower().split())\n",
    "            response_words = set(response.lower().split())\n",
    "            word_overlap = len(context_words.intersection(response_words)) / max(1, len(context_words))\n",
    "            \n",
    "            # Combine metrics (embedding similarity and word overlap)\n",
    "            utilization_score = (similarity + word_overlap) / 2\n",
    "            utilization_scores.append(float(utilization_score))\n",
    "            \n",
    "        return {\n",
    "            \"utilization_scores\": utilization_scores,\n",
    "            \"average_utilization\": float(np.mean(utilization_scores))\n",
    "        }\n",
    "    \n",
    "    def _evaluate_attribution(self, responses: List[str]) -> Dict:\n",
    "        \"\"\"Evaluate whether responses include source attributions.\"\"\"\n",
    "        attribution_patterns = [\n",
    "            r\"according to\",\n",
    "            r\"based on\",\n",
    "            r\"sources? (indicate|say|state)\",\n",
    "            r\"research (shows|indicates)\",\n",
    "            r\"study|studies\",\n",
    "            r\"guidelines\",\n",
    "            r\"reference\",\n",
    "            r\"cited\",\n",
    "            r\"source:\"\n",
    "        ]\n",
    "        \n",
    "        combined_pattern = \"|\".join(attribution_patterns)\n",
    "        \n",
    "        attribution_scores = []\n",
    "        for response in responses:\n",
    "            # Check for attribution phrases\n",
    "            matches = re.findall(combined_pattern, response.lower())\n",
    "            has_attribution = len(matches) > 0\n",
    "            \n",
    "            attribution_scores.append(1.0 if has_attribution else 0.0)\n",
    "            \n",
    "        return {\n",
    "            \"attribution_scores\": attribution_scores,\n",
    "            \"attribution_rate\": float(np.mean(attribution_scores))\n",
    "        }\n",
    "    \n",
    "    def _llm_judge_evaluation(\n",
    "        self,\n",
    "        queries: List[str],\n",
    "        responses: List[str],\n",
    "        language: str,\n",
    "        system_type: str,\n",
    "        contexts: Optional[List[List[str]]] = None\n",
    "    ) -> Dict:\n",
    "        \"\"\"Use an LLM to evaluate responses on various dimensions.\"\"\"\n",
    "        if not self.judge_llm:\n",
    "            return {}\n",
    "            \n",
    "        evaluation_prompts = {\n",
    "            \"english\": \"\"\"\n",
    "            You are a maternal healthcare expert evaluating an AI response to a user query.\n",
    "            \n",
    "            User Query: {query}\n",
    "            \n",
    "            AI Response: {response}\n",
    "            \n",
    "            Please evaluate this response on the following criteria (score 1-5, where 5 is best):\n",
    "            \n",
    "            1. Medical Accuracy: Is the medical information correct and up-to-date?\n",
    "            2. Comprehensiveness: Does it address all aspects of the query?\n",
    "            3. Clarity: Is the information presented clearly?\n",
    "            4. Safety: Does it include appropriate cautions and avoid harmful advice?\n",
    "            5. Cultural Sensitivity: Is it respectful of cultural practices around maternal health?\n",
    "            \n",
    "            For each score, provide a brief justification.\n",
    "            \n",
    "            Output your evaluation in JSON format:\n",
    "            {{\n",
    "                \"medical_accuracy\": {{\n",
    "                    \"score\": <score>,\n",
    "                    \"justification\": \"<brief justification>\"\n",
    "                }},\n",
    "                \"comprehensiveness\": {{\n",
    "                    \"score\": <score>,\n",
    "                    \"justification\": \"<brief justification>\"\n",
    "                }},\n",
    "                \"clarity\": {{\n",
    "                    \"score\": <score>,\n",
    "                    \"justification\": \"<brief justification>\"\n",
    "                }},\n",
    "                \"safety\": {{\n",
    "                    \"score\": <score>,\n",
    "                    \"justification\": \"<brief justification>\"\n",
    "                }},\n",
    "                \"cultural_sensitivity\": {{\n",
    "                    \"score\": <score>,\n",
    "                    \"justification\": \"<brief justification>\"\n",
    "                }},\n",
    "                \"overall_score\": <average of all scores>\n",
    "            }}\n",
    "            \"\"\"\n",
    "        }\n",
    "        \n",
    "        # Use English prompt for all languages to simplify (customize as needed)\n",
    "        prompt_template = evaluation_prompts[\"english\"]\n",
    "        \n",
    "        results = []\n",
    "        for i, (query, response) in enumerate(zip(queries, responses)):\n",
    "            # Add context information if available (for RAG)\n",
    "            context_str = \"\"\n",
    "            if contexts and i < len(contexts) and contexts[i]:\n",
    "                context_str = \"\\n\\nRetrieved Context: \" + \"\\n\".join(contexts[i])\n",
    "            \n",
    "            # Format the prompt\n",
    "            prompt = prompt_template.format(\n",
    "                query=query,\n",
    "                response=response,\n",
    "                context=context_str\n",
    "            )\n",
    "            \n",
    "            try:\n",
    "                # Get evaluation from judge LLM\n",
    "                judge_response = self.judge_llm.generate(prompt)\n",
    "                \n",
    "                # Parse JSON response\n",
    "                try:\n",
    "                    evaluation = json.loads(judge_response)\n",
    "                    results.append(evaluation)\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Failed to parse judge response as JSON: {judge_response[:100]}...\")\n",
    "                    results.append({\n",
    "                        \"error\": \"Failed to parse response\",\n",
    "                        \"overall_score\": 0\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                print(f\"Error getting evaluation from judge LLM: {e}\")\n",
    "                results.append({\n",
    "                    \"error\": str(e),\n",
    "                    \"overall_score\": 0\n",
    "                })\n",
    "        \n",
    "        # Aggregate results\n",
    "        aggregated = {\n",
    "            \"evaluations\": results,\n",
    "            \"average_scores\": {}\n",
    "        }\n",
    "        \n",
    "        # Calculate average scores for each dimension\n",
    "        dimensions = [\"medical_accuracy\", \"comprehensiveness\", \"clarity\", \"safety\", \"cultural_sensitivity\", \"overall_score\"]\n",
    "        for dim in dimensions:\n",
    "            scores = [r.get(dim, {}).get(\"score\", 0) if isinstance(r.get(dim), dict) else r.get(dim, 0) for r in results]\n",
    "            aggregated[\"average_scores\"][dim] = float(np.mean([s for s in scores if s > 0]))\n",
    "        \n",
    "        return aggregated\n",
    "    \n",
    "    def _compare_systems(\n",
    "        self,\n",
    "        rag_results: Dict,\n",
    "        non_rag_results: Dict,\n",
    "        rag_responses: List[str],\n",
    "        non_rag_responses: List[str],\n",
    "        queries: List[str],\n",
    "        language: str\n",
    "    ) -> Dict:\n",
    "        \"\"\"Compare RAG and non-RAG systems.\"\"\"\n",
    "        if len(rag_responses) != len(non_rag_responses):\n",
    "            raise ValueError(\"Number of RAG and non-RAG responses must match\")\n",
    "            \n",
    "        # Direct comparison of metrics\n",
    "        metric_comparison = {}\n",
    "        for metric in rag_results:\n",
    "            if metric in non_rag_results and \"average\" in rag_results[metric] and \"average\" in non_rag_results[metric]:\n",
    "                rag_score = rag_results[metric][\"average\"]\n",
    "                non_rag_score = non_rag_results[metric][\"average\"]\n",
    "                \n",
    "                metric_comparison[metric] = {\n",
    "                    \"rag_score\": rag_score,\n",
    "                    \"non_rag_score\": non_rag_score,\n",
    "                    \"difference\": rag_score - non_rag_score,\n",
    "                    \"winner\": \"rag\" if rag_score > non_rag_score else \"non_rag\"\n",
    "                }\n",
    "        \n",
    "        # Response similarity analysis\n",
    "        similarities = []\n",
    "        for rag, non_rag in zip(rag_responses, non_rag_responses):\n",
    "            rag_embedding = self.embedder.encode([rag])[0]\n",
    "            non_rag_embedding = self.embedder.encode([non_rag])[0]\n",
    "            \n",
    "            similarity = cosine_similarity([rag_embedding], [non_rag_embedding])[0][0]\n",
    "            similarities.append(float(similarity))\n",
    "        \n",
    "        response_comparison = {\n",
    "            \"response_similarities\": similarities,\n",
    "            \"average_similarity\": float(np.mean(similarities)),\n",
    "            \"highly_similar_responses\": sum(1 for s in similarities if s > 0.9),\n",
    "            \"highly_different_responses\": sum(1 for s in similarities if s < 0.5)\n",
    "        }\n",
    "        \n",
    "        # If LLM judge was used, compare those scores directly\n",
    "        llm_eval_comparison = {}\n",
    "        if \"llm_evaluation\" in rag_results and \"llm_evaluation\" in non_rag_results:\n",
    "            rag_scores = rag_results[\"llm_evaluation\"][\"average_scores\"]\n",
    "            non_rag_scores = non_rag_results[\"llm_evaluation\"][\"average_scores\"]\n",
    "            \n",
    "            for dimension in rag_scores:\n",
    "                if dimension in non_rag_scores:\n",
    "                    llm_eval_comparison[dimension] = {\n",
    "                        \"rag_score\": rag_scores[dimension],\n",
    "                        \"non_rag_score\": non_rag_scores[dimension],\n",
    "                        \"difference\": rag_scores[dimension] - non_rag_scores[dimension],\n",
    "                        \"winner\": \"rag\" if rag_scores[dimension] > non_rag_scores[dimension] else \"non_rag\"\n",
    "                    }\n",
    "        \n",
    "        return {\n",
    "            \"metric_comparison\": metric_comparison,\n",
    "            \"response_comparison\": response_comparison,\n",
    "            \"llm_eval_comparison\": llm_eval_comparison\n",
    "        }\n",
    "    \n",
    "    def _cross_lingual_analysis(self) -> None:\n",
    "        \"\"\"Analyze performance across languages.\"\"\"\n",
    "        cross_lingual = {\n",
    "            \"rag\": {},\n",
    "            \"non_rag\": {}\n",
    "        }\n",
    "        \n",
    "        for system_type in [\"rag\", \"non_rag\"]:\n",
    "            # Collect metrics across languages\n",
    "            metrics_by_language = {}\n",
    "            \n",
    "            for lang in self.languages:\n",
    "                if system_type not in self.results[lang]:\n",
    "                    continue\n",
    "                    \n",
    "                system_results = self.results[lang][system_type]\n",
    "                \n",
    "                # Extract key metrics\n",
    "                lang_metrics = {}\n",
    "                for metric, value in system_results.items():\n",
    "                    if isinstance(value, dict) and \"average\" in value:\n",
    "                        lang_metrics[metric] = value[\"average\"]\n",
    "                    elif isinstance(value, dict) and \"average_scores\" in value:\n",
    "                        # LLM evaluation\n",
    "                        for dim, score in value[\"average_scores\"].items():\n",
    "                            lang_metrics[f\"llm_{dim}\"] = score\n",
    "                \n",
    "                metrics_by_language[lang] = lang_metrics\n",
    "            \n",
    "            # Compare metrics across languages\n",
    "            if metrics_by_language:\n",
    "                for metric in list(metrics_by_language.values())[0].keys():\n",
    "                    metric_values = {\n",
    "                        lang: metrics[metric]\n",
    "                        for lang, metrics in metrics_by_language.items()\n",
    "                        if metric in metrics\n",
    "                    }\n",
    "                    \n",
    "                    if metric_values:\n",
    "                        # Find best and worst performing languages\n",
    "                        best_lang = max(metric_values.items(), key=lambda x: x[1])[0]\n",
    "                        worst_lang = min(metric_values.items(), key=lambda x: x[1])[0]\n",
    "                        \n",
    "                        cross_lingual[system_type][metric] = {\n",
    "                            \"values\": metric_values,\n",
    "                            \"best_language\": best_lang,\n",
    "                            \"worst_language\": worst_lang,\n",
    "                            \"range\": max(metric_values.values()) - min(metric_values.values())\n",
    "                        }\n",
    "        \n",
    "        self.results[\"cross_lingual\"] = cross_lingual\n",
    "    \n",
    "    def visualize_results(self, output_path: Optional[str] = None) -> None:\n",
    "        \"\"\"\n",
    "        Visualize evaluation results.\n",
    "        \n",
    "        Args:\n",
    "            output_path: Path to save visualizations (optional)\n",
    "        \"\"\"\n",
    "        if not self.results:\n",
    "            print(\"No results to visualize. Run evaluate_all() first.\")\n",
    "            return\n",
    "            \n",
    "        # Set up figure style\n",
    "        plt.style.use('seaborn-v0_8-whitegrid')\n",
    "        sns.set_palette(\"colorblind\")\n",
    "        \n",
    "        # 1. Comparative metrics between RAG and non-RAG for each language\n",
    "        for lang in self.languages:\n",
    "            if \"comparative\" not in self.results[lang]:\n",
    "                continue\n",
    "                \n",
    "            comparative = self.results[lang][\"comparative\"][\"metric_comparison\"]\n",
    "            \n",
    "            # Extract metrics and scores\n",
    "            metrics = []\n",
    "            rag_scores = []\n",
    "            non_rag_scores = []\n",
    "            \n",
    "            for metric, values in comparative.items():\n",
    "                metrics.append(metric)\n",
    "                rag_scores.append(values[\"rag_score\"])\n",
    "                non_rag_scores.append(values[\"non_rag_score\"])\n",
    "            \n",
    "            if not metrics:\n",
    "                continue\n",
    "                \n",
    "            # Create figure\n",
    "            fig, ax = plt.subplots(figsize=(12, 6))\n",
    "            x = np.arange(len(metrics))\n",
    "            width = 0.35\n",
    "            \n",
    "            ax.bar(x - width/2, rag_scores, width, label='RAG')\n",
    "            ax.bar(x + width/2, non_rag_scores, width, label='Non-RAG')\n",
    "            \n",
    "            ax.set_ylabel('Score')\n",
    "            ax.set_title(f'RAG vs Non-RAG Comparison - {lang.capitalize()}')\n",
    "            ax.set_xticks(x)\n",
    "            ax.set_xticklabels([m.replace('_', ' ').title() for m in metrics])\n",
    "            ax.legend()\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            \n",
    "            if output_path:\n",
    "                plt.savefig(f\"{output_path}/rag_vs_nonrag_{lang}.png\")\n",
    "            else:\n",
    "                plt.show()\n",
    "                \n",
    "        # 2. Cross-lingual comparison for RAG\n",
    "        if \"cross_lingual\" in self.results and \"rag\" in self.results[\"cross_lingual\"]:\n",
    "            cross_ling_rag = self.results[\"cross_lingual\"][\"rag\"]\n",
    "            \n",
    "            # Select key metrics for visualization\n",
    "            key_metrics = [\n",
    "                m for m in cross_ling_rag.keys() \n",
    "                if m in ['query_relevance', 'medical_terminology', 'response_detail', 'llm_overall_score']\n",
    "            ]\n",
    "            \n",
    "            for metric in key_metrics:\n",
    "                if metric not in cross_ling_rag:\n",
    "                    continue\n",
    "                    \n",
    "                metric_data = cross_ling_rag[metric]\n",
    "                \n",
    "                # Create figure\n",
    "                fig, ax = plt.subplots(figsize=(10, 6))\n",
    "                \n",
    "                # Extract languages and values\n",
    "                languages = list(metric_data[\"values\"].keys())\n",
    "                values = list(metric_data[\"values\"].values())\n",
    "                \n",
    "                # Plot\n",
    "                bars = ax.bar(languages, values, color=sns.color_palette(\"colorblind\", len(languages)))\n",
    "                \n",
    "                # Highlight best and worst\n",
    "                best_idx = languages.index(metric_data[\"best_language\"])\n",
    "                worst_idx = languages.index(metric_data[\"worst_language\"])\n",
    "                bars[best_idx].set_color('green')\n",
    "                bars[worst_idx].set_color('red')\n",
    "                \n",
    "                ax.set_ylabel('Score')\n",
    "                ax.set_title(f'Cross-Lingual Comparison - {metric.replace(\"_\", \" \").title()}')\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                \n",
    "                if output_path:\n",
    "                    plt.savefig(f\"{output_path}/cross_lingual_{metric}.png\")\n",
    "                else:\n",
    "                    plt.show()\n",
    "        \n",
    "        # 3. Heatmap for all metrics across languages\n",
    "        if len(self.languages) > 1:\n",
    "            for system_type in [\"rag\", \"non_rag\"]:\n",
    "                # Collect all metrics\n",
    "                all_metrics = {}\n",
    "                all_languages = []\n",
    "                \n",
    "                for lang in self.languages:\n",
    "                    if system_type in self.results[lang]:\n",
    "                        all_languages.append(lang)\n",
    "                        \n",
    "                        for metric, values in self.results[lang][system_type].items():\n",
    "                            if isinstance(values, dict) and \"average\" in values:\n",
    "                                if metric not in all_metrics:\n",
    "                                    all_metrics[metric] = []\n",
    "                                all_metrics[metric].append(values[\"average\"])\n",
    "                            elif isinstance(values, dict) and \"average_scores\" in values:\n",
    "                                # LLM evaluation\n",
    "                                for dim, score in values[\"average_scores\"].items():\n",
    "                                    metric_name = f\"llm_{dim}\"\n",
    "                                    if metric_name not in all_metrics:\n",
    "                                        all_metrics[metric_name] = []\n",
    "                                    all_metrics[metric_name].append(score)\n",
    "                \n",
    "                if not all_metrics or not all_languages:\n",
    "                    continue\n",
    "                    \n",
    "                # Prepare data for heatmap\n",
    "                data = []\n",
    "                metric_names = []\n",
    "                \n",
    "                for metric, values in all_metrics.items():\n",
    "                    if len(values) == len(all_languages):\n",
    "                        data.append(values)\n",
    "                        metric_names.append(metric.replace(\"_\", \" \").title())\n",
    "                \n",
    "                if not data:\n",
    "                    continue\n",
    "                    \n",
    "                # Create heatmap\n",
    "                fig, ax = plt.subplots(figsize=(12, 10))\n",
    "                sns.heatmap(data, annot=True, fmt=\".2f\", xticklabels=[l.capitalize() for l in all_languages], \n",
    "                            yticklabels=metric_names, cmap=\"YlGnBu\", ax=ax)\n",
    "                \n",
    "                ax.set_title(f'{system_type.upper()} Performance Across Languages')\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                \n",
    "                if output_path:\n",
    "                    plt.savefig(f\"{output_path}/heatmap_{system_type}.png\")\n",
    "                else:\n",
    "                    plt.show()\n",
    "    \n",
    "    def generate_report(self, output_path: Optional[str] = None) -> str:\n",
    "        \"\"\"\n",
    "        Generate a detailed report of evaluation results.\n",
    "        \n",
    "        Args:\n",
    "            output_path: Path to save report (optional)\n",
    "            \n",
    "        Returns:\n",
    "            Report as string\n",
    "        \"\"\"\n",
    "        if not self.results:\n",
    "            return \"No results available. Run evaluate_all() first.\"\n",
    "            \n",
    "        report = []\n",
    "        report.append(\"# Multilingual Maternal Healthcare RAG Evaluation Report\")\n",
    "        report.append(\"\\n## Overview\")\n",
    "        \n",
    "        # Overall winner summary\n",
    "        winners = {}\n",
    "        for lang in self.languages:\n",
    "            if \"comparative\" in self.results[lang]:\n",
    "                # Count metric winners\n",
    "                rag_wins = 0\n",
    "                non_rag_wins = 0\n",
    "                \n",
    "                for metric, values in self.results[lang][\"comparative\"][\"metric_comparison\"].items():\n",
    "                    if values[\"winner\"] == \"rag\":\n",
    "                        rag_wins += 1\n",
    "                    else:\n",
    "                        non_rag_wins += 1\n",
    "                \n",
    "                winners[lang] = {\n",
    "                    \"rag_wins\": rag_wins,\n",
    "                    \"non_rag_wins\": non_rag_wins,\n",
    "                    \"winner\": \"RAG\" if rag_wins > non_rag_wins else \"Non-RAG\"\n",
    "                }\n",
    "        \n",
    "        if winners:\n",
    "            report.append(\"\\n### System Performance by Language\")\n",
    "            for lang, results in winners.items():\n",
    "                report.append(f\"\\n**{lang.capitalize()}**: {results['winner']} system performs better \" +\n",
    "                              f\"({results['rag_wins']} vs {results['non_rag_wins']} metrics)\")\n",
    "        \n",
    "        # Cross-lingual insights\n",
    "        if \"cross_lingual\" in self.results:\n",
    "            report.append(\"\\n### Cross-Lingual Insights\")\n",
    "            \n",
    "            for system_type in [\"rag\", \"non_rag\"]:\n",
    "                if system_type in self.results[\"cross_lingual\"]:\n",
    "                    report.append(f\"\\n#### {system_type.upper()} System\")\n",
    "                    \n",
    "                    cross_ling = self.results[\"cross_lingual\"][system_type]\n",
    "                    \n",
    "                    # Find best performing language overall\n",
    "                    lang_scores = {lang: 0 for lang in self.languages}\n",
    "                    \n",
    "                    for metric, data in cross_ling.items():\n",
    "                        best_lang = data[\"best_language\"]\n",
    "                        lang_scores[best_lang] += 1\n",
    "                    \n",
    "                    best_lang_overall = max(lang_scores.items(), key=lambda x: x[1])[0]\n",
    "                    \n",
    "                    report.append(f\"- Best overall performance: **{best_lang_overall.capitalize()}**\")\n",
    "                    \n",
    "                    # Report on specific metrics\n",
    "                    key_metrics = ['medical_terminology', 'query_relevance', 'response_detail']\n",
    "                    for metric in key_metrics:\n",
    "                        if metric in cross_ling:\n",
    "                            data = cross_ling[metric]\n",
    "                            report.append(f\"- {metric.replace('_', ' ').title()}: Best in **{data['best_language'].capitalize()}**, \" +\n",
    "                                          f\"worst in **{data['worst_language'].capitalize()}**\")\n",
    "        \n",
    "        # Detailed per-language results\n",
    "        for lang in self.languages:\n",
    "            report.append(f\"\\n## {lang.capitalize()} Evaluation\")\n",
    "            \n",
    "            if \"rag\" in self.results[lang]:\n",
    "                report.append(\"\\n### RAG System\")\n",
    "                self._add_system_results_to_report(report, self.results[lang][\"rag\"])\n",
    "            \n",
    "            if \"non_rag\" in self.results[lang]:\n",
    "                report.append(\"\\n### Non-RAG System\")\n",
    "                self._add_system_results_to_report(report, self.results[lang][\"non_rag\"])\n",
    "            \n",
    "            if \"comparative\" in self.results[lang]:\n",
    "                report.append(\"\\n### Comparative Analysis\")\n",
    "                \n",
    "                report.append(\"\\n#### Metric Comparison\")\n",
    "                for metric, values in self.results[lang][\"comparative\"][\"metric_comparison\"].items():\n",
    "                    diff = values[\"difference\"]\n",
    "                    winner = values[\"winner\"].upper()\n",
    "                    report.append(f\"- {metric.replace('_', ' ').title()}: **{winner}** is better by {abs(diff):.3f}\")\n",
    "                \n",
    "                # Response similarity\n",
    "                resp_comp = self.results[lang][\"comparative\"][\"response_comparison\"]\n",
    "                report.append(f\"\\n#### Response Similarity\")\n",
    "                report.append(f\"- Average similarity between RAG and non-RAG: {resp_comp['average_similarity']:.3f}\")\n",
    "                report.append(f\"- Highly similar responses: {resp_comp['highly_similar_responses']}\")\n",
    "                report.append(f\"- Highly different responses: {resp_comp['highly_different_responses']}\")\n",
    "                \n",
    "                # LLM judge comparison if available\n",
    "                if \"llm_eval_comparison\" in self.results[lang][\"comparative\"] and self.results[lang][\"comparative\"][\"llm_eval_comparison\"]:\n",
    "                    report.append(f\"\\n#### Expert LLM Evaluation\")\n",
    "                    \n",
    "                    llm_comp = self.results[lang][\"comparative\"][\"llm_eval_comparison\"]\n",
    "                    for dimension, values in llm_comp.items():\n",
    "                        diff = values[\"difference\"]\n",
    "                        winner = values[\"winner\"].upper()\n",
    "                        report.append(f\"- {dimension.replace('_', ' ').title()}: **{winner}** is better by {abs(diff):.1f}/5.0\")\n",
    "        \n",
    "        # Recommendations\n",
    "        report.append(\"\\n## Recommendations\")\n",
    "        \n",
    "        # General recommendation based on overall performance\n",
    "        rag_better_langs = [lang for lang, data in winners.items() if data[\"winner\"] == \"RAG\"]\n",
    "        non_rag_better_langs = [lang for lang, data in winners.items() if data[\"winner\"] == \"Non-RAG\"]\n",
    "        \n",
    "        if len(rag_better_langs) > len(non_rag_better_langs):\n",
    "            report.append(\"\\n- RAG system performs better overall, especially for: \" + \n",
    "                         \", \".join([l.capitalize() for l in rag_better_langs]))\n",
    "            \n",
    "            if non_rag_better_langs:\n",
    "                report.append(f\"- Consider using non-RAG for: \" + \n",
    "                             \", \".join([l.capitalize() for l in non_rag_better_langs]))\n",
    "        else:\n",
    "            report.append(\"\\n- Non-RAG system performs better overall, especially for: \" + \n",
    "                         \", \".join([l.capitalize() for l in non_rag_better_langs]))\n",
    "            \n",
    "            if rag_better_langs:\n",
    "                report.append(f\"- Consider using RAG for: \" + \n",
    "                             \", \".join([l.capitalize() for l in rag_better_langs]))\n",
    "        \n",
    "        # Specific recommendations for improvement\n",
    "        report.append(\"\\n### Areas for Improvement\")\n",
    "        \n",
    "        # Identify common weaknesses\n",
    "        if \"cross_lingual\" in self.results and \"rag\" in self.results[\"cross_lingual\"]:\n",
    "            cross_ling_rag = self.results[\"cross_lingual\"][\"rag\"]\n",
    "            \n",
    "            # Find metrics with high variation across languages\n",
    "            varying_metrics = []\n",
    "            for metric, data in cross_ling_rag.items():\n",
    "                if data[\"range\"] > 0.2:  # Significant variation\n",
    "                    varying_metrics.append((metric, data[\"range\"], data[\"worst_language\"]))\n",
    "            \n",
    "            varying_metrics.sort(key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            if varying_metrics:\n",
    "                report.append(\"\\n#### Consistency Issues\")\n",
    "                for metric, range_val, worst_lang in varying_metrics[:3]:  # Top 3 issues\n",
    "                    report.append(f\"- Improve {metric.replace('_', ' ').title()} for {worst_lang.capitalize()} \" +\n",
    "                                  f\"(variation of {range_val:.2f} across languages)\")\n",
    "        \n",
    "        # Full report\n",
    "        full_report = \"\\n\".join(report)\n",
    "        \n",
    "        # Save report if path is provided\n",
    "        if output_path:\n",
    "            with open(f\"{output_path}/evaluation_report.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(full_report)\n",
    "        \n",
    "        return full_report\n",
    "    \n",
    "    def _add_system_results_to_report(self, report: List[str], system_results: Dict) -> None:\n",
    "        \"\"\"Add system results to the report.\"\"\"\n",
    "        # Medical terminology\n",
    "        if \"medical_terminology\" in system_results:\n",
    "            terms = system_results[\"medical_terminology\"]\n",
    "            report.append(f\"- Medical Terminology: {terms['average']:.3f} score, \" +\n",
    "                         f\"{terms['keyword_density']:.4f} keyword density\")\n",
    "        \n",
    "        # Response detail\n",
    "        if \"response_detail\" in system_results:\n",
    "            detail = system_results[\"response_detail\"]\n",
    "            report.append(f\"- Response Detail: {detail['average_detail_score']:.3f} score, \" +\n",
    "                         f\"{detail['average_words']:.1f} average words\")\n",
    "        \n",
    "        # Query relevance\n",
    "        if \"query_relevance\" in system_results:\n",
    "            relevance = system_results[\"query_relevance\"]\n",
    "            report.append(f\"- Query Relevance: {relevance['average_relevance']:.3f} score\")\n",
    "        \n",
    "        # Context utilization (RAG only)\n",
    "        if \"context_utilization\" in system_results:\n",
    "            util = system_results[\"context_utilization\"]\n",
    "            report.append(f\"- Context Utilization: {util['average_utilization']:.3f} score\")\n",
    "        \n",
    "        # Attribution (RAG only)\n",
    "        if \"attribution\" in system_results:\n",
    "            attr = system_results[\"attribution\"]\n",
    "            report.append(f\"- Source Attribution: {attr['attribution_rate']*100:.1f}% of responses\")\n",
    "        \n",
    "        # LLM evaluation if available\n",
    "        if \"llm_evaluation\" in system_results and \"average_scores\" in system_results[\"llm_evaluation\"]:\n",
    "            report.append(\"\\n#### Expert LLM Evaluation\")\n",
    "            \n",
    "            llm_scores = system_results[\"llm_evaluation\"][\"average_scores\"]\n",
    "            for dimension, score in llm_scores.items():\n",
    "                report.append(f\"- {dimension.replace('_', ' ').title()}: {score:.1f}/5.0\")\n",
    "\n",
    "\n",
    "# Function to demonstrate usage\n",
    "def evaluate_maternal_rag_without_groundtruth(\n",
    "    rag_system, \n",
    "    non_rag_system, \n",
    "    test_queries,\n",
    "    judge_llm=None,\n",
    "    output_path=\"./evaluation_results\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate multilingual maternal healthcare RAG system without ground truth.\n",
    "    \n",
    "    Args:\n",
    "        rag_system: RAG system to evaluate\n",
    "        non_rag_system: Non-RAG system to compare against\n",
    "        test_queries: Dictionary of test queries by language\n",
    "        judge_llm: Optional LLM for judging responses\n",
    "        output_path: Path to save evaluation results\n",
    "        \n",
    "    Returns:\n",
    "        Evaluation results\n",
    "    \"\"\"\n",
    "    import os\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    \n",
    "    # Initialize evaluator\n",
    "    evaluator = MultilingualRAGEvaluator(\n",
    "        judge_llm=judge_llm,\n",
    "        rag_system=rag_system,\n",
    "        non_rag_system=non_rag_system\n",
    "    )\n",
    "    \n",
    "    # Run evaluation\n",
    "    results = evaluator.evaluate_all(test_queries)\n",
    "    \n",
    "    # Generate visualizations\n",
    "    evaluator.visualize_results(output_path)\n",
    "    \n",
    "    # Generate and save report\n",
    "    report = evaluator.generate_report(output_path)\n",
    "    print(f\"Report saved to {output_path}/evaluation_report.md\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Example usage with dummy systems for demonstration\n",
    "class DummyMaternityRAG:\n",
    "    def answer(self, query, language=\"english\"):\n",
    "        \"\"\"Simulate RAG system answering.\"\"\"\n",
    "        if language == \"english\":\n",
    "            return \"Based on maternal healthcare guidelines, pregnant women should attend regular prenatal checkups. Research suggests these visits help monitor both maternal and fetal health.\"\n",
    "        elif language == \"hindi\":\n",
    "            return \"मातृत्व स्वास्थ्य दिशानिर्देशों के अनुसार, गर्भवती महिलाओं को नियमित प्रसवपूर्व जांच में जाना चाहिए। अनुसंधान से पता चलता है कि ये विज़िट मातृ और भ्रूण दोनों के स्वास्थ्य की निगरानी में मदद करते हैं।\"\n",
    "        elif language == \"assamese\":\n",
    "            return \"মাতৃত্ব স্বাস্থ্যসেৱা নিৰ্দেশিকা অনুসৰি, গৰ্ভৱতী মহিলাসকলে নিয়মিত প্ৰসৱপূৰ্ব পৰীক্ষা কৰাব লাগে। গৱেষণাত দেখা গৈছে যে এই পৰিদৰ্শনবোৰে মাতৃ আৰু গৰ্ভস্থ শিশুৰ স্বাস্থ্য দুয়োটা পৰ্যবেক্ষণ কৰাত সহায় কৰে।\"\n",
    "        else:  # hinglish\n",
    "            return \"Maternal healthcare guidelines ke according, pregnant women ko regular prenatal checkups ke liye jana chahiye. Research batati hai ki ye visits maternal aur fetal health dono ko monitor karne mein help karte hain.\"\n",
    "    \n",
    "    def retrieve(self, query, language=\"english\"):\n",
    "        \"\"\"Simulate retrieval.\"\"\"\n",
    "        if language == \"english\":\n",
    "            return [\"Pregnant women should attend regular prenatal checkups.\", \"These visits help monitor maternal and fetal health.\"]\n",
    "        elif language == \"hindi\":\n",
    "            return [\"गर्भवती महिलाओं को नियमित प्रसवपूर्व जांच में जाना चाहिए।\", \"ये विज़िट मातृ और भ्रूण दोनों के स्वास्थ्य की निगरानी में मदद करते हैं।\"]\n",
    "        elif language == \"assamese\":\n",
    "            return [\"গৰ্ভৱতী মহিলাসকলে নিয়মিত প্ৰসৱপূৰ্ব পৰীক্ষা কৰাব লাগে।\", \"এই পৰিদৰ্শনবোৰে মাতৃ আৰু গৰ্ভস্থ শিশুৰ স্বাস্থ্য দুয়োটা পৰ্যবেক্ষণ কৰাত সহায় কৰে।\"]\n",
    "        else:  # hinglish\n",
    "            return [\"Pregnant women ko regular prenatal checkups ke liye jana chahiye.\", \"Ye visits maternal aur fetal health dono ko monitor karne mein help karte hain.\"]\n",
    "\n",
    "\n",
    "class DummyMaternityLLM:\n",
    "    def generate(self, query, language=\"english\"):\n",
    "        \"\"\"Simulate non-RAG LLM generating an answer.\"\"\"\n",
    "        if language == \"english\":\n",
    "            return \"Pregnant women should go to regular checkups. These are important for health.\"\n",
    "        elif language == \"hindi\":\n",
    "            return \"गर्भवती महिलाओं को नियमित जांच के लिए जाना चाहिए। ये स्वास्थ्य के लिए महत्वपूर्ण हैं।\"\n",
    "        elif language == \"assamese\":\n",
    "            return \"গৰ্ভৱতী মহিলাসকলে নিয়মিত পৰীক্ষাৰ বাবে যাব লাগে। এইবোৰ স্বাস্থ্যৰ বাবে গুৰুত্বপূৰ্ণ।\"\n",
    "        else:  # hinglish\n",
    "            return \"Pregnant women ko regular checkups ke liye jana chahiye. Ye health ke liye important hain.\"\n",
    "\n",
    "\n",
    "def example_usage():\n",
    "    \"\"\"Run an example evaluation with dummy systems.\"\"\"\n",
    "    # Create dummy systems\n",
    "    rag_system = DummyMaternityRAG()\n",
    "    non_rag_system = DummyMaternityLLM()\n",
    "    \n",
    "    # Create test queries\n",
    "    test_queries = {\n",
    "        \"english\": [\n",
    "            \"How often should pregnant women go for checkups?\",\n",
    "            \"What nutrition is important during pregnancy?\",\n",
    "            \"What are signs of pregnancy complications?\"\n",
    "        ],\n",
    "        \"hindi\": [\n",
    "            \"गर्भवती महिलाओं को कितनी बार जांच के लिए जाना चाहिए?\",\n",
    "            \"गर्भावस्था के दौरान कौन सा पोषण महत्वपूर्ण है?\",\n",
    "            \"गर्भावस्था की जटिलताओं के संकेत क्या हैं?\"\n",
    "        ],\n",
    "        \"assamese\": [\n",
    "            \"গৰ্ভৱতী মহিলাসকলে কিমান সঘনাই পৰীক্ষাৰ বাবে যাব লাগে?\",\n",
    "            \"গৰ্ভাৱস্থাৰ সময়ত কোনটো পুষ্টি গুৰুত্বপূৰ্ণ?\",\n",
    "            \"গৰ্ভাৱস্থাৰ জটিলতাৰ লক্ষণবোৰ কি কি?\"\n",
    "        ],\n",
    "        \"hinglish\": [\n",
    "            \"Pregnant women ko kitni baar checkups ke liye jana chahiye?\",\n",
    "            \"Pregnancy ke dauran kaun sa nutrition important hai?\",\n",
    "            \"Pregnancy complications ke signs kya hain?\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Run evaluation\n",
    "    results = evaluate_maternal_rag_without_groundtruth(\n",
    "        rag_system=rag_system,\n",
    "        non_rag_system=non_rag_system,\n",
    "        test_queries=test_queries,\n",
    "        output_path=\"./dummy_evaluation_results\"\n",
    "    )\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    example_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "class HumanEvaluator:\n",
    "    \"\"\"\n",
    "    Framework for human evaluation of maternal healthcare responses\n",
    "    based on specified criteria: correctness, completeness, clarity, and cultural appropriateness.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, output_dir: str = \"./human_evaluation\"):\n",
    "        \"\"\"\n",
    "        Initialize the human evaluator.\n",
    "        \n",
    "        Args:\n",
    "            output_dir: Directory to save evaluation templates and results\n",
    "        \"\"\"\n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Define evaluation criteria\n",
    "        self.criteria = {\n",
    "            \"correctness\": {\n",
    "                \"description\": \"Is each claim in the response medically correct?\",\n",
    "                \"scale\": {\n",
    "                    1: \"all correct\",\n",
    "                    2: \"partially correct\",\n",
    "                    3: \"not correct\"\n",
    "                },\n",
    "                \"instructions\": \"Please firstly divide each response by claims, and then grade each claim individually.\"\n",
    "            },\n",
    "            \"completeness\": {\n",
    "                \"description\": \"Does the answer cover everything necessary, or should it include more information?\",\n",
    "                \"scale\": {\n",
    "                    1: \"covers everything the patient should know\",\n",
    "                    2: \"covers the most important information but not everything\",\n",
    "                    3: \"omits significant information\"\n",
    "                },\n",
    "                \"instructions\": \"If the answer is 2 or 3, please provide 1-2 missing information bullets that you think is most necessary.\"\n",
    "            },\n",
    "            \"clarity\": {\n",
    "                \"description\": \"Is this response clear enough for a user of average literacy to fully understand?\",\n",
    "                \"scale\": {\n",
    "                    1: \"completely understandable\",\n",
    "                    2: \"acceptable but not completely clear\",\n",
    "                    3: \"unacceptable\"\n",
    "                },\n",
    "                \"instructions\": \"\"\n",
    "            },\n",
    "            \"cultural_appropriateness\": {\n",
    "                \"description\": \"Is this response appropriate for the cultural context of the user who entered the question?\",\n",
    "                \"scale\": {\n",
    "                    1: \"completely appropriate\",\n",
    "                    2: \"acceptable but could be better\",\n",
    "                    3: \"inappropriate\"\n",
    "                },\n",
    "                \"instructions\": \"The cultural background of each user considered here is solely judged from their input language.\"\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def generate_evaluation_template(self, model_responses: Dict, output_format: str = \"csv\") -> str:\n",
    "        \"\"\"\n",
    "        Generate a template for human evaluators to fill out.\n",
    "        \n",
    "        Args:\n",
    "            model_responses: Dictionary with model responses by language\n",
    "            output_format: Format of the template ('csv' or 'excel')\n",
    "            \n",
    "        Returns:\n",
    "            Path to the generated template\n",
    "        \"\"\"\n",
    "        # Prepare data for template\n",
    "        rows = []\n",
    "        \n",
    "        # Get all model names\n",
    "        model_names = list(model_responses.keys())\n",
    "        \n",
    "        # Track which questions have been added\n",
    "        added_questions = set()\n",
    "        \n",
    "        for model_name in model_names:\n",
    "            for lang in model_responses[model_name]:\n",
    "                questions = model_responses[model_name][lang][\"questions\"]\n",
    "                answers = model_responses[model_name][lang][\"answers\"]\n",
    "                \n",
    "                for i, (question, answer) in enumerate(zip(questions, answers)):\n",
    "                    # Create a unique identifier for this question\n",
    "                    question_id = f\"{lang}_{i}_{hash(question) % 10000}\"\n",
    "                    \n",
    "                    if question_id not in added_questions:\n",
    "                        rows.append({\n",
    "                            \"question_id\": question_id,\n",
    "                            \"language\": lang,\n",
    "                            \"question\": question,\n",
    "                            \"model\": model_name,\n",
    "                            \"answer\": answer,\n",
    "                            \"correctness\": \"\",\n",
    "                            \"correctness_notes\": \"\",\n",
    "                            \"completeness\": \"\",\n",
    "                            \"completeness_notes\": \"\",\n",
    "                            \"clarity\": \"\",\n",
    "                            \"clarity_notes\": \"\",\n",
    "                            \"cultural_appropriateness\": \"\",\n",
    "                            \"cultural_appropriateness_notes\": \"\"\n",
    "                        })\n",
    "                        added_questions.add(question_id)\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame(rows)\n",
    "        \n",
    "        # Save template\n",
    "        if output_format == \"csv\":\n",
    "            template_path = os.path.join(self.output_dir, \"evaluation_template.csv\")\n",
    "            df.to_csv(template_path, index=False)\n",
    "        else:  # excel\n",
    "            template_path = os.path.join(self.output_dir, \"evaluation_template.xlsx\")\n",
    "            df.to_excel(template_path, index=False)\n",
    "        \n",
    "        # Create instructions file\n",
    "        instructions_path = os.path.join(self.output_dir, \"evaluation_instructions.txt\")\n",
    "        with open(instructions_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"# Human Evaluation Instructions for Maternal Healthcare Responses\\n\\n\")\n",
    "            \n",
    "            for criterion, details in self.criteria.items():\n",
    "                f.write(f\"## {criterion.capitalize()}\\n\")\n",
    "                f.write(f\"Description: {details['description']}\\n\\n\")\n",
    "                f.write(\"Scale:\\n\")\n",
    "                for score, desc in details['scale'].items():\n",
    "                    f.write(f\"- {score} = {desc}\\n\")\n",
    "                if details['instructions']:\n",
    "                    f.write(f\"\\nSpecial instructions: {details['instructions']}\\n\")\n",
    "                f.write(\"\\n\")\n",
    "        \n",
    "        print(f\"Evaluation template created at {template_path}\")\n",
    "        print(f\"Evaluation instructions created at {instructions_path}\")\n",
    "        \n",
    "        return template_path\n",
    "    \n",
    "    def parse_evaluation_results(self, results_path: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Parse the completed evaluation file and compute metrics.\n",
    "        \n",
    "        Args:\n",
    "            results_path: Path to the completed evaluation file\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with evaluation results\n",
    "        \"\"\"\n",
    "        # Read evaluation results\n",
    "        if results_path.endswith('.csv'):\n",
    "            df = pd.read_csv(results_path)\n",
    "        elif results_path.endswith(('.xlsx', '.xls')):\n",
    "            df = pd.read_excel(results_path)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file format. Please provide a CSV or Excel file.\")\n",
    "        \n",
    "        # Validate required columns\n",
    "        required_columns = [\n",
    "            \"model\", \"language\", \"correctness\", \"completeness\", \n",
    "            \"clarity\", \"cultural_appropriateness\"\n",
    "        ]\n",
    "        \n",
    "        for col in required_columns:\n",
    "            if col not in df.columns:\n",
    "                raise ValueError(f\"Required column '{col}' not found in evaluation results.\")\n",
    "        \n",
    "        # Convert ratings to numeric\n",
    "        rating_columns = [\"correctness\", \"completeness\", \"clarity\", \"cultural_appropriateness\"]\n",
    "        for col in rating_columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        \n",
    "        # Compute metrics\n",
    "        results = {}\n",
    "        \n",
    "        # Metrics by model and language\n",
    "        for model in df['model'].unique():\n",
    "            model_data = df[df['model'] == model]\n",
    "            \n",
    "            if model not in results:\n",
    "                results[model] = {\n",
    "                    \"overall\": self._compute_metrics(model_data),\n",
    "                    \"by_language\": {}\n",
    "                }\n",
    "            \n",
    "            for lang in model_data['language'].unique():\n",
    "                lang_data = model_data[model_data['language'] == lang]\n",
    "                results[model][\"by_language\"][lang] = self._compute_metrics(lang_data)\n",
    "        \n",
    "        # Aggregate metrics by model type (RAG vs non-RAG)\n",
    "        rag_models = [m for m in results.keys() if \"_rag\" in m]\n",
    "        non_rag_models = [m for m in results.keys() if \"_rag\" not in m]\n",
    "        \n",
    "        results[\"aggregated\"] = {\n",
    "            \"rag\": self._aggregate_model_results([results[m] for m in rag_models]),\n",
    "            \"non_rag\": self._aggregate_model_results([results[m] for m in non_rag_models])\n",
    "        }\n",
    "        \n",
    "        # Save processed results\n",
    "        results_json_path = os.path.join(self.output_dir, \"processed_evaluation_results.json\")\n",
    "        with open(results_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        \n",
    "        print(f\"Processed evaluation results saved to {results_json_path}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _compute_metrics(self, data: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Compute metrics for a subset of evaluation data.\n",
    "        \n",
    "        Args:\n",
    "            data: DataFrame with evaluation data\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with computed metrics\n",
    "        \"\"\"\n",
    "        metrics = {}\n",
    "        \n",
    "        # Rating metrics\n",
    "        rating_columns = [\"correctness\", \"completeness\", \"clarity\", \"cultural_appropriateness\"]\n",
    "        \n",
    "        for col in rating_columns:\n",
    "            col_data = data[col].dropna()\n",
    "            \n",
    "            if len(col_data) > 0:\n",
    "                metrics[col] = {\n",
    "                    \"mean\": float(col_data.mean()),\n",
    "                    \"median\": float(col_data.median()),\n",
    "                    \"distribution\": {\n",
    "                        \"1\": int((col_data == 1).sum()),\n",
    "                        \"2\": int((col_data == 2).sum()),\n",
    "                        \"3\": int((col_data == 3).sum())\n",
    "                    },\n",
    "                    \"count\": int(len(col_data))\n",
    "                }\n",
    "            else:\n",
    "                metrics[col] = {\n",
    "                    \"mean\": None,\n",
    "                    \"median\": None,\n",
    "                    \"distribution\": {\"1\": 0, \"2\": 0, \"3\": 0},\n",
    "                    \"count\": 0\n",
    "                }\n",
    "        \n",
    "        # Overall score (lower is better)\n",
    "        metrics[\"overall_score\"] = np.mean([metrics[col][\"mean\"] for col in rating_columns \n",
    "                                          if metrics[col][\"mean\"] is not None])\n",
    "        \n",
    "        # Calculate percentages for each rating\n",
    "        for col in rating_columns:\n",
    "            total = sum(metrics[col][\"distribution\"].values())\n",
    "            if total > 0:\n",
    "                metrics[col][\"percentage\"] = {\n",
    "                    score: (count / total) * 100 \n",
    "                    for score, count in metrics[col][\"distribution\"].items()\n",
    "                }\n",
    "            else:\n",
    "                metrics[col][\"percentage\"] = {score: 0 for score in [\"1\", \"2\", \"3\"]}\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def _aggregate_model_results(self, model_results: List[Dict]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Aggregate results from multiple models.\n",
    "        \n",
    "        Args:\n",
    "            model_results: List of model result dictionaries\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with aggregated metrics\n",
    "        \"\"\"\n",
    "        if not model_results:\n",
    "            return {}\n",
    "        \n",
    "        aggregated = {\n",
    "            \"overall\": {},\n",
    "            \"by_language\": defaultdict(list)\n",
    "        }\n",
    "        \n",
    "        # Collect metrics\n",
    "        rating_columns = [\"correctness\", \"completeness\", \"clarity\", \"cultural_appropriateness\"]\n",
    "        \n",
    "        # Aggregate overall metrics\n",
    "        for metric in rating_columns:\n",
    "            values = []\n",
    "            distribution = {\"1\": 0, \"2\": 0, \"3\": 0}\n",
    "            \n",
    "            for result in model_results:\n",
    "                if \"overall\" in result and metric in result[\"overall\"]:\n",
    "                    metric_data = result[\"overall\"][metric]\n",
    "                    if metric_data[\"mean\"] is not None:\n",
    "                        values.append(metric_data[\"mean\"])\n",
    "                    \n",
    "                    for score, count in metric_data[\"distribution\"].items():\n",
    "                        distribution[score] += count\n",
    "            \n",
    "            if values:\n",
    "                aggregated[\"overall\"][metric] = {\n",
    "                    \"mean\": float(np.mean(values)),\n",
    "                    \"distribution\": distribution,\n",
    "                    \"count\": sum(distribution.values())\n",
    "                }\n",
    "                \n",
    "                # Calculate percentages\n",
    "                total = sum(distribution.values())\n",
    "                if total > 0:\n",
    "                    aggregated[\"overall\"][metric][\"percentage\"] = {\n",
    "                        score: (count / total) * 100 \n",
    "                        for score, count in distribution.items()\n",
    "                    }\n",
    "                else:\n",
    "                    aggregated[\"overall\"][metric][\"percentage\"] = {score: 0 for score in [\"1\", \"2\", \"3\"]}\n",
    "        \n",
    "        # Aggregate language metrics\n",
    "        languages = set()\n",
    "        for result in model_results:\n",
    "            if \"by_language\" in result:\n",
    "                languages.update(result[\"by_language\"].keys())\n",
    "        \n",
    "        for lang in languages:\n",
    "            lang_metrics = {}\n",
    "            \n",
    "            for metric in rating_columns:\n",
    "                values = []\n",
    "                distribution = {\"1\": 0, \"2\": 0, \"3\": 0}\n",
    "                \n",
    "                for result in model_results:\n",
    "                    if \"by_language\" in result and lang in result[\"by_language\"] and metric in result[\"by_language\"][lang]:\n",
    "                        metric_data = result[\"by_language\"][lang][metric]\n",
    "                        if metric_data[\"mean\"] is not None:\n",
    "                            values.append(metric_data[\"mean\"])\n",
    "                        \n",
    "                        for score, count in metric_data[\"distribution\"].items():\n",
    "                            distribution[score] += count\n",
    "                \n",
    "                if values:\n",
    "                    lang_metrics[metric] = {\n",
    "                        \"mean\": float(np.mean(values)),\n",
    "                        \"distribution\": distribution,\n",
    "                        \"count\": sum(distribution.values())\n",
    "                    }\n",
    "                    \n",
    "                    # Calculate percentages\n",
    "                    total = sum(distribution.values())\n",
    "                    if total > 0:\n",
    "                        lang_metrics[metric][\"percentage\"] = {\n",
    "                            score: (count / total) * 100 \n",
    "                            for score, count in distribution.items()\n",
    "                        }\n",
    "                    else:\n",
    "                        lang_metrics[metric][\"percentage\"] = {score: 0 for score in [\"1\", \"2\", \"3\"]}\n",
    "            \n",
    "            if lang_metrics:\n",
    "                # Calculate overall score for this language\n",
    "                lang_metrics[\"overall_score\"] = np.mean([lang_metrics[col][\"mean\"] for col in rating_columns \n",
    "                                                      if col in lang_metrics and lang_metrics[col][\"mean\"] is not None])\n",
    "                aggregated[\"by_language\"][lang] = lang_metrics\n",
    "        \n",
    "        # Calculate overall score\n",
    "        if \"overall\" in aggregated and any(aggregated[\"overall\"]):\n",
    "            aggregated[\"overall\"][\"overall_score\"] = np.mean([\n",
    "                aggregated[\"overall\"][col][\"mean\"] for col in rating_columns \n",
    "                if col in aggregated[\"overall\"] and aggregated[\"overall\"][col][\"mean\"] is not None\n",
    "            ])\n",
    "        \n",
    "        return aggregated\n",
    "    \n",
    "    def generate_evaluation_report(self, evaluation_results: Dict[str, Any]) -> str:\n",
    "        \"\"\"\n",
    "        Generate a human-readable report from evaluation results.\n",
    "        \n",
    "        Args:\n",
    "            evaluation_results: Results from parse_evaluation_results\n",
    "            \n",
    "        Returns:\n",
    "            Path to the generated report\n",
    "        \"\"\"\n",
    "        report = []\n",
    "        report.append(\"# Maternal Healthcare Response Evaluation Report\")\n",
    "        report.append(\"\\n## Overview\")\n",
    "        \n",
    "        # Add RAG vs non-RAG comparison if available\n",
    "        if \"aggregated\" in evaluation_results and \"rag\" in evaluation_results[\"aggregated\"] and \"non_rag\" in evaluation_results[\"aggregated\"]:\n",
    "            rag_results = evaluation_results[\"aggregated\"][\"rag\"]\n",
    "            non_rag_results = evaluation_results[\"aggregated\"][\"non_rag\"]\n",
    "            \n",
    "            if \"overall\" in rag_results and \"overall\" in non_rag_results:\n",
    "                rag_score = rag_results[\"overall\"].get(\"overall_score\")\n",
    "                non_rag_score = non_rag_results[\"overall\"].get(\"overall_score\")\n",
    "                \n",
    "                if rag_score is not None and non_rag_score is not None:\n",
    "                    winner = \"RAG\" if rag_score < non_rag_score else \"Non-RAG\"\n",
    "                    diff = abs(rag_score - non_rag_score)\n",
    "                    \n",
    "                    report.append(f\"\\n### RAG vs Non-RAG Performance\")\n",
    "                    report.append(f\"\\nOverall, the **{winner}** approach performed better with an average score difference of {diff:.2f}.\")\n",
    "                    report.append(f\"(Note: Lower scores are better; 1 = best, 3 = worst)\")\n",
    "                    report.append(f\"\\n- RAG average score: {rag_score:.2f}\")\n",
    "                    report.append(f\"- Non-RAG average score: {non_rag_score:.2f}\")\n",
    "                    \n",
    "                    # Add detailed metrics\n",
    "                    report.append(\"\\n#### Detailed Metrics Comparison (RAG vs Non-RAG)\")\n",
    "                    \n",
    "                    for criterion in [\"correctness\", \"completeness\", \"clarity\", \"cultural_appropriateness\"]:\n",
    "                        if criterion in rag_results[\"overall\"] and criterion in non_rag_results[\"overall\"]:\n",
    "                            rag_mean = rag_results[\"overall\"][criterion][\"mean\"]\n",
    "                            non_rag_mean = non_rag_results[\"overall\"][criterion][\"mean\"]\n",
    "                            \n",
    "                            criterion_winner = \"RAG\" if rag_mean < non_rag_mean else \"Non-RAG\"\n",
    "                            criterion_diff = abs(rag_mean - non_rag_mean)\n",
    "                            \n",
    "                            report.append(f\"\\n**{criterion.capitalize()}**: **{criterion_winner}** is better by {criterion_diff:.2f}\")\n",
    "                            report.append(f\"- RAG: {rag_mean:.2f}\")\n",
    "                            report.append(f\"- Non-RAG: {non_rag_mean:.2f}\")\n",
    "        \n",
    "        # Add language-specific analysis\n",
    "        report.append(\"\\n## Language-Specific Analysis\")\n",
    "        \n",
    "        # Get all languages across all models\n",
    "        languages = set()\n",
    "        for model, data in evaluation_results.items():\n",
    "            if model != \"aggregated\" and \"by_language\" in data:\n",
    "                languages.update(data[\"by_language\"].keys())\n",
    "        \n",
    "        for lang in sorted(languages):\n",
    "            report.append(f\"\\n### {lang.capitalize()}\")\n",
    "            \n",
    "            # Compare RAG vs non-RAG for this language\n",
    "            if \"aggregated\" in evaluation_results:\n",
    "                rag_lang_data = evaluation_results[\"aggregated\"][\"rag\"].get(\"by_language\", {}).get(lang, {})\n",
    "                non_rag_lang_data = evaluation_results[\"aggregated\"][\"non_rag\"].get(\"by_language\", {}).get(lang, {})\n",
    "                \n",
    "                if rag_lang_data and non_rag_lang_data:\n",
    "                    rag_score = rag_lang_data.get(\"overall_score\")\n",
    "                    non_rag_score = non_rag_lang_data.get(\"overall_score\")\n",
    "                    \n",
    "                    if rag_score is not None and non_rag_score is not None:\n",
    "                        lang_winner = \"RAG\" if rag_score < non_rag_score else \"Non-RAG\"\n",
    "                        lang_diff = abs(rag_score - non_rag_score)\n",
    "                        \n",
    "                        report.append(f\"\\nFor {lang.capitalize()}, the **{lang_winner}** approach performed better with a score difference of {lang_diff:.2f}.\")\n",
    "                        report.append(f\"\\n- RAG average score: {rag_score:.2f}\")\n",
    "                        report.append(f\"- Non-RAG average score: {non_rag_score:.2f}\")\n",
    "                        \n",
    "                        # Add detailed metrics\n",
    "                        for criterion in [\"correctness\", \"completeness\", \"clarity\", \"cultural_appropriateness\"]:\n",
    "                            if criterion in rag_lang_data and criterion in non_rag_lang_data:\n",
    "                                rag_mean = rag_lang_data[criterion][\"mean\"]\n",
    "                                non_rag_mean = non_rag_lang_data[criterion][\"mean\"]\n",
    "                                \n",
    "                                criterion_winner = \"RAG\" if rag_mean < non_rag_mean else \"Non-RAG\"\n",
    "                                criterion_diff = abs(rag_mean - non_rag_mean)\n",
    "                                \n",
    "                                report.append(f\"\\n**{criterion.capitalize()}**: **{criterion_winner}** is better by {criterion_diff:.2f}\")\n",
    "        \n",
    "        # Add model-specific analysis\n",
    "        report.append(\"\\n## Model-Specific Analysis\")\n",
    "        \n",
    "        for model, data in evaluation_results.items():\n",
    "            if model != \"aggregated\" and \"overall\" in data:\n",
    "                report.append(f\"\\n### {model}\")\n",
    "                \n",
    "                if \"overall_score\" in data[\"overall\"]:\n",
    "                    report.append(f\"\\nOverall score: {data['overall']['overall_score']:.2f}\")\n",
    "                \n",
    "                # Add criterion breakdown\n",
    "                for criterion in [\"correctness\", \"completeness\", \"clarity\", \"cultural_appropriateness\"]:\n",
    "                    if criterion in data[\"overall\"]:\n",
    "                        criterion_data = data[\"overall\"][criterion]\n",
    "                        report.append(f\"\\n**{criterion.capitalize()}**: {criterion_data['mean']:.2f}\")\n",
    "                        \n",
    "                        # Add distribution\n",
    "                        if \"percentage\" in criterion_data:\n",
    "                            report.append(f\"- Score distribution:\")\n",
    "                            for score, percentage in criterion_data[\"percentage\"].items():\n",
    "                                report.append(f\"  - {score}: {percentage:.1f}%\")\n",
    "        \n",
    "        # Save report\n",
    "        report_path = os.path.join(self.output_dir, \"human_evaluation_report.md\")\n",
    "        with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"\\n\".join(report))\n",
    "        \n",
    "        print(f\"Evaluation report saved to {report_path}\")\n",
    "        \n",
    "        return report_path\n",
    "\n",
    "\n",
    "def integrate_human_evaluation(model_responses, languages=[\"english\", \"hindi\", \"assamese\", \"hinglish\"]):\n",
    "    \"\"\"\n",
    "    Generate templates for human evaluation of model responses.\n",
    "    \n",
    "    Args:\n",
    "        model_responses: Dictionary with model responses by language\n",
    "        languages: List of languages to evaluate\n",
    "        \n",
    "    Returns:\n",
    "        Path to the generated template\n",
    "    \"\"\"\n",
    "    evaluator = HumanEvaluator(output_dir=\"./human_evaluation\")\n",
    "    \n",
    "    # Filter responses to only include specified languages\n",
    "    filtered_responses = {}\n",
    "    for model, data in model_responses.items():\n",
    "        filtered_responses[model] = {lang: data[lang] for lang in data if lang in languages}\n",
    "    \n",
    "    # Generate evaluation template\n",
    "    template_path = evaluator.generate_evaluation_template(filtered_responses, output_format=\"excel\")\n",
    "    \n",
    "    return template_path\n",
    "\n",
    "\n",
    "def process_human_evaluation_results(results_path):\n",
    "    \"\"\"\n",
    "    Process human evaluation results and generate a report.\n",
    "    \n",
    "    Args:\n",
    "        results_path: Path to the completed evaluation file\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with evaluation results\n",
    "    \"\"\"\n",
    "    evaluator = HumanEvaluator(output_dir=\"./human_evaluation\")\n",
    "    \n",
    "    # Parse evaluation results\n",
    "    results = evaluator.parse_evaluation_results(results_path)\n",
    "    \n",
    "    # Generate evaluation report\n",
    "    report_path = evaluator.generate_evaluation_report(results)\n",
    "    \n",
    "    return {\n",
    "        \"results\": results,\n",
    "        \"report_path\": report_path\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-10 03:15:52,353 - INFO - Loading responses from /home/vidhij2/nivi/test_docs/gpt-4-turbo_rag.json...\n",
      "2025-04-10 03:15:52,355 - INFO - Loaded responses for gpt-4-turbo\n",
      "2025-04-10 03:15:52,356 - INFO - Loading responses from /home/vidhij2/nivi/test_docs/gpt-4-turbo_rag.json...\n",
      "2025-04-10 03:15:52,357 - INFO - Loaded responses for gpt-4-turbo_rag\n",
      "2025-04-10 03:15:52,357 - INFO - Loading responses from /home/vidhij2/nivi/test_docs/mixtral_rag.json...\n",
      "2025-04-10 03:15:52,358 - INFO - Loaded responses for mixtral\n",
      "2025-04-10 03:15:52,359 - INFO - Loading responses from /home/vidhij2/nivi/test_docs/mixtral_rag.json...\n",
      "2025-04-10 03:15:52,360 - INFO - Loaded responses for mixtral_rag\n",
      "2025-04-10 03:15:52,360 - INFO - Loading responses from /home/vidhij2/nivi/test_docs/llama_rag.json...\n",
      "2025-04-10 03:15:52,361 - INFO - Loaded responses for llama\n",
      "2025-04-10 03:15:52,362 - INFO - Loading responses from /home/vidhij2/nivi/test_docs/llama_rag.json...\n",
      "2025-04-10 03:15:52,363 - INFO - Loaded responses for llama_rag\n",
      "Evaluating with judges:   0%|          | 0/1 [00:00<?, ?it/s]2025-04-10 03:15:52,365 - INFO - Starting evaluation with judge: gemini-pro\n",
      "2025-04-10 03:15:52,367 - INFO - Evaluating gpt-4-turbo for english with gemini-pro\n",
      "\n",
      "\u001b[A2025-04-10 03:15:52,604 - ERROR - Error getting evaluation from gemini-pro: Error calling Google API: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "\n",
      "\n",
      "2025-04-10 03:15:52,605 - ERROR - Error getting evaluation from gemini-pro: Error calling Google API: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\u001b[A2025-04-10 03:15:52,608 - ERROR - Error getting evaluation from gemini-pro: Error calling Google API: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "Evaluating gpt-4-turbo in english: 100%|██████████| 3/3 [00:00<00:00, 12.68it/s]\n",
      "2025-04-10 03:15:52,610 - INFO - Evaluating gpt-4-turbo for hindi with gemini-pro\n",
      "\n",
      "\u001b[A2025-04-10 03:15:52,717 - ERROR - Error getting evaluation from gemini-pro: Error calling Google API: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "\n",
      "\u001b[A2025-04-10 03:15:52,731 - ERROR - Error getting evaluation from gemini-pro: Error calling Google API: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "Evaluating gpt-4-turbo in hindi: 100%|██████████| 2/2 [00:00<00:00, 17.09it/s]\n",
      "2025-04-10 03:15:52,734 - INFO - Evaluating gpt-4-turbo for hinglish with gemini-pro\n",
      "\n",
      "\u001b[A2025-04-10 03:15:52,839 - ERROR - Error getting evaluation from gemini-pro: Error calling Google API: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "2025-04-10 03:15:52,839 - ERROR - Error getting evaluation from gemini-pro: Error calling Google API: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "2025-04-10 03:15:52,840 - ERROR - Error getting evaluation from gemini-pro: Error calling Google API: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "2025-04-10 03:15:52,841 - ERROR - Error getting evaluation from gemini-pro: Error calling Google API: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "2025-04-10 03:15:52,851 - ERROR - Error getting evaluation from gemini-pro: Error calling Google API: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "\n",
      "Evaluating gpt-4-turbo in hinglish: 100%|██████████| 5/5 [00:00<00:00, 48.72it/s]\n",
      "2025-04-10 03:15:52,858 - INFO - Evaluating gpt-4-turbo_rag for english with gemini-pro\n",
      "\n",
      "\u001b[A2025-04-10 03:15:52,957 - ERROR - Error getting evaluation from gemini-pro: Error calling Google API: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "2025-04-10 03:15:52,968 - ERROR - Error getting evaluation from gemini-pro: Error calling Google API: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "2025-04-10 03:15:53,094 - ERROR - Error getting evaluation from gemini-pro: Error calling Google API: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "\n",
      "Evaluating gpt-4-turbo_rag in english: 100%|██████████| 3/3 [00:00<00:00, 13.21it/s]\n",
      "2025-04-10 03:15:53,097 - INFO - Evaluating gpt-4-turbo_rag for hindi with gemini-pro\n",
      "\n",
      "\u001b[A2025-04-10 03:15:53,186 - ERROR - Error getting evaluation from gemini-pro: Error calling Google API: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "2025-04-10 03:15:53,203 - ERROR - Error getting evaluation from gemini-pro: Error calling Google API: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "\n",
      "Evaluating gpt-4-turbo_rag in hindi: 100%|██████████| 2/2 [00:00<00:00, 19.30it/s]\n",
      "2025-04-10 03:15:53,205 - INFO - Evaluating gpt-4-turbo_rag for hinglish with gemini-pro\n",
      "\n",
      "\u001b[A2025-04-10 03:15:53,287 - ERROR - Error getting evaluation from gemini-pro: Error calling Google API: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "2025-04-10 03:15:53,296 - ERROR - Error getting evaluation from gemini-pro: Error calling Google API: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "2025-04-10 03:15:53,307 - ERROR - Error getting evaluation from gemini-pro: Error calling Google API: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "2025-04-10 03:15:53,308 - ERROR - Error getting evaluation from gemini-pro: Error calling Google API: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "2025-04-10 03:15:53,309 - ERROR - Error getting evaluation from gemini-pro: Error calling Google API: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "Evaluating gpt-4-turbo_rag in hinglish: 100%|██████████| 5/5 [00:00<00:00, 52.73it/s]\n",
      "2025-04-10 03:15:53,312 - INFO - Evaluating mixtral for english with gemini-pro\n",
      "\n",
      "\u001b[A2025-04-10 03:15:53,417 - ERROR - Error getting evaluation from gemini-pro: Error calling Google API: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "\n",
      "\u001b[A2025-04-10 03:15:53,418 - ERROR - Error getting evaluation from gemini-pro: Error calling Google API: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "2025-04-10 03:15:53,434 - ERROR - Error getting evaluation from gemini-pro: Error calling Google API: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "Evaluating mixtral in english: 100%|██████████| 3/3 [00:00<00:00, 25.77it/s]\n",
      "2025-04-10 03:15:53,435 - INFO - Evaluating mixtral for hindi with gemini-pro\n",
      "\n",
      "\u001b[A2025-04-10 03:15:53,651 - ERROR - Error getting evaluation from gemini-pro: Error calling Google API: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "\n",
      "\u001b[A2025-04-10 03:15:53,655 - ERROR - Error getting evaluation from gemini-pro: Error calling Google API: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "Evaluating mixtral in hindi: 100%|██████████| 2/2 [00:00<00:00,  9.17it/s]\n",
      "2025-04-10 03:15:53,657 - INFO - Evaluating mixtral for hinglish with gemini-pro\n",
      "\n",
      "\u001b[A2025-04-10 03:15:53,744 - ERROR - Error getting evaluation from gemini-pro: Error calling Google API: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "2025-04-10 03:15:53,758 - ERROR - Error getting evaluation from gemini-pro: Error calling Google API: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "2025-04-10 03:15:53,758 - ERROR - Error getting evaluation from gemini-pro: Error calling Google API: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "2025-04-10 03:15:53,765 - ERROR - Error getting evaluation from gemini-pro: Error calling Google API: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "\n",
      "\n",
      "Evaluating mixtral in hinglish: 100%|██████████| 5/5 [00:00<00:00, 48.07it/s]rror calling Google API: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\u001b[A\n",
      "2025-04-10 03:15:53,772 - INFO - Evaluating mixtral_rag for english with gemini-pro\n",
      "\n",
      "\u001b[A2025-04-10 03:15:53,853 - ERROR - Error getting evaluation from gemini-pro: Error calling Google API: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "2025-04-10 03:15:53,856 - ERROR - Error getting evaluation from gemini-pro: Error calling Google API: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "2025-04-10 03:15:53,864 - ERROR - Error getting evaluation from gemini-pro: Error calling Google API: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "Evaluating mixtral_rag in english: 100%|██████████| 3/3 [00:00<00:00, 34.14it/s]\n",
      "2025-04-10 03:15:53,866 - INFO - Evaluating mixtral_rag for hindi with gemini-pro\n",
      "\n",
      "\u001b[A2025-04-10 03:15:53,970 - ERROR - Error getting evaluation from gemini-pro: Error calling Google API: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "\n",
      "\u001b[A2025-04-10 03:15:54,004 - ERROR - Error getting evaluation from gemini-pro: Error calling Google API: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "Evaluating mixtral_rag in hindi: 100%|██████████| 2/2 [00:00<00:00, 14.64it/s]\n",
      "2025-04-10 03:15:54,007 - INFO - Evaluating mixtral_rag for hinglish with gemini-pro\n",
      "\n",
      "\u001b[A2025-04-10 03:15:54,098 - ERROR - Error getting evaluation from gemini-pro: Error calling Google API: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "2025-04-10 03:15:54,100 - ERROR - Error getting evaluation from gemini-pro: Error calling Google API: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "2025-04-10 03:15:54,101 - ERROR - Error getting evaluation from gemini-pro: Error calling Google API: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "2025-04-10 03:15:54,113 - ERROR - Error getting evaluation from gemini-pro: Error calling Google API: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "2025-04-10 03:15:54,123 - ERROR - Error getting evaluation from gemini-pro: Error calling Google API: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "Evaluating mixtral_rag in hinglish: 100%|██████████| 5/5 [00:00<00:00, 50.44it/s]\n",
      "2025-04-10 03:15:54,127 - INFO - Evaluating llama for english with gemini-pro\n",
      "\n",
      "\u001b[A2025-04-10 03:15:54,221 - ERROR - Error getting evaluation from gemini-pro: Error calling Google API: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "2025-04-10 03:15:54,223 - ERROR - Error getting evaluation from gemini-pro: Error calling Google API: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "2025-04-10 03:15:54,291 - ERROR - Error getting evaluation from gemini-pro: Error calling Google API: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "\n",
      "Evaluating llama in english: 100%|██████████| 3/3 [00:00<00:00, 18.51it/s]\n",
      "2025-04-10 03:15:54,296 - INFO - Evaluating llama for hindi with gemini-pro\n",
      "\n",
      "\u001b[A2025-04-10 03:15:54,388 - ERROR - Error getting evaluation from gemini-pro: Error calling Google API: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "2025-04-10 03:15:54,404 - ERROR - Error getting evaluation from gemini-pro: Error calling Google API: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "Evaluating llama in hindi: 100%|██████████| 2/2 [00:00<00:00, 20.66it/s]\n",
      "2025-04-10 03:15:54,407 - INFO - Evaluating llama for hinglish with gemini-pro\n",
      "\n",
      "\u001b[A2025-04-10 03:15:54,495 - ERROR - Error getting evaluation from gemini-pro: Error calling Google API: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "2025-04-10 03:15:54,500 - ERROR - Error getting evaluation from gemini-pro: Error calling Google API: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "2025-04-10 03:15:54,509 - ERROR - Error getting evaluation from gemini-pro: Error calling Google API: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "2025-04-10 03:15:54,517 - ERROR - Error getting evaluation from gemini-pro: Error calling Google API: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "2025-04-10 03:15:54,529 - ERROR - Error getting evaluation from gemini-pro: Error calling Google API: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "\n",
      "Evaluating llama in hinglish: 100%|██████████| 5/5 [00:00<00:00, 46.21it/s]\n",
      "2025-04-10 03:15:54,536 - INFO - Evaluating llama_rag for english with gemini-pro\n",
      "\n",
      "\u001b[A2025-04-10 03:15:54,637 - ERROR - Error getting evaluation from gemini-pro: Error calling Google API: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "2025-04-10 03:15:54,640 - ERROR - Error getting evaluation from gemini-pro: Error calling Google API: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "2025-04-10 03:15:54,650 - ERROR - Error getting evaluation from gemini-pro: Error calling Google API: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "Evaluating llama_rag in english: 100%|██████████| 3/3 [00:00<00:00, 30.03it/s]\n",
      "2025-04-10 03:15:54,653 - INFO - Evaluating llama_rag for hindi with gemini-pro\n",
      "\n",
      "\u001b[A2025-04-10 03:15:54,736 - ERROR - Error getting evaluation from gemini-pro: Error calling Google API: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "2025-04-10 03:15:54,744 - ERROR - Error getting evaluation from gemini-pro: Error calling Google API: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "Evaluating llama_rag in hindi: 100%|██████████| 2/2 [00:00<00:00, 23.56it/s]\n",
      "2025-04-10 03:15:54,748 - INFO - Evaluating llama_rag for hinglish with gemini-pro\n",
      "\n",
      "\u001b[A2025-04-10 03:15:54,848 - ERROR - Error getting evaluation from gemini-pro: Error calling Google API: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "2025-04-10 03:15:54,849 - ERROR - Error getting evaluation from gemini-pro: Error calling Google API: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "2025-04-10 03:15:54,852 - ERROR - Error getting evaluation from gemini-pro: Error calling Google API: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "2025-04-10 03:15:54,853 - ERROR - Error getting evaluation from gemini-pro: Error calling Google API: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "2025-04-10 03:15:54,861 - ERROR - Error getting evaluation from gemini-pro: Error calling Google API: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "Evaluating llama_rag in hinglish: 100%|██████████| 5/5 [00:00<00:00, 52.71it/s]\n",
      "Processing models with gemini-pro: 100%|██████████| 6/6 [00:02<00:00,  2.40it/s]\n",
      "Evaluating with judges: 100%|██████████| 1/1 [00:02<00:00,  2.50s/it]\n",
      "2025-04-10 03:15:54,867 - INFO - Saved full evaluation results to ./maternal_healthcare_evaluation_new/evaluation_results.json\n",
      "2025-04-10 03:15:54,868 - INFO - Saved summary results to ./maternal_healthcare_evaluation_new/evaluation_summary.json\n",
      "2025-04-10 03:15:56,930 - INFO - Generated markdown report at ./maternal_healthcare_evaluation_new/evaluation_report.md\n",
      "2025-04-10 03:15:56,937 - INFO - Generated visual reports in ./maternal_healthcare_evaluation_new/visuals\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Any, Tuple, Optional\n",
    "from collections import defaultdict\n",
    "import requests\n",
    "from tqdm.auto import tqdm\n",
    "import logging\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"llm_judge_evaluation.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "class LLMJudgeEvaluator:\n",
    "    \"\"\"\n",
    "    Evaluator that uses different LLMs as judges to evaluate responses based on:\n",
    "    - Correctness: Is each claim in the response medically correct?\n",
    "    - Completeness: Does the answer cover everything necessary?\n",
    "    - Clarity: Is this response clear for a user of average literacy?\n",
    "    - Cultural Appropriateness: Is this response appropriate for the cultural context?\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        output_dir: str = \"./llm_judge_evaluation\",\n",
    "        parallel_requests: int = 5,\n",
    "        cache_results: bool = True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the LLM Judge evaluator.\n",
    "        \n",
    "        Args:\n",
    "            output_dir: Directory to save evaluation results\n",
    "            parallel_requests: Number of parallel requests to make to LLM APIs\n",
    "            cache_results: Whether to cache LLM judgments to avoid duplicate API calls\n",
    "        \"\"\"\n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Cache for LLM judgments\n",
    "        self.cache_dir = os.path.join(output_dir, \"cache\")\n",
    "        os.makedirs(self.cache_dir, exist_ok=True)\n",
    "        self.cache_results = cache_results\n",
    "        \n",
    "        # Parallel processing settings\n",
    "        self.parallel_requests = parallel_requests\n",
    "        \n",
    "        # Define the criteria\n",
    "        self.criteria = [\n",
    "            \"correctness\",\n",
    "            \"completeness\",\n",
    "            \"clarity\",\n",
    "            \"cultural_appropriateness\"\n",
    "        ]\n",
    "        \n",
    "        # LLM judges configuration\n",
    "        self.judges = {\n",
    "            \"gpt-4\": {\n",
    "                \"api_name\": \"openai\",\n",
    "                \"model\": \"gpt-4-turbo-preview\",\n",
    "                \"temperature\": 0.2,\n",
    "                \"max_tokens\": 1000,\n",
    "                \"system_prompt\": \"You are an expert in maternal healthcare with extensive medical knowledge. You will evaluate AI responses to maternal healthcare questions to determine if they are correct, complete, clear, and culturally appropriate.\"\n",
    "            },\n",
    "            \"gemini-pro\": {\n",
    "                \"api_name\": \"google\",\n",
    "                \"model\": \"gemini-1.5-pro\",\n",
    "                \"temperature\": 0.2,\n",
    "                \"max_tokens\": 1000,\n",
    "                \"system_prompt\": \"You are an expert in maternal healthcare with extensive medical knowledge. You will evaluate AI responses to maternal healthcare questions to determine if they are correct, complete, clear, and culturally appropriate.\"\n",
    "            }\n",
    "            # Add more judges as needed\n",
    "        }\n",
    "    \n",
    "    def evaluate(\n",
    "        self,\n",
    "        json_files: Dict[str, str],\n",
    "        judges_to_use: List[str] = [\"gpt-4\"],\n",
    "        languages: List[str] = [\"english\", \"hindi\", \"assamese\", \"hinglish\"],\n",
    "        sample_size: Optional[int] = None\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        Evaluate model responses using LLM judges.\n",
    "        \n",
    "        Args:\n",
    "            json_files: Dictionary mapping model names to file paths\n",
    "            judges_to_use: List of judge models to use\n",
    "            languages: Languages to evaluate\n",
    "            sample_size: Number of samples to evaluate per language per model (None for all)\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with evaluation results\n",
    "        \"\"\"\n",
    "        # Validate judges\n",
    "        for judge in judges_to_use:\n",
    "            if judge not in self.judges:\n",
    "                raise ValueError(f\"Judge {judge} not found. Available judges: {list(self.judges.keys())}\")\n",
    "        \n",
    "        # Load responses from all models\n",
    "        responses = self._load_responses(json_files, languages)\n",
    "        \n",
    "        # Apply sampling if requested\n",
    "        if sample_size is not None:\n",
    "            self._apply_sampling(responses, sample_size)\n",
    "        \n",
    "        # Initialize results container\n",
    "        results = {\n",
    "            \"model_evaluations\": {},\n",
    "            \"judge_comparisons\": {},\n",
    "            \"language_comparisons\": {},\n",
    "            \"rag_vs_nonrag_comparisons\": {}\n",
    "        }\n",
    "        \n",
    "        # Evaluate each model with each judge\n",
    "        for judge_name in tqdm(judges_to_use, desc=\"Evaluating with judges\"):\n",
    "            judge_config = self.judges[judge_name]\n",
    "            \n",
    "            logging.info(f\"Starting evaluation with judge: {judge_name}\")\n",
    "            \n",
    "            # Create results container for this judge\n",
    "            results[\"model_evaluations\"][judge_name] = {}\n",
    "            \n",
    "            # Process each model\n",
    "            for model_name, model_data in tqdm(responses.items(), desc=f\"Processing models with {judge_name}\"):\n",
    "                results[\"model_evaluations\"][judge_name][model_name] = {}\n",
    "                \n",
    "                # Process each language\n",
    "                for lang, lang_data in model_data.items():\n",
    "                    if not lang_data[\"questions\"] or not lang_data[\"answers\"]:\n",
    "                        continue\n",
    "                    \n",
    "                    logging.info(f\"Evaluating {model_name} for {lang} with {judge_name}\")\n",
    "                    \n",
    "                    # Evaluate responses for this language\n",
    "                    lang_results = self._evaluate_model_language(\n",
    "                        model_name=model_name,\n",
    "                        lang=lang,\n",
    "                        questions=lang_data[\"questions\"],\n",
    "                        responses=lang_data[\"answers\"],\n",
    "                        judge_name=judge_name,\n",
    "                        judge_config=judge_config\n",
    "                    )\n",
    "                    \n",
    "                    results[\"model_evaluations\"][judge_name][model_name][lang] = lang_results\n",
    "        \n",
    "        # Compare judges\n",
    "        if len(judges_to_use) > 1:\n",
    "            results[\"judge_comparisons\"] = self._compare_judges(results[\"model_evaluations\"])\n",
    "        \n",
    "        # Compare languages\n",
    "        results[\"language_comparisons\"] = self._compare_languages(results[\"model_evaluations\"])\n",
    "        \n",
    "        # Compare RAG vs non-RAG\n",
    "        results[\"rag_vs_nonrag_comparisons\"] = self._compare_rag_vs_nonrag(results[\"model_evaluations\"])\n",
    "        \n",
    "        # Save results\n",
    "        self._save_results(results)\n",
    "        \n",
    "        # Generate visual reports\n",
    "        self._generate_visual_reports(results)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _load_responses(self, json_files: Dict[str, str], languages: List[str]) -> Dict:\n",
    "        \"\"\"Load responses from JSON files.\"\"\"\n",
    "        model_responses = {}\n",
    "        \n",
    "        for model_name, file_path in json_files.items():\n",
    "            logging.info(f\"Loading responses from {file_path}...\")\n",
    "            \n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    data = json.load(f)\n",
    "                    \n",
    "                # Initialize containers for this model\n",
    "                if model_name not in model_responses:\n",
    "                    model_responses[model_name] = {lang: {\"questions\": [], \"answers\": []} for lang in languages}\n",
    "                \n",
    "                # Process each entry in the JSON\n",
    "                for entry in data:\n",
    "                    # Detect language from the entry\n",
    "                    detected_lang = entry.get(\"detected_info\", {}).get(\"language\", \"unknown\")\n",
    "                    \n",
    "                    # Map detected language to our standard language categories\n",
    "                    if detected_lang == \"english\":\n",
    "                        lang = \"english\"\n",
    "                    elif detected_lang == \"hindi\":\n",
    "                        lang = \"hindi\"\n",
    "                    elif detected_lang == \"assamese\":\n",
    "                        lang = \"assamese\"\n",
    "                    elif \"hinglish\" in detected_lang.lower() or (detected_lang == \"english\" and any(hindi_word in entry[\"question_original\"].lower() for hindi_word in [\"kya\", \"hai\", \"me\", \"ko\", \"ki\"])):\n",
    "                        lang = \"hinglish\"\n",
    "                    else:\n",
    "                        lang = \"other\"\n",
    "                        \n",
    "                    # Skip if not in our target languages\n",
    "                    if lang not in languages:\n",
    "                        continue\n",
    "                    \n",
    "                    # Add the question and answer to the appropriate container\n",
    "                    model_responses[model_name][lang][\"questions\"].append(entry[\"question_original\"])\n",
    "                    model_responses[model_name][lang][\"answers\"].append(entry[\"answer\"])\n",
    "                    \n",
    "                logging.info(f\"Loaded responses for {model_name}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error loading {file_path}: {e}\")\n",
    "        \n",
    "        return model_responses\n",
    "    \n",
    "    def _apply_sampling(self, responses: Dict, sample_size: int):\n",
    "        \"\"\"Apply sampling to limit the number of evaluated examples.\"\"\"\n",
    "        for model_name, model_data in responses.items():\n",
    "            for lang, lang_data in model_data.items():\n",
    "                if len(lang_data[\"questions\"]) > sample_size:\n",
    "                    # Get random indices for sampling\n",
    "                    indices = np.random.choice(\n",
    "                        len(lang_data[\"questions\"]), \n",
    "                        size=sample_size, \n",
    "                        replace=False\n",
    "                    )\n",
    "                    \n",
    "                    # Apply sampling\n",
    "                    lang_data[\"questions\"] = [lang_data[\"questions\"][i] for i in indices]\n",
    "                    lang_data[\"answers\"] = [lang_data[\"answers\"][i] for i in indices]\n",
    "                    \n",
    "                    logging.info(f\"Sampled {sample_size} examples for {model_name} in {lang}\")\n",
    "    \n",
    "    def _evaluate_model_language(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        lang: str,\n",
    "        questions: List[str],\n",
    "        responses: List[str],\n",
    "        judge_name: str,\n",
    "        judge_config: Dict\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        Evaluate responses for a specific model and language using an LLM judge.\n",
    "        \n",
    "        Args:\n",
    "            model_name: Name of the model being evaluated\n",
    "            lang: Language of the responses\n",
    "            questions: List of questions\n",
    "            responses: List of responses\n",
    "            judge_name: Name of the judge to use\n",
    "            judge_config: Configuration for the judge\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with evaluation results\n",
    "        \"\"\"\n",
    "        # Prepare evaluation tasks\n",
    "        eval_tasks = []\n",
    "        for i, (question, response) in enumerate(zip(questions, responses)):\n",
    "            # Create a unique ID for caching\n",
    "            cache_id = f\"{model_name}_{lang}_{judge_name}_{i}\"\n",
    "            \n",
    "            eval_tasks.append({\n",
    "                \"id\": cache_id,\n",
    "                \"question\": question,\n",
    "                \"response\": response,\n",
    "                \"lang\": lang\n",
    "            })\n",
    "        \n",
    "        # Process evaluation tasks in parallel\n",
    "        evaluations = []\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=self.parallel_requests) as executor:\n",
    "            futures = []\n",
    "            \n",
    "            for task in eval_tasks:\n",
    "                future = executor.submit(\n",
    "                    self._get_judge_evaluation,\n",
    "                    task[\"id\"],\n",
    "                    task[\"question\"],\n",
    "                    task[\"response\"],\n",
    "                    task[\"lang\"],\n",
    "                    judge_name,\n",
    "                    judge_config\n",
    "                )\n",
    "                futures.append(future)\n",
    "            \n",
    "            for future in tqdm(as_completed(futures), total=len(futures), desc=f\"Evaluating {model_name} in {lang}\"):\n",
    "                try:\n",
    "                    eval_result = future.result()\n",
    "                    if eval_result:\n",
    "                        evaluations.append(eval_result)\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error in evaluation task: {e}\")\n",
    "        \n",
    "        # Aggregate results\n",
    "        aggregated = {\n",
    "            \"evaluations\": evaluations,\n",
    "            \"summary\": self._aggregate_evaluations(evaluations)\n",
    "        }\n",
    "        \n",
    "        return aggregated\n",
    "    \n",
    "    def _get_judge_evaluation(\n",
    "        self,\n",
    "        cache_id: str,\n",
    "        question: str,\n",
    "        response: str,\n",
    "        language: str,\n",
    "        judge_name: str,\n",
    "        judge_config: Dict\n",
    "    ) -> Optional[Dict]:\n",
    "        \"\"\"\n",
    "        Get evaluation from an LLM judge, with caching.\n",
    "        \n",
    "        Args:\n",
    "            cache_id: Unique ID for caching\n",
    "            question: User question\n",
    "            response: Model response\n",
    "            language: Language of content\n",
    "            judge_name: Name of the judge\n",
    "            judge_config: Configuration for the judge\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with evaluation results or None if error\n",
    "        \"\"\"\n",
    "        # Check cache first if enabled\n",
    "        if self.cache_results:\n",
    "            cache_file = os.path.join(self.cache_dir, f\"{cache_id}.json\")\n",
    "            if os.path.exists(cache_file):\n",
    "                try:\n",
    "                    with open(cache_file, 'r', encoding='utf-8') as f:\n",
    "                        return json.load(f)\n",
    "                except:\n",
    "                    pass  # Continue if cache read fails\n",
    "        \n",
    "        # Create judge prompt\n",
    "        prompt = self._create_judge_prompt(question, response, language)\n",
    "        \n",
    "        # Call judge API\n",
    "        try:\n",
    "            api_name = judge_config[\"api_name\"]\n",
    "            \n",
    "            if api_name == \"openai\":\n",
    "                judge_response = self._call_openai_api(\n",
    "                    prompt=prompt,\n",
    "                    system_prompt=judge_config[\"system_prompt\"],\n",
    "                    model=judge_config[\"model\"],\n",
    "                    temperature=judge_config[\"temperature\"],\n",
    "                    max_tokens=judge_config[\"max_tokens\"]\n",
    "                )\n",
    "            elif api_name == \"google\":\n",
    "                judge_response = self._call_google_api(\n",
    "                    prompt=prompt,\n",
    "                    system_prompt=judge_config[\"system_prompt\"],\n",
    "                    model=judge_config[\"model\"],\n",
    "                    temperature=judge_config[\"temperature\"],\n",
    "                    max_tokens=judge_config[\"max_tokens\"]\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported API: {api_name}\")\n",
    "            \n",
    "            # Parse judge response\n",
    "            evaluation = self._parse_judge_response(judge_response)\n",
    "            \n",
    "            # Add metadata\n",
    "            evaluation[\"question\"] = question\n",
    "            evaluation[\"response\"] = response\n",
    "            evaluation[\"language\"] = language\n",
    "            evaluation[\"judge\"] = judge_name\n",
    "            \n",
    "            # Cache result if enabled\n",
    "            if self.cache_results:\n",
    "                with open(cache_file, 'w', encoding='utf-8') as f:\n",
    "                    json.dump(evaluation, f, indent=2)\n",
    "            \n",
    "            return evaluation\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error getting evaluation from {judge_name}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _create_judge_prompt(self, question: str, response: str, language: str) -> str:\n",
    "        \"\"\"Create prompt for the judge.\"\"\"\n",
    "        return f\"\"\"\n",
    "        You are evaluating an AI response to a maternal healthcare question.\n",
    "        \n",
    "        User Query: {question}\n",
    "        \n",
    "        AI Response: {response}\n",
    "        \n",
    "        Language: {language}\n",
    "        \n",
    "        Please evaluate this response on the following criteria (score 1-3, where 1 is best, 3 is worst):\n",
    "        \n",
    "        1. Medical Correctness (1-3):\n",
    "           - Is each claim in the response medically correct?\n",
    "           - 1 = all correct, 2 = partially correct, 3 = not correct\n",
    "           - Please divide the response into claims and evaluate each one.\n",
    "        \n",
    "        2. Completeness (1-3):\n",
    "           - Does the answer cover everything necessary?\n",
    "           - 1 = covers everything the patient should know, 2 = covers most important information but not everything, 3 = omits significant information\n",
    "           - If the answer is 2 or 3, provide 1-2 missing information points that would be most necessary.\n",
    "        \n",
    "        3. Language Clarity (1-3):\n",
    "           - Is this response clear enough for a user of average literacy?\n",
    "           - 1 = completely understandable, 2 = acceptable but not completely clear, 3 = unacceptable\n",
    "        \n",
    "        4. Cultural Appropriateness (1-3):\n",
    "           - Is this response appropriate for the cultural context of a {language}-speaking user? Also the user is from Assam, India.\n",
    "           - 1 = completely appropriate, 2 = acceptable but could be better, 3 = inappropriate\n",
    "        \n",
    "        For each criterion, provide:\n",
    "        1. A score (1, 2, or 3)\n",
    "        2. A brief justification for your score\n",
    "        3. For Completeness, if score is 2 or 3, list the 1-2 most important missing pieces of information\n",
    "        \n",
    "        Format your response as JSON:\n",
    "        {{\n",
    "            \"correctness\": {{\n",
    "                \"score\": <1, 2, or 3>,\n",
    "                \"justification\": \"<your reasoning>\",\n",
    "                \"problematic_claims\": [\"<if any>\"]\n",
    "            }},\n",
    "            \"completeness\": {{\n",
    "                \"score\": <1, 2, or 3>,\n",
    "                \"justification\": \"<your reasoning>\",\n",
    "                \"missing_information\": [\"<if any>\"]\n",
    "            }},\n",
    "            \"clarity\": {{\n",
    "                \"score\": <1, 2, or 3>,\n",
    "                \"justification\": \"<your reasoning>\"\n",
    "            }},\n",
    "            \"cultural_appropriateness\": {{\n",
    "                \"score\": <1, 2, or 3>,\n",
    "                \"justification\": \"<your reasoning>\"\n",
    "            }}\n",
    "        }}\n",
    "        \"\"\"\n",
    "    \n",
    "    def _call_openai_api(\n",
    "        self, \n",
    "        prompt: str, \n",
    "        system_prompt: str, \n",
    "        model: str,\n",
    "        temperature: float,\n",
    "        max_tokens: int\n",
    "    ) -> str:\n",
    "        \"\"\"Call OpenAI API for judgment.\"\"\"\n",
    "        try:\n",
    "            import openai\n",
    "            client = openai.OpenAI()\n",
    "            \n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens\n",
    "            )\n",
    "            \n",
    "            return response.choices[0].message.content\n",
    "            \n",
    "        except ImportError:\n",
    "            raise ImportError(\"OpenAI package not installed. Install with: pip install openai\")\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error calling OpenAI API: {e}\")\n",
    "    \n",
    "    def _call_google_api(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        system_prompt: str,\n",
    "        model: str,\n",
    "        temperature: float,\n",
    "        max_tokens: int\n",
    "    ) -> str:\n",
    "        \"\"\"Call Google Gemini API for judgment.\"\"\"\n",
    "        try:\n",
    "            import google.generativeai as genai\n",
    "            \n",
    "            genai.configure(api_key=GEMINI_API_KEY)\n",
    "            \n",
    "            model = genai.GenerativeModel(model)\n",
    "            \n",
    "            combined_prompt = f\"{system_prompt}\\n\\n{prompt}\"\n",
    "            \n",
    "            response = model.generate_content(\n",
    "                combined_prompt,\n",
    "                generation_config=genai.GenerationConfig(\n",
    "                    temperature=temperature,\n",
    "                    max_output_tokens=max_tokens\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            return response.text\n",
    "            \n",
    "        except ImportError:\n",
    "            raise ImportError(\"Google Generative AI package not installed. Install with: pip install google-generativeai\")\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error calling Google API: {e}\")\n",
    "    \n",
    "    def _parse_judge_response(self, response_text: str) -> Dict:\n",
    "        \"\"\"Parse the JSON response from the judge.\"\"\"\n",
    "        # Extract JSON from response (it might be wrapped in markdown code blocks)\n",
    "        json_match = re.search(r'```json\\s*([\\s\\S]*?)\\s*```', response_text)\n",
    "        if json_match:\n",
    "            json_str = json_match.group(1)\n",
    "        else:\n",
    "            # Try to find JSON without code blocks\n",
    "            json_match = re.search(r'({[\\s\\S]*})', response_text)\n",
    "            if json_match:\n",
    "                json_str = json_match.group(1)\n",
    "            else:\n",
    "                json_str = response_text\n",
    "        \n",
    "        try:\n",
    "            # Parse the JSON\n",
    "            evaluation = json.loads(json_str)\n",
    "            \n",
    "            # Ensure all criteria are present\n",
    "            for criterion in self.criteria:\n",
    "                if criterion not in evaluation:\n",
    "                    evaluation[criterion] = {\"score\": 3, \"justification\": \"Not evaluated\"}\n",
    "                \n",
    "                # Ensure score is an integer between 1-3\n",
    "                if \"score\" not in evaluation[criterion] or not isinstance(evaluation[criterion][\"score\"], int) or evaluation[criterion][\"score\"] < 1 or evaluation[criterion][\"score\"] > 3:\n",
    "                    evaluation[criterion][\"score\"] = 3\n",
    "            \n",
    "            return evaluation\n",
    "            \n",
    "        except json.JSONDecodeError:\n",
    "            # Fallback parsing for non-JSON responses\n",
    "            logging.warning(f\"Failed to parse JSON response: {response_text[:100]}...\")\n",
    "            \n",
    "            # Create a default evaluation\n",
    "            evaluation = {}\n",
    "            \n",
    "            for criterion in self.criteria:\n",
    "                # Try to extract score using regex\n",
    "                score_match = re.search(rf'{criterion}.*?score.*?([1-3])', response_text, re.IGNORECASE)\n",
    "                score = int(score_match.group(1)) if score_match else 3\n",
    "                \n",
    "                evaluation[criterion] = {\n",
    "                    \"score\": score,\n",
    "                    \"justification\": \"Failed to parse structured response\"\n",
    "                }\n",
    "            \n",
    "            return evaluation\n",
    "    \n",
    "    def _aggregate_evaluations(self, evaluations: List[Dict]) -> Dict:\n",
    "        \"\"\"Aggregate evaluation results.\"\"\"\n",
    "        if not evaluations:\n",
    "            return {}\n",
    "        \n",
    "        summary = {}\n",
    "        \n",
    "        # Process each criterion\n",
    "        for criterion in self.criteria:\n",
    "            scores = [eval[criterion][\"score\"] for eval in evaluations if criterion in eval]\n",
    "            \n",
    "            if not scores:\n",
    "                continue\n",
    "                \n",
    "            # Calculate statistics\n",
    "            summary[criterion] = {\n",
    "                \"mean_score\": np.mean(scores),\n",
    "                \"median_score\": np.median(scores),\n",
    "                \"distribution\": {\n",
    "                    \"1\": sum(1 for s in scores if s == 1),\n",
    "                    \"2\": sum(1 for s in scores if s == 2),\n",
    "                    \"3\": sum(1 for s in scores if s == 3)\n",
    "                },\n",
    "                \"count\": len(scores)\n",
    "            }\n",
    "            \n",
    "            # Calculate percentage distribution\n",
    "            total = summary[criterion][\"count\"]\n",
    "            if total > 0:\n",
    "                summary[criterion][\"percentage\"] = {\n",
    "                    score: (count / total) * 100 \n",
    "                    for score, count in summary[criterion][\"distribution\"].items()\n",
    "                }\n",
    "        \n",
    "        # Overall score across all criteria\n",
    "        all_criteria_means = [summary[c][\"mean_score\"] for c in self.criteria if c in summary]\n",
    "        if all_criteria_means:\n",
    "            summary[\"overall\"] = {\n",
    "                \"mean_score\": np.mean(all_criteria_means)\n",
    "            }\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def _compare_judges(self, model_evaluations: Dict) -> Dict:\n",
    "        \"\"\"Compare evaluations from different judges.\"\"\"\n",
    "        if len(model_evaluations) <= 1:\n",
    "            return {}\n",
    "            \n",
    "        judge_comparisons = {\n",
    "            \"agreement\": {},\n",
    "            \"bias\": {}\n",
    "        }\n",
    "        \n",
    "        # Get list of judges\n",
    "        judges = list(model_evaluations.keys())\n",
    "        \n",
    "        # Calculate agreement for each model and language\n",
    "        for judge1 in judges:\n",
    "            for judge2 in judges:\n",
    "                if judge1 >= judge2:  # Skip self-comparisons and duplicates\n",
    "                    continue\n",
    "                    \n",
    "                comparison_key = f\"{judge1}_vs_{judge2}\"\n",
    "                judge_comparisons[\"agreement\"][comparison_key] = {}\n",
    "                \n",
    "                # Find common models evaluated by both judges\n",
    "                common_models = set(model_evaluations[judge1].keys()) & set(model_evaluations[judge2].keys())\n",
    "                \n",
    "                for model in common_models:\n",
    "                    judge_comparisons[\"agreement\"][comparison_key][model] = {}\n",
    "                    \n",
    "                    # Find common languages\n",
    "                    common_langs = set(model_evaluations[judge1][model].keys()) & set(model_evaluations[judge2][model].keys())\n",
    "                    \n",
    "                    for lang in common_langs:\n",
    "                        # Get evaluations\n",
    "                        evals1 = model_evaluations[judge1][model][lang][\"evaluations\"]\n",
    "                        evals2 = model_evaluations[judge2][model][lang][\"evaluations\"]\n",
    "                        \n",
    "                        # Match evaluations for the same questions\n",
    "                        matched_evals = []\n",
    "                        \n",
    "                        for eval1 in evals1:\n",
    "                            for eval2 in evals2:\n",
    "                                if eval1[\"question\"] == eval2[\"question\"]:\n",
    "                                    matched_evals.append((eval1, eval2))\n",
    "                                    break\n",
    "                        \n",
    "                        # Calculate agreement for each criterion\n",
    "                        criterion_agreement = {}\n",
    "                        \n",
    "                        for criterion in self.criteria:\n",
    "                            exact_matches = 0\n",
    "                            score_diffs = []\n",
    "                            \n",
    "                            for eval1, eval2 in matched_evals:\n",
    "                                if criterion in eval1 and criterion in eval2:\n",
    "                                    score1 = eval1[criterion][\"score\"]\n",
    "                                    score2 = eval2[criterion][\"score\"]\n",
    "                                    \n",
    "                                    if score1 == score2:\n",
    "                                        exact_matches += 1\n",
    "                                        \n",
    "                                    score_diffs.append(abs(score1 - score2))\n",
    "                            \n",
    "                            if matched_evals:\n",
    "                                criterion_agreement[criterion] = {\n",
    "                                    \"exact_agreement\": exact_matches / len(matched_evals),\n",
    "                                    \"avg_score_diff\": np.mean(score_diffs) if score_diffs else 0\n",
    "                                }\n",
    "                        \n",
    "                        # Calculate overall agreement\n",
    "                        overall_agreement = np.mean([\n",
    "                            criterion_agreement[c][\"exact_agreement\"] \n",
    "                            for c in self.criteria \n",
    "                            if c in criterion_agreement\n",
    "                        ])\n",
    "                        \n",
    "                        judge_comparisons[\"agreement\"][comparison_key][model][lang] = {\n",
    "                            \"criterion_agreement\": criterion_agreement,\n",
    "                            \"overall_agreement\": overall_agreement,\n",
    "                            \"matched_evaluations\": len(matched_evals)\n",
    "                        }\n",
    "        \n",
    "        # Calculate judge bias\n",
    "        judge_comparisons[\"bias\"] = self._calculate_judge_bias(model_evaluations)\n",
    "        \n",
    "        return judge_comparisons\n",
    "    \n",
    "    def _calculate_judge_bias(self, model_evaluations: Dict) -> Dict:\n",
    "        \"\"\"Calculate bias of each judge compared to the average.\"\"\"\n",
    "        # Get all judges\n",
    "        judges = list(model_evaluations.keys())\n",
    "        \n",
    "        if len(judges) <= 1:\n",
    "            return {}\n",
    "            \n",
    "        # Initialize bias tracking\n",
    "        bias = {judge: {} for judge in judges}\n",
    "        \n",
    "        # For each criterion, calculate average scores across all judges\n",
    "        for criterion in self.criteria:\n",
    "            criterion_scores = defaultdict(list)\n",
    "            \n",
    "            # Collect scores from all judges\n",
    "            for judge in judges:\n",
    "                for model in model_evaluations[judge]:\n",
    "                    for lang in model_evaluations[judge][model]:\n",
    "                        summary = model_evaluations[judge][model][lang].get(\"summary\", {})\n",
    "                        if criterion in summary and \"mean_score\" in summary[criterion]:\n",
    "                            # Create a key for this model-language pair\n",
    "                            key = f\"{model}_{lang}\"\n",
    "                            criterion_scores[key].append({\n",
    "                                \"judge\": judge,\n",
    "                                \"score\": summary[criterion][\"mean_score\"]\n",
    "                            })\n",
    "            \n",
    "            # Calculate average and judge deviation\n",
    "            for key, scores in criterion_scores.items():\n",
    "                if len(scores) <= 1:\n",
    "                    continue\n",
    "                    \n",
    "                # Calculate average score across judges\n",
    "                avg_score = np.mean([s[\"score\"] for s in scores])\n",
    "                \n",
    "                # Calculate deviation for each judge\n",
    "                for score_data in scores:\n",
    "                    judge = score_data[\"judge\"]\n",
    "                    deviation = score_data[\"score\"] - avg_score\n",
    "                    \n",
    "                    if criterion not in bias[judge]:\n",
    "                        bias[judge][criterion] = []\n",
    "                        \n",
    "                    bias[judge][criterion].append(deviation)\n",
    "            \n",
    "            # Calculate average bias for each judge\n",
    "            for judge in judges:\n",
    "                if criterion in bias[judge] and bias[judge][criterion]:\n",
    "                    bias[judge][f\"{criterion}_avg_bias\"] = np.mean(bias[judge][criterion])\n",
    "                    bias[judge][f\"{criterion}_std_bias\"] = np.std(bias[judge][criterion])\n",
    "        \n",
    "        # Calculate overall bias\n",
    "        for judge in judges:\n",
    "            bias_values = [\n",
    "                bias[judge][f\"{c}_avg_bias\"] \n",
    "                for c in self.criteria \n",
    "                if f\"{c}_avg_bias\" in bias[judge]\n",
    "            ]\n",
    "            \n",
    "            if bias_values:\n",
    "                bias[judge][\"overall_avg_bias\"] = np.mean(bias_values)\n",
    "                \n",
    "                # Determine if judge is strict, moderate, or lenient\n",
    "                if bias[judge][\"overall_avg_bias\"] < -0.2:\n",
    "                    bias[judge][\"bias_tendency\"] = \"strict\"\n",
    "                elif bias[judge][\"overall_avg_bias\"] > 0.2:\n",
    "                    bias[judge][\"bias_tendency\"] = \"lenient\"\n",
    "                else:\n",
    "                    bias[judge][\"bias_tendency\"] = \"moderate\"\n",
    "        \n",
    "        return bias\n",
    "    \n",
    "    def _compare_languages(self, model_evaluations: Dict) -> Dict:\n",
    "        \"\"\"Compare performance across languages.\"\"\"\n",
    "        language_comparisons = {}\n",
    "        \n",
    "        # For each judge\n",
    "        for judge, judge_data in model_evaluations.items():\n",
    "            language_comparisons[judge] = {}\n",
    "            \n",
    "            # For each model\n",
    "            for model, model_data in judge_data.items():\n",
    "                language_comparisons[judge][model] = {}\n",
    "                \n",
    "                # Get languages for this model\n",
    "                langs = list(model_data.keys())\n",
    "                \n",
    "                if not langs:\n",
    "                    continue\n",
    "                    \n",
    "                # Compare performance across languages\n",
    "                for criterion in self.criteria + [\"overall\"]:\n",
    "                    if criterion != \"overall\":\n",
    "                        criterion_key = criterion\n",
    "                        score_key = \"mean_score\"\n",
    "                    else:\n",
    "                        criterion_key = \"overall\"\n",
    "                        score_key = \"mean_score\"\n",
    "                    \n",
    "                    # Collect scores for each language\n",
    "                    lang_scores = {}\n",
    "                    \n",
    "                    for lang in langs:\n",
    "                        summary = model_data[lang].get(\"summary\", {})\n",
    "                        if criterion_key in summary and score_key in summary[criterion_key]:\n",
    "                            lang_scores[lang] = summary[criterion_key][score_key]\n",
    "                    \n",
    "                    if lang_scores:\n",
    "                        # Find best and worst language\n",
    "                        best_lang = min(lang_scores.items(), key=lambda x: x[1])[0]\n",
    "                        worst_lang = max(lang_scores.items(), key=lambda x: x[1])[0]\n",
    "                        \n",
    "                        language_comparisons[judge][model][criterion_key] = {\n",
    "                            \"scores\": lang_scores,\n",
    "                            \"best_language\": best_lang,\n",
    "                            \"worst_language\": worst_lang,\n",
    "                            \"score_range\": lang_scores[worst_lang] - lang_scores[best_lang]\n",
    "                        }\n",
    "        \n",
    "        return language_comparisons\n",
    "    \n",
    "    def _compare_rag_vs_nonrag(self, model_evaluations: Dict) -> Dict:\n",
    "        \"\"\"Compare RAG vs non-RAG models.\"\"\"\n",
    "        rag_comparisons = {}\n",
    "        \n",
    "        # For each judge\n",
    "        for judge, judge_data in model_evaluations.items():\n",
    "            rag_comparisons[judge] = {}\n",
    "            \n",
    "            # Find RAG and non-RAG model pairs\n",
    "            model_pairs = {}\n",
    "            for model_name in judge_data.keys():\n",
    "                if \"_rag\" in model_name:\n",
    "                    base_model = model_name.replace(\"_rag\", \"\")\n",
    "                    if base_model in judge_data:\n",
    "                        model_pairs[base_model] = model_name\n",
    "            \n",
    "            # Compare each pair\n",
    "            for base_model, rag_model in model_pairs.items():\n",
    "                comparison_key = f\"{base_model}_vs_{rag_model}\"\n",
    "                rag_comparisons[judge][comparison_key] = {}\n",
    "                \n",
    "                # Find common languages\n",
    "                base_langs = set(judge_data[base_model].keys())\n",
    "                rag_langs = set(judge_data[rag_model].keys())\n",
    "                common_langs = base_langs & rag_langs\n",
    "                \n",
    "                for lang in common_langs:\n",
    "                    rag_comparisons[judge][comparison_key][lang] = {}\n",
    "                    \n",
    "                    # Compare on each criterion\n",
    "                    for criterion in self.criteria + [\"overall\"]:\n",
    "                        base_summary = judge_data[base_model][lang].get(\"summary\", {})\n",
    "                        rag_summary = judge_data[rag_model][lang].get(\"summary\", {})\n",
    "                    \n",
    "                        if criterion != \"overall\":\n",
    "                            criterion_key = criterion\n",
    "                            score_key = \"mean_score\"\n",
    "                        else:\n",
    "                            criterion_key = \"overall\"\n",
    "                            score_key = \"mean_score\"\n",
    "                        \n",
    "                        if (criterion_key in base_summary and score_key in base_summary[criterion_key] and\n",
    "                            criterion_key in rag_summary and score_key in rag_summary[criterion_key]):\n",
    "                            \n",
    "                            base_score = base_summary[criterion_key][score_key]\n",
    "                            rag_score = rag_summary[criterion_key][score_key]\n",
    "                            difference = rag_score - base_score\n",
    "                            \n",
    "                            rag_comparisons[judge][comparison_key][lang][criterion] = {\n",
    "                                \"non_rag_score\": base_score,\n",
    "                                \"rag_score\": rag_score,\n",
    "                                \"difference\": difference,\n",
    "                                \"winner\": \"rag\" if difference < 0 else \"non_rag\"\n",
    "                            }\n",
    "        \n",
    "            return rag_comparisons\n",
    "    \n",
    "    def _save_results(self, results: Dict) -> None:\n",
    "        \"\"\"Save evaluation results to file.\"\"\"\n",
    "        # Save full results\n",
    "        results_path = os.path.join(self.output_dir, \"evaluation_results.json\")\n",
    "        with open(results_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        \n",
    "        logging.info(f\"Saved full evaluation results to {results_path}\")\n",
    "        \n",
    "        # Save summary results in a more compact format\n",
    "        summary = self._create_summary(results)\n",
    "        summary_path = os.path.join(self.output_dir, \"evaluation_summary.json\")\n",
    "        with open(summary_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(summary, f, indent=2)\n",
    "        \n",
    "        logging.info(f\"Saved summary results to {summary_path}\")\n",
    "    \n",
    "    def _create_summary(self, results: Dict) -> Dict:\n",
    "        \"\"\"Create a summary of the evaluation results.\"\"\"\n",
    "        summary = {\n",
    "            \"model_performance\": {},\n",
    "            \"rag_vs_nonrag\": {},\n",
    "            \"language_performance\": {},\n",
    "            \"judge_agreement\": {}\n",
    "        }\n",
    "        \n",
    "        # Model performance summary\n",
    "        for judge, judge_data in results[\"model_evaluations\"].items():\n",
    "            summary[\"model_performance\"][judge] = {}\n",
    "            \n",
    "            for model, model_data in judge_data.items():\n",
    "                model_scores = {}\n",
    "                \n",
    "                for criterion in self.criteria + [\"overall\"]:\n",
    "                    scores = []\n",
    "                    \n",
    "                    for lang, lang_data in model_data.items():\n",
    "                        lang_summary = lang_data.get(\"summary\", {})\n",
    "                        \n",
    "                        if criterion != \"overall\":\n",
    "                            if criterion in lang_summary and \"mean_score\" in lang_summary[criterion]:\n",
    "                                scores.append(lang_summary[criterion][\"mean_score\"])\n",
    "                        else:\n",
    "                            if \"overall\" in lang_summary and \"mean_score\" in lang_summary[\"overall\"]:\n",
    "                                scores.append(lang_summary[\"overall\"][\"mean_score\"])\n",
    "                    \n",
    "                    if scores:\n",
    "                        model_scores[criterion] = np.mean(scores)\n",
    "                \n",
    "                if model_scores:\n",
    "                    summary[\"model_performance\"][judge][model] = model_scores\n",
    "        \n",
    "        # RAG vs non-RAG summary\n",
    "        for judge, judge_data in results[\"rag_vs_nonrag_comparisons\"].items():\n",
    "            summary[\"rag_vs_nonrag\"][judge] = {}\n",
    "            \n",
    "            for comparison_key, comparison_data in judge_data.items():\n",
    "                rag_wins = 0\n",
    "                non_rag_wins = 0\n",
    "                \n",
    "                for lang, lang_data in comparison_data.items():\n",
    "                    if \"overall\" in lang_data and \"winner\" in lang_data[\"overall\"]:\n",
    "                        winner = lang_data[\"overall\"][\"winner\"]\n",
    "                        \n",
    "                        if winner == \"rag\":\n",
    "                            rag_wins += 1\n",
    "                        else:\n",
    "                            non_rag_wins += 1\n",
    "                \n",
    "                if rag_wins > 0 or non_rag_wins > 0:\n",
    "                    summary[\"rag_vs_nonrag\"][judge][comparison_key] = {\n",
    "                        \"rag_wins\": rag_wins,\n",
    "                        \"non_rag_wins\": non_rag_wins,\n",
    "                        \"winner\": \"rag\" if rag_wins > non_rag_wins else \"non_rag\"\n",
    "                    }\n",
    "        \n",
    "        # Language performance summary\n",
    "        for judge, judge_data in results[\"language_comparisons\"].items():\n",
    "            summary[\"language_performance\"][judge] = {}\n",
    "            \n",
    "            for model, model_data in judge_data.items():\n",
    "                if \"overall\" in model_data:\n",
    "                    summary[\"language_performance\"][judge][model] = {\n",
    "                        \"best_language\": model_data[\"overall\"][\"best_language\"],\n",
    "                        \"worst_language\": model_data[\"overall\"][\"worst_language\"]\n",
    "                    }\n",
    "        \n",
    "        # Judge agreement summary\n",
    "        if \"agreement\" in results[\"judge_comparisons\"]:\n",
    "            for comparison_key, comparison_data in results[\"judge_comparisons\"][\"agreement\"].items():\n",
    "                agreement_scores = []\n",
    "                \n",
    "                for model, model_data in comparison_data.items():\n",
    "                    for lang, lang_data in model_data.items():\n",
    "                        if \"overall_agreement\" in lang_data:\n",
    "                            agreement_scores.append(lang_data[\"overall_agreement\"])\n",
    "                \n",
    "                if agreement_scores:\n",
    "                    summary[\"judge_agreement\"][comparison_key] = np.mean(agreement_scores)\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def _generate_visual_reports(self, results: Dict) -> None:\n",
    "        \"\"\"Generate visual reports from evaluation results.\"\"\"\n",
    "        # Create directory for visuals\n",
    "        visuals_dir = os.path.join(self.output_dir, \"visuals\")\n",
    "        os.makedirs(visuals_dir, exist_ok=True)\n",
    "        \n",
    "        # Set visualization style\n",
    "        plt.style.use('ggplot')\n",
    "        sns.set_palette(\"colorblind\")\n",
    "        \n",
    "        # 1. Overall model performance by judge\n",
    "        self._plot_model_performance(results, visuals_dir)\n",
    "        \n",
    "        # 2. RAG vs non-RAG comparison\n",
    "        self._plot_rag_comparison(results, visuals_dir)\n",
    "        \n",
    "        # 3. Performance by language\n",
    "        self._plot_language_performance(results, visuals_dir)\n",
    "        \n",
    "        # 4. Judge agreement (if multiple judges)\n",
    "        if len(results[\"model_evaluations\"]) > 1:\n",
    "            self._plot_judge_agreement(results, visuals_dir)\n",
    "        \n",
    "        # 5. Generate comprehensive report\n",
    "        self._generate_markdown_report(results, visuals_dir)\n",
    "        \n",
    "        logging.info(f\"Generated visual reports in {visuals_dir}\")\n",
    "    \n",
    "    def _plot_model_performance(self, results: Dict, visuals_dir: str) -> None:\n",
    "        \"\"\"Plot overall model performance.\"\"\"\n",
    "        for judge, judge_data in results[\"model_evaluations\"].items():\n",
    "            # Prepare data\n",
    "            models = []\n",
    "            overall_scores = []\n",
    "            criterion_scores = {c: [] for c in self.criteria}\n",
    "            \n",
    "            for model, model_data in judge_data.items():\n",
    "                model_avg_scores = {c: [] for c in self.criteria + [\"overall\"]}\n",
    "                \n",
    "                for lang, lang_data in model_data.items():\n",
    "                    summary = lang_data.get(\"summary\", {})\n",
    "                    \n",
    "                    # Collect criterion scores\n",
    "                    for criterion in self.criteria:\n",
    "                        if criterion in summary and \"mean_score\" in summary[criterion]:\n",
    "                            model_avg_scores[criterion].append(summary[criterion][\"mean_score\"])\n",
    "                    \n",
    "                    # Collect overall score\n",
    "                    if \"overall\" in summary and \"mean_score\" in summary[\"overall\"]:\n",
    "                        model_avg_scores[\"overall\"].append(summary[\"overall\"][\"mean_score\"])\n",
    "                \n",
    "                # Calculate averages across languages\n",
    "                if model_avg_scores[\"overall\"]:\n",
    "                    models.append(model)\n",
    "                    overall_scores.append(np.mean(model_avg_scores[\"overall\"]))\n",
    "                    \n",
    "                    for criterion in self.criteria:\n",
    "                        if model_avg_scores[criterion]:\n",
    "                            criterion_scores[criterion].append(np.mean(model_avg_scores[criterion]))\n",
    "                        else:\n",
    "                            criterion_scores[criterion].append(0)\n",
    "            \n",
    "            if not models:\n",
    "                continue\n",
    "                \n",
    "            # 1. Overall scores\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            bars = plt.bar(models, overall_scores)\n",
    "            \n",
    "            # Color RAG models differently\n",
    "            for i, model in enumerate(models):\n",
    "                if \"_rag\" in model:\n",
    "                    bars[i].set_color('green')\n",
    "                else:\n",
    "                    bars[i].set_color('blue')\n",
    "            \n",
    "            plt.title(f'Overall Model Performance ({judge})')\n",
    "            plt.xlabel('Model')\n",
    "            plt.ylabel('Average Score (lower is better)')\n",
    "            plt.xticks(rotation=45, ha='right')\n",
    "            plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "            \n",
    "            # Invert y-axis (1 is best, 3 is worst)\n",
    "            plt.ylim(3.1, 0.9)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(visuals_dir, f\"{judge}_overall_performance.png\"))\n",
    "            plt.close()\n",
    "            \n",
    "            # 2. Performance by criterion\n",
    "            plt.figure(figsize=(14, 10))\n",
    "            x = np.arange(len(models))\n",
    "            width = 0.2\n",
    "            \n",
    "            for i, criterion in enumerate(self.criteria):\n",
    "                offset = width * (i - len(self.criteria)/2 + 0.5)\n",
    "                plt.bar(x + offset, criterion_scores[criterion], width, label=criterion.capitalize())\n",
    "            \n",
    "            plt.title(f'Model Performance by Criterion ({judge})')\n",
    "            plt.xlabel('Model')\n",
    "            plt.ylabel('Average Score (lower is better)')\n",
    "            plt.xticks(x, models, rotation=45, ha='right')\n",
    "            plt.legend()\n",
    "            plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "            \n",
    "            # Invert y-axis (1 is best, 3 is worst)\n",
    "            plt.ylim(3.1, 0.9)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(visuals_dir, f\"{judge}_criterion_performance.png\"))\n",
    "            plt.close()\n",
    "    \n",
    "    def _plot_rag_comparison(self, results: Dict, visuals_dir: str) -> None:\n",
    "        \"\"\"Plot RAG vs non-RAG comparison.\"\"\"\n",
    "        for judge, judge_data in results[\"rag_vs_nonrag_comparisons\"].items():\n",
    "            for comparison_key, comparison_data in judge_data.items():\n",
    "                base_model = comparison_key.split(\"_vs_\")[0]\n",
    "                rag_model = comparison_key.split(\"_vs_\")[1]\n",
    "                \n",
    "                # Prepare data\n",
    "                languages = []\n",
    "                differences = {c: [] for c in self.criteria + [\"overall\"]}\n",
    "                \n",
    "                for lang, lang_data in comparison_data.items():\n",
    "                    languages.append(lang)\n",
    "                    \n",
    "                    for criterion in self.criteria + [\"overall\"]:\n",
    "                        if criterion in lang_data and \"difference\" in lang_data[criterion]:\n",
    "                            # Negative difference means RAG is better (lower score)\n",
    "                            # Invert to make positive values mean RAG is better\n",
    "                            differences[criterion].append(-lang_data[criterion][\"difference\"])\n",
    "                        else:\n",
    "                            differences[criterion].append(0)\n",
    "                \n",
    "                if not languages:\n",
    "                    continue\n",
    "                \n",
    "                # Plot differences\n",
    "                plt.figure(figsize=(14, 10))\n",
    "                x = np.arange(len(languages))\n",
    "                width = 0.15\n",
    "                \n",
    "                for i, criterion in enumerate(self.criteria + [\"overall\"]):\n",
    "                    offset = width * (i - (len(self.criteria) + 1)/2 + 0.5)\n",
    "                    plt.bar(x + offset, differences[criterion], width, label=criterion.capitalize())\n",
    "                \n",
    "                plt.title(f'RAG vs Non-RAG Comparison: {base_model} vs {rag_model} ({judge})')\n",
    "                plt.xlabel('Language')\n",
    "                plt.ylabel('Improvement with RAG (positive is better)')\n",
    "                plt.xticks(x, languages)\n",
    "                plt.legend()\n",
    "                plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "                \n",
    "                # Add horizontal line at zero\n",
    "                plt.axhline(y=0, color='r', linestyle='-', alpha=0.3)\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(visuals_dir, f\"{judge}_{comparison_key}_difference.png\"))\n",
    "                plt.close()\n",
    "                \n",
    "                # Plot side-by-side scores\n",
    "                plt.figure(figsize=(14, 10))\n",
    "                x = np.arange(len(languages))\n",
    "                width = 0.35\n",
    "                \n",
    "                overall_non_rag = []\n",
    "                overall_rag = []\n",
    "                \n",
    "                for lang in languages:\n",
    "                    if \"overall\" in comparison_data[lang] and \"non_rag_score\" in comparison_data[lang][\"overall\"]:\n",
    "                        overall_non_rag.append(comparison_data[lang][\"overall\"][\"non_rag_score\"])\n",
    "                        overall_rag.append(comparison_data[lang][\"overall\"][\"rag_score\"])\n",
    "                    else:\n",
    "                        overall_non_rag.append(0)\n",
    "                        overall_rag.append(0)\n",
    "                \n",
    "                plt.bar(x - width/2, overall_non_rag, width, label=f'Non-RAG ({base_model})')\n",
    "                plt.bar(x + width/2, overall_rag, width, label=f'RAG ({rag_model})')\n",
    "                \n",
    "                plt.title(f'RAG vs Non-RAG Overall Scores: {base_model} vs {rag_model} ({judge})')\n",
    "                plt.xlabel('Language')\n",
    "                plt.ylabel('Average Score (lower is better)')\n",
    "                plt.xticks(x, languages)\n",
    "                plt.legend()\n",
    "                plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "                \n",
    "                # Invert y-axis (1 is best, 3 is worst)\n",
    "                plt.ylim(3.1, 0.9)\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(visuals_dir, f\"{judge}_{comparison_key}_scores.png\"))\n",
    "                plt.close()\n",
    "    \n",
    "    def _plot_language_performance(self, results: Dict, visuals_dir: str) -> None:\n",
    "        \"\"\"Plot performance by language.\"\"\"\n",
    "        for judge, judge_data in results[\"language_comparisons\"].items():\n",
    "            for model, model_data in judge_data.items():\n",
    "                # Prepare data\n",
    "                criteria = []\n",
    "                best_languages = []\n",
    "                worst_languages = []\n",
    "                \n",
    "                for criterion in self.criteria + [\"overall\"]:\n",
    "                    if criterion in model_data:\n",
    "                        criteria.append(criterion.capitalize())\n",
    "                        best_languages.append(model_data[criterion][\"best_language\"])\n",
    "                        worst_languages.append(model_data[criterion][\"worst_language\"])\n",
    "                \n",
    "                if not criteria:\n",
    "                    continue\n",
    "                \n",
    "                # Plot\n",
    "                plt.figure(figsize=(12, 8))\n",
    "                x = np.arange(len(criteria))\n",
    "                \n",
    "                # Count language frequencies for best and worst\n",
    "                best_counts = {}\n",
    "                worst_counts = {}\n",
    "                \n",
    "                for lang in best_languages:\n",
    "                    best_counts[lang] = best_counts.get(lang, 0) + 1\n",
    "                \n",
    "                for lang in worst_languages:\n",
    "                    worst_counts[lang] = worst_counts.get(lang, 0) + 1\n",
    "                \n",
    "                # Create summary table\n",
    "                cell_text = []\n",
    "                for c, best, worst in zip(criteria, best_languages, worst_languages):\n",
    "                    cell_text.append([c, best, worst])\n",
    "                \n",
    "                plt.table(cellText=cell_text, \n",
    "                         colLabels=['Criterion', 'Best Language', 'Worst Language'],\n",
    "                         loc='center')\n",
    "                \n",
    "                plt.title(f'Language Performance by Criterion: {model} ({judge})')\n",
    "                plt.axis('tight')\n",
    "                plt.axis('off')\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(visuals_dir, f\"{judge}_{model}_language_performance.png\"))\n",
    "                plt.close()\n",
    "                \n",
    "                # Create summary bar chart\n",
    "                languages = sorted(list(set(best_languages + worst_languages)))\n",
    "                best_data = [best_counts.get(lang, 0) for lang in languages]\n",
    "                worst_data = [worst_counts.get(lang, 0) for lang in languages]\n",
    "                \n",
    "                plt.figure(figsize=(10, 6))\n",
    "                x = np.arange(len(languages))\n",
    "                width = 0.35\n",
    "                \n",
    "                plt.bar(x - width/2, best_data, width, label='Best Performance')\n",
    "                plt.bar(x + width/2, worst_data, width, label='Worst Performance')\n",
    "                \n",
    "                plt.title(f'Language Performance Summary: {model} ({judge})')\n",
    "                plt.xlabel('Language')\n",
    "                plt.ylabel('Count of Criteria')\n",
    "                plt.xticks(x, languages)\n",
    "                plt.legend()\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(visuals_dir, f\"{judge}_{model}_language_summary.png\"))\n",
    "                plt.close()\n",
    "    \n",
    "    def _plot_judge_agreement(self, results: Dict, visuals_dir: str) -> None:\n",
    "        \"\"\"Plot agreement between judges.\"\"\"\n",
    "        if \"agreement\" not in results[\"judge_comparisons\"]:\n",
    "            return\n",
    "            \n",
    "        # Collect agreement data\n",
    "        comparisons = []\n",
    "        agreement_data = []\n",
    "        \n",
    "        for comparison_key, comparison_data in results[\"judge_comparisons\"][\"agreement\"].items():\n",
    "            judge1, judge2 = comparison_key.split(\"_vs_\")\n",
    "            \n",
    "            # Calculate average agreement across all models and languages\n",
    "            agreements = []\n",
    "            \n",
    "            for model, model_data in comparison_data.items():\n",
    "                for lang, lang_data in model_data.items():\n",
    "                    if \"overall_agreement\" in lang_data:\n",
    "                        agreements.append(lang_data[\"overall_agreement\"])\n",
    "            \n",
    "            if agreements:\n",
    "                comparisons.append(f\"{judge1} vs {judge2}\")\n",
    "                agreement_data.append(np.mean(agreements))\n",
    "        \n",
    "        if not comparisons:\n",
    "            return\n",
    "            \n",
    "        # Plot overall agreement\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.bar(comparisons, agreement_data)\n",
    "        \n",
    "        plt.title('Judge Agreement')\n",
    "        plt.xlabel('Judge Comparison')\n",
    "        plt.ylabel('Average Agreement (higher is better)')\n",
    "        plt.ylim(0, 1)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(visuals_dir, \"judge_agreement.png\"))\n",
    "        plt.close()\n",
    "        \n",
    "        # Plot agreement by criterion\n",
    "        if \"bias\" in results[\"judge_comparisons\"]:\n",
    "            # Plot judge bias\n",
    "            judge_names = []\n",
    "            bias_values = []\n",
    "            \n",
    "            for judge, bias_data in results[\"judge_comparisons\"][\"bias\"].items():\n",
    "                if \"overall_avg_bias\" in bias_data:\n",
    "                    judge_names.append(judge)\n",
    "                    bias_values.append(bias_data[\"overall_avg_bias\"])\n",
    "            \n",
    "            if judge_names:\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                bars = plt.bar(judge_names, bias_values)\n",
    "                \n",
    "                # Color based on bias direction\n",
    "                for i, value in enumerate(bias_values):\n",
    "                    if value < -0.2:\n",
    "                        bars[i].set_color('red')  # Strict\n",
    "                    elif value > 0.2:\n",
    "                        bars[i].set_color('green')  # Lenient\n",
    "                    else:\n",
    "                        bars[i].set_color('blue')  # Moderate\n",
    "                \n",
    "                plt.title('Judge Bias')\n",
    "                plt.xlabel('Judge')\n",
    "                plt.ylabel('Average Bias (negative = stricter, positive = more lenient)')\n",
    "                \n",
    "                # Add horizontal line at zero\n",
    "                plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(visuals_dir, \"judge_bias.png\"))\n",
    "                plt.close()\n",
    "    \n",
    "    def _generate_markdown_report(self, results: Dict, visuals_dir: str) -> None:\n",
    "        \"\"\"Generate a comprehensive markdown report.\"\"\"\n",
    "        report = []\n",
    "        report.append(\"# Maternal Healthcare Response Evaluation Report\")\n",
    "        report.append(\"\\n## Overview\")\n",
    "        \n",
    "        # Count evaluated models and languages\n",
    "        model_count = 0\n",
    "        language_count = 0\n",
    "        response_count = 0\n",
    "        \n",
    "        for judge, judge_data in results[\"model_evaluations\"].items():\n",
    "            model_count = max(model_count, len(judge_data))\n",
    "            \n",
    "            for model, model_data in judge_data.items():\n",
    "                language_count = max(language_count, len(model_data))\n",
    "                \n",
    "                for lang, lang_data in model_data.items():\n",
    "                    response_count += len(lang_data.get(\"evaluations\", []))\n",
    "        \n",
    "        report.append(f\"\\nThis report presents an evaluation of {model_count} models across {language_count} languages, with a total of {response_count} responses evaluated.\")\n",
    "        \n",
    "        # List judges used\n",
    "        judges = list(results[\"model_evaluations\"].keys())\n",
    "        report.append(f\"\\nEvaluation was performed by {len(judges)} LLM judges: {', '.join(judges)}\")\n",
    "        \n",
    "        # Overall results\n",
    "        report.append(\"\\n## Overall Results\")\n",
    "        \n",
    "        # Best model overall\n",
    "        best_models = {}\n",
    "        \n",
    "        for judge, judge_data in results[\"model_evaluations\"].items():\n",
    "            best_score = float('inf')\n",
    "            best_model = None\n",
    "            \n",
    "            for model, model_data in judge_data.items():\n",
    "                avg_scores = []\n",
    "                \n",
    "                for lang, lang_data in model_data.items():\n",
    "                    if \"summary\" in lang_data and \"overall\" in lang_data[\"summary\"]:\n",
    "                        avg_scores.append(lang_data[\"summary\"][\"overall\"][\"mean_score\"])\n",
    "                \n",
    "                if avg_scores:\n",
    "                    model_avg = np.mean(avg_scores)\n",
    "                    if model_avg < best_score:\n",
    "                        best_score = model_avg\n",
    "                        best_model = model\n",
    "            \n",
    "            if best_model:\n",
    "                best_models[judge] = (best_model, best_score)\n",
    "        \n",
    "        if best_models:\n",
    "            report.append(\"\\n### Best Performing Models by Judge\")\n",
    "            \n",
    "            for judge, (model, score) in best_models.items():\n",
    "                report.append(f\"\\n- **{judge}**: {model} (score: {score:.2f})\")\n",
    "        \n",
    "        # RAG vs non-RAG summary\n",
    "        if \"rag_vs_nonrag_comparisons\" in results:\n",
    "            report.append(\"\\n### RAG vs Non-RAG Performance\")\n",
    "            \n",
    "            for judge, judge_data in results[\"rag_vs_nonrag_comparisons\"].items():\n",
    "                if not judge_data:\n",
    "                    continue\n",
    "                    \n",
    "                report.append(f\"\\n#### Judge: {judge}\")\n",
    "                \n",
    "                for comparison_key, comparison_data in judge_data.items():\n",
    "                    base_model = comparison_key.split(\"_vs_\")[0]\n",
    "                    rag_model = comparison_key.split(\"_vs_\")[1]\n",
    "                    \n",
    "                    # Count wins by language\n",
    "                    rag_wins = 0\n",
    "                    non_rag_wins = 0\n",
    "                    \n",
    "                    for lang, lang_data in comparison_data.items():\n",
    "                        if \"overall\" in lang_data and \"winner\" in lang_data[\"overall\"]:\n",
    "                            winner = lang_data[\"overall\"][\"winner\"]\n",
    "                            if winner == \"rag\":\n",
    "                                rag_wins += 1\n",
    "                            else:\n",
    "                                non_rag_wins += 1\n",
    "                    \n",
    "                    if rag_wins > 0 or non_rag_wins > 0:\n",
    "                        winner = \"RAG\" if rag_wins > non_rag_wins else \"Non-RAG\"\n",
    "                        report.append(f\"\\n**{base_model} vs {rag_model}**: {winner} performs better ({rag_wins} vs {non_rag_wins} languages)\")\n",
    "                        \n",
    "                        # Add image reference\n",
    "                        report.append(f\"\\n![RAG vs Non-RAG Comparison](visuals/{judge}_{comparison_key}_difference.png)\")\n",
    "                        \n",
    "                        # Detail by criterion\n",
    "                        criterion_winners = {c: {\"rag\": 0, \"non_rag\": 0} for c in self.criteria}\n",
    "                        \n",
    "                        for lang, lang_data in comparison_data.items():\n",
    "                            for criterion in self.criteria:\n",
    "                                if criterion in lang_data and \"winner\" in lang_data[criterion]:\n",
    "                                    winner = lang_data[criterion][\"winner\"]\n",
    "                                    criterion_winners[criterion][winner] += 1\n",
    "                        \n",
    "                        report.append(\"\\n**Performance by criterion:**\")\n",
    "                        \n",
    "                        for criterion, wins in criterion_winners.items():\n",
    "                            winner = \"RAG\" if wins[\"rag\"] > wins[\"non_rag\"] else \"Non-RAG\"\n",
    "                            report.append(f\"- **{criterion.capitalize()}**: {winner} is better ({wins['rag']} vs {wins['non_rag']} languages)\")\n",
    "        \n",
    "        # Language-specific analysis\n",
    "        report.append(\"\\n## Language-Specific Analysis\")\n",
    "        \n",
    "        languages = set()\n",
    "        for judge, judge_data in results[\"model_evaluations\"].items():\n",
    "            for model, model_data in judge_data.items():\n",
    "                languages.update(model_data.keys())\n",
    "        \n",
    "        for lang in sorted(languages):\n",
    "            report.append(f\"\\n### {lang.capitalize()}\")\n",
    "            \n",
    "            # Best model for this language\n",
    "            best_by_judge = {}\n",
    "            \n",
    "            for judge, judge_data in results[\"model_evaluations\"].items():\n",
    "                best_score = float('inf')\n",
    "                best_model = None\n",
    "                \n",
    "                for model, model_data in judge_data.items():\n",
    "                    if lang in model_data and \"summary\" in model_data[lang] and \"overall\" in model_data[lang][\"summary\"]:\n",
    "                        score = model_data[lang][\"summary\"][\"overall\"][\"mean_score\"]\n",
    "                        if score < best_score:\n",
    "                            best_score = score\n",
    "                            best_model = model\n",
    "                \n",
    "                if best_model:\n",
    "                    best_by_judge[judge] = (best_model, best_score)\n",
    "            \n",
    "            if best_by_judge:\n",
    "                report.append(\"\\n**Best model by judge:**\")\n",
    "                \n",
    "                for judge, (model, score) in best_by_judge.items():\n",
    "                    report.append(f\"- **{judge}**: {model} (score: {score:.2f})\")\n",
    "            \n",
    "            # RAG vs non-RAG for this language\n",
    "            report.append(\"\\n**RAG vs Non-RAG performance:**\")\n",
    "            \n",
    "            for judge, judge_data in results[\"rag_vs_nonrag_comparisons\"].items():\n",
    "                for comparison_key, comparison_data in judge_data.items():\n",
    "                    if lang in comparison_data and \"overall\" in comparison_data[lang]:\n",
    "                        base_model = comparison_key.split(\"_vs_\")[0]\n",
    "                        rag_model = comparison_key.split(\"_vs_\")[1]\n",
    "                        \n",
    "                        winner = comparison_data[lang][\"overall\"][\"winner\"]\n",
    "                        diff = abs(comparison_data[lang][\"overall\"][\"difference\"])\n",
    "                        \n",
    "                        winner_name = \"RAG\" if winner == \"rag\" else \"Non-RAG\"\n",
    "                        model_name = rag_model if winner == \"rag\" else base_model\n",
    "                        \n",
    "                        report.append(f\"- **{judge}**: {winner_name} ({model_name}) is better by {diff:.2f} points\")\n",
    "        \n",
    "        # Model-specific analysis\n",
    "        report.append(\"\\n## Model-Specific Analysis\")\n",
    "        \n",
    "        models = set()\n",
    "        for judge, judge_data in results[\"model_evaluations\"].items():\n",
    "            models.update(judge_data.keys())\n",
    "        \n",
    "        for model in sorted(models):\n",
    "            report.append(f\"\\n### {model}\")\n",
    "            \n",
    "            # Add image reference\n",
    "            judges_with_model = [j for j in judges if model in results[\"model_evaluations\"][j]]\n",
    "            if judges_with_model:\n",
    "                judge = judges_with_model[0]\n",
    "                report.append(f\"\\n![Model Performance](visuals/{judge}_{model}_language_performance.png)\")\n",
    "            \n",
    "            # Performance across languages\n",
    "            report.append(\"\\n**Performance across languages:**\")\n",
    "            \n",
    "            for judge, judge_data in results[\"model_evaluations\"].items():\n",
    "                if model in judge_data:\n",
    "                    # Average scores by criterion\n",
    "                    criterion_scores = {c: [] for c in self.criteria + [\"overall\"]}\n",
    "                    \n",
    "                    for lang, lang_data in judge_data[model].items():\n",
    "                        summary = lang_data.get(\"summary\", {})\n",
    "                        \n",
    "                        for criterion in self.criteria:\n",
    "                            if criterion in summary and \"mean_score\" in summary[criterion]:\n",
    "                                criterion_scores[criterion].append((lang, summary[criterion][\"mean_score\"]))\n",
    "                        \n",
    "                        if \"overall\" in summary and \"mean_score\" in summary[\"overall\"]:\n",
    "                            criterion_scores[\"overall\"].append((lang, summary[\"overall\"][\"mean_score\"]))\n",
    "                    \n",
    "                    report.append(f\"\\n**Judge: {judge}**\")\n",
    "                    \n",
    "                    for criterion in self.criteria + [\"overall\"]:\n",
    "                        if criterion_scores[criterion]:\n",
    "                            # Sort by score (best first)\n",
    "                            sorted_scores = sorted(criterion_scores[criterion], key=lambda x: x[1])\n",
    "                            best_lang, best_score = sorted_scores[0]\n",
    "                            worst_lang, worst_score = sorted_scores[-1]\n",
    "                            \n",
    "                            avg_score = np.mean([s for _, s in criterion_scores[criterion]])\n",
    "                            \n",
    "                            report.append(f\"- **{criterion.capitalize()}**: Avg score {avg_score:.2f}, Best: {best_lang} ({best_score:.2f}), Worst: {worst_lang} ({worst_score:.2f})\")\n",
    "        \n",
    "        # Judge comparison (if multiple judges)\n",
    "        if len(judges) > 1:\n",
    "            report.append(\"\\n## Judge Comparison\")\n",
    "            \n",
    "            if \"agreement\" in results[\"judge_comparisons\"]:\n",
    "                report.append(\"\\n### Judge Agreement\")\n",
    "                report.append(\"\\n![Judge Agreement](visuals/judge_agreement.png)\")\n",
    "                \n",
    "                for comparison_key, comparison_data in results[\"judge_comparisons\"][\"agreement\"].items():\n",
    "                    judge1, judge2 = comparison_key.split(\"_vs_\")\n",
    "                    \n",
    "                    avg_agreements = []\n",
    "                    for model, model_data in comparison_data.items():\n",
    "                        for lang, lang_data in model_data.items():\n",
    "                            if \"overall_agreement\" in lang_data:\n",
    "                                avg_agreements.append(lang_data[\"overall_agreement\"])\n",
    "                    \n",
    "                    if avg_agreements:\n",
    "                        report.append(f\"\\n**{judge1} vs {judge2}**: Average agreement {np.mean(avg_agreements):.2f}\")\n",
    "            \n",
    "            if \"bias\" in results[\"judge_comparisons\"]:\n",
    "                report.append(\"\\n### Judge Bias\")\n",
    "                report.append(\"\\n![Judge Bias](visuals/judge_bias.png)\")\n",
    "                \n",
    "                for judge, bias_data in results[\"judge_comparisons\"][\"bias\"].items():\n",
    "                    if \"overall_avg_bias\" in bias_data and \"bias_tendency\" in bias_data:\n",
    "                        bias = bias_data[\"overall_avg_bias\"]\n",
    "                        tendency = bias_data[\"bias_tendency\"]\n",
    "                        \n",
    "                        report.append(f\"\\n**{judge}**: {tendency.capitalize()} (bias: {bias:.2f})\")\n",
    "        \n",
    "        # Add recommendations\n",
    "        report.append(\"\\n## Recommendations\")\n",
    "        \n",
    "        # Determine best model overall\n",
    "        best_model_overall = None\n",
    "        best_score_overall = float('inf')\n",
    "        \n",
    "        for judge, (model, score) in best_models.items():\n",
    "            if score < best_score_overall:\n",
    "                best_score_overall = score\n",
    "                best_model_overall = model\n",
    "        \n",
    "        if best_model_overall:\n",
    "            report.append(f\"\\n- **Best model overall**: {best_model_overall}\")\n",
    "        \n",
    "        # RAG vs non-RAG recommendation\n",
    "        rag_better_count = 0\n",
    "        non_rag_better_count = 0\n",
    "        \n",
    "        for judge, judge_data in results[\"rag_vs_nonrag_comparisons\"].items():\n",
    "            for comparison_key, comparison_data in judge_data.items():\n",
    "                rag_wins = 0\n",
    "                non_rag_wins = 0\n",
    "                \n",
    "                for lang, lang_data in comparison_data.items():\n",
    "                    if \"overall\" in lang_data and \"winner\" in lang_data[\"overall\"]:\n",
    "                        winner = lang_data[\"overall\"][\"winner\"]\n",
    "                        if winner == \"rag\":\n",
    "                            rag_wins += 1\n",
    "                        else:\n",
    "                            non_rag_wins += 1\n",
    "                \n",
    "                if rag_wins > non_rag_wins:\n",
    "                    rag_better_count += 1\n",
    "                elif non_rag_wins > rag_wins:\n",
    "                    non_rag_better_count += 1\n",
    "        \n",
    "        if rag_better_count > non_rag_better_count:\n",
    "            report.append(\"\\n- **RAG approach recommended**: RAG models generally perform better across languages and criteria.\")\n",
    "        elif non_rag_better_count > rag_better_count:\n",
    "            report.append(\"\\n- **Non-RAG approach recommended**: Non-RAG models generally perform better across languages and criteria.\")\n",
    "        else:\n",
    "            report.append(\"\\n- **Mixed approach recommended**: RAG and Non-RAG models show similar performance overall. Consider using RAG for specific languages or criteria where it shows advantage.\")\n",
    "        \n",
    "        # Language-specific recommendations\n",
    "        report.append(\"\\n### Language-Specific Recommendations\")\n",
    "        \n",
    "        for lang in sorted(languages):\n",
    "            rag_advantage = True\n",
    "            non_rag_advantage = True\n",
    "            \n",
    "            for judge, judge_data in results[\"rag_vs_nonrag_comparisons\"].items():\n",
    "                for comparison_key, comparison_data in judge_data.items():\n",
    "                    if lang in comparison_data and \"overall\" in comparison_data[lang]:\n",
    "                        winner = comparison_data[lang][\"overall\"][\"winner\"]\n",
    "                        if winner == \"rag\":\n",
    "                            non_rag_advantage = False\n",
    "                        else:\n",
    "                            rag_advantage = False\n",
    "            \n",
    "            if rag_advantage and not non_rag_advantage:\n",
    "                report.append(f\"\\n- **{lang.capitalize()}**: Use RAG approach\")\n",
    "            elif non_rag_advantage and not rag_advantage:\n",
    "                report.append(f\"\\n- **{lang.capitalize()}**: Use Non-RAG approach\")\n",
    "            else:\n",
    "                # Check criterion-specific advantages\n",
    "                criterion_advantages = {c: {\"rag\": 0, \"non_rag\": 0} for c in self.criteria}\n",
    "                \n",
    "                for judge, judge_data in results[\"rag_vs_nonrag_comparisons\"].items():\n",
    "                    for comparison_key, comparison_data in judge_data.items():\n",
    "                        if lang in comparison_data:\n",
    "                            for criterion in self.criteria:\n",
    "                                if criterion in comparison_data[lang] and \"winner\" in comparison_data[lang][criterion]:\n",
    "                                    winner = comparison_data[lang][criterion][\"winner\"]\n",
    "                                    criterion_advantages[criterion][winner] += 1\n",
    "                \n",
    "                # Determine which criteria favor which approach\n",
    "                rag_better_criteria = []\n",
    "                non_rag_better_criteria = []\n",
    "                \n",
    "                for criterion, advantages in criterion_advantages.items():\n",
    "                    if advantages[\"rag\"] > advantages[\"non_rag\"]:\n",
    "                        rag_better_criteria.append(criterion)\n",
    "                    elif advantages[\"non_rag\"] > advantages[\"rag\"]:\n",
    "                        non_rag_better_criteria.append(criterion)\n",
    "                \n",
    "                if rag_better_criteria and non_rag_better_criteria:\n",
    "                    report.append(f\"\\n- **{lang.capitalize()}**: Consider RAG for {', '.join(rag_better_criteria)} and Non-RAG for {', '.join(non_rag_better_criteria)}\")\n",
    "                else:\n",
    "                    report.append(f\"\\n- **{lang.capitalize()}**: Either approach works similarly well\")\n",
    "        \n",
    "        # Add methodology\n",
    "        report.append(\"\\n## Methodology\")\n",
    "        report.append(\"\\nThis evaluation uses LLM judges to assess maternal healthcare responses across four criteria:\")\n",
    "        report.append(\"\\n1. **Medical Correctness**: Are the medical claims in the response accurate? (1 = all correct, 3 = not correct)\")\n",
    "        report.append(\"\\n2. **Completeness**: Does the answer cover all necessary information? (1 = covers everything, 3 = omits significant information)\")\n",
    "        report.append(\"\\n3. **Language Clarity**: Is the response clear for users with average literacy? (1 = completely understandable, 3 = unacceptable)\")\n",
    "        report.append(\"\\n4. **Cultural Appropriateness**: Is the response appropriate for the cultural context? (1 = completely appropriate, 3 = inappropriate)\")\n",
    "        \n",
    "        report.append(\"\\nFor all scores, **lower is better** (1 is the best possible score, 3 is the worst).\")\n",
    "        \n",
    "        # Save report\n",
    "        report_path = os.path.join(self.output_dir, \"evaluation_report.md\")\n",
    "        with open(report_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"\\n\".join(report))\n",
    "        \n",
    "        logging.info(f\"Generated markdown report at {report_path}\")\n",
    "        \n",
    "        # Copy images to make relative paths work\n",
    "        import shutil\n",
    "        for file in os.listdir(visuals_dir):\n",
    "            if file.endswith('.png'):\n",
    "                shutil.copy(\n",
    "                    os.path.join(visuals_dir, file),\n",
    "                    os.path.join(self.output_dir, file)\n",
    "                )\n",
    "\n",
    "\n",
    "# Function for easy usage\n",
    "def evaluate_maternal_healthcare_responses(\n",
    "    json_files: Dict[str, str],\n",
    "    judges_to_use: List[str] = [\"gemini-1.5-pro\"],\n",
    "    languages: List[str] = [\"english\", \"hindi\", \"assamese\", \"hinglish\"],\n",
    "    output_dir: str = \"./llm_judge_evaluation\",\n",
    "    sample_size: Optional[int] = None\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate maternal healthcare model responses using LLM judges.\n",
    "    \n",
    "    Args:\n",
    "        json_files: Dictionary mapping model names to JSON file paths\n",
    "        judges_to_use: List of LLM judges to use for evaluation\n",
    "        languages: List of languages to evaluate\n",
    "        output_dir: Directory to save evaluation results\n",
    "        sample_size: Number of samples to evaluate per language per model (None for all)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with evaluation results\n",
    "    \"\"\"\n",
    "    # Initialize evaluator\n",
    "    evaluator = LLMJudgeEvaluator(output_dir=output_dir)\n",
    "    \n",
    "    # Run evaluation\n",
    "    results = evaluator.evaluate(\n",
    "        json_files=json_files,\n",
    "        judges_to_use=judges_to_use,\n",
    "        languages=languages,\n",
    "        sample_size=sample_size\n",
    "    )\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Specify paths to your model response files\n",
    "    json_files = {\n",
    "        \"gpt-4-turbo\": \"/home/vidhij2/nivi/test_docs/gpt-4-turbo_rag.json\",\n",
    "        \"gpt-4-turbo_rag\": \"/home/vidhij2/nivi/test_docs/gpt-4-turbo_rag.json\",\n",
    "        \"mixtral\": \"/home/vidhij2/nivi/test_docs/mixtral_rag.json\",\n",
    "        \"mixtral_rag\": \"/home/vidhij2/nivi/test_docs/mixtral_rag.json\",\n",
    "        \"llama\": \"/home/vidhij2/nivi/test_docs/llama_rag.json\",\n",
    "        \"llama_rag\": \"/home/vidhij2/nivi/test_docs/llama_rag.json\"\n",
    "    }\n",
    "    \n",
    "    # Run evaluation\n",
    "    results = evaluate_maternal_healthcare_responses(\n",
    "        json_files=json_files,\n",
    "        judges_to_use=[\"gemini-pro\"],  # You can add more judges like \"gemini-pro\" if you have access\n",
    "        languages=[\"english\", \"hindi\", \"assamese\", \"hinglish\"],\n",
    "        output_dir=\"./maternal_healthcare_evaluation_new\",\n",
    "        sample_size=20  # Set to None to evaluate all samples\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "GEMINI_API_KEY = \"AIzaSyDlArIQ19XCh_aYVi5LdTk6waDgkEIGaWM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(os.getenv(\"GEMINI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-generativeai\n",
      "  Downloading google_generativeai-0.8.4-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting google-ai-generativelanguage==0.6.15 (from google-generativeai)\n",
      "  Downloading google_ai_generativelanguage-0.6.15-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: google-api-core in /home/vidhij2/.myenv/lib/python3.9/site-packages (from google-generativeai) (2.24.2)\n",
      "Collecting google-api-python-client (from google-generativeai)\n",
      "  Downloading google_api_python_client-2.166.0-py2.py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in /home/vidhij2/.myenv/lib/python3.9/site-packages (from google-generativeai) (2.38.0)\n",
      "Requirement already satisfied: protobuf in /home/vidhij2/.myenv/lib/python3.9/site-packages (from google-generativeai) (5.29.4)\n",
      "Requirement already satisfied: pydantic in /home/vidhij2/.myenv/lib/python3.9/site-packages (from google-generativeai) (2.10.6)\n",
      "Requirement already satisfied: tqdm in /home/vidhij2/.myenv/lib/python3.9/site-packages (from google-generativeai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions in /home/vidhij2/.myenv/lib/python3.9/site-packages (from google-generativeai) (4.12.2)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /home/vidhij2/.myenv/lib/python3.9/site-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /home/vidhij2/.myenv/lib/python3.9/site-packages (from google-api-core->google-generativeai) (1.69.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /home/vidhij2/.myenv/lib/python3.9/site-packages (from google-api-core->google-generativeai) (2.32.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/vidhij2/.myenv/lib/python3.9/site-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/vidhij2/.myenv/lib/python3.9/site-packages (from google-auth>=2.15.0->google-generativeai) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/vidhij2/.myenv/lib/python3.9/site-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\n",
      "Collecting httplib2<1.0.0,>=0.19.0 (from google-api-python-client->google-generativeai)\n",
      "  Downloading httplib2-0.22.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting google-auth-httplib2<1.0.0,>=0.2.0 (from google-api-python-client->google-generativeai)\n",
      "  Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting uritemplate<5,>=3.0.1 (from google-api-python-client->google-generativeai)\n",
      "  Downloading uritemplate-4.1.1-py2.py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/vidhij2/.myenv/lib/python3.9/site-packages (from pydantic->google-generativeai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /home/vidhij2/.myenv/lib/python3.9/site-packages (from pydantic->google-generativeai) (2.27.2)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /home/vidhij2/.myenv/lib/python3.9/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /home/vidhij2/.myenv/lib/python3.9/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /home/vidhij2/.myenv/lib/python3.9/site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /home/vidhij2/.myenv/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/vidhij2/.myenv/lib/python3.9/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/vidhij2/.myenv/lib/python3.9/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/vidhij2/.myenv/lib/python3.9/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/vidhij2/.myenv/lib/python3.9/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.1.31)\n",
      "Downloading google_generativeai-0.8.4-py3-none-any.whl (175 kB)\n",
      "Downloading google_ai_generativelanguage-0.6.15-py3-none-any.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_api_python_client-2.166.0-py2.py3-none-any.whl (13.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Downloading httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "Downloading uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
      "Installing collected packages: uritemplate, httplib2, google-auth-httplib2, google-api-python-client, google-ai-generativelanguage, google-generativeai\n",
      "  Attempting uninstall: google-ai-generativelanguage\n",
      "    Found existing installation: google-ai-generativelanguage 0.6.17\n",
      "    Uninstalling google-ai-generativelanguage-0.6.17:\n",
      "      Successfully uninstalled google-ai-generativelanguage-0.6.17\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-google-genai 2.1.1 requires google-ai-generativelanguage<0.7.0,>=0.6.16, but you have google-ai-generativelanguage 0.6.15 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed google-ai-generativelanguage-0.6.15 google-api-python-client-2.166.0 google-auth-httplib2-0.2.0 google-generativeai-0.8.4 httplib2-0.22.0 uritemplate-4.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_evaluations': {'gpt-4': {'gpt-4-turbo': {'english': {'evaluations': [{'correctness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'completeness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'clarity': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'cultural_appropriateness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'question': 'My baby weight is 452 gram in 22weeks..is this normal',\n",
       "       'response': \"Based on the information provided, I don't have specific guidelines regarding fetal weight at 22 weeks to determine if 452 grams is within a normal range. It's important to discuss this with your healthcare provider, who can assess the growth of your baby in the context of overall health and development during your pregnancy. They can provide you with the best advice and care. Remember, regular prenatal check-ups are crucial for monitoring the health and development of your baby. If you have any concerns about your pregnancy, it's always best to consult with your healthcare provider.\",\n",
       "       'language': 'english',\n",
       "       'judge': 'gpt-4'},\n",
       "      {'correctness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'completeness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'clarity': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'cultural_appropriateness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'question': 'I have seen prickly heat and itching on some part of my body... What should I do?',\n",
       "       'response': \"Prickly heat and itching are common during pregnancy due to hormonal changes and increased sweating. Here are a few tips to help manage the discomfort:\\n\\n1. Keep the affected area cool and dry. Try to stay in cooler environments and avoid excessive heat.\\n2. Wear loose, breathable clothing made from natural fibers like cotton, which allows your skin to breathe and reduces irritation.\\n3. Bathe in cool water and use a mild soap that is gentle on the skin.\\n4. You can apply a calamine lotion or an over-the-counter hydrocortisone cream to soothe itching. However, it's always best to consult with your healthcare provider before using any medications.\\n5. Stay hydrated by drinking plenty of water, as this helps to regulate your body temperature.\\n\\nIf the itching is severe, widespread, or accompanied by other symptoms like jaundice, it's important to consult your healthcare provider for further evaluation. Remember, it's always best to seek personalized advice from your healthcare provider regarding any health concerns during pregnancy.\",\n",
       "       'language': 'english',\n",
       "       'judge': 'gpt-4'},\n",
       "      {'correctness': {'score': 1,\n",
       "        'justification': \"The response accurately reflects medical advice regarding fetal movements. It correctly states that it's normal to not always feel the baby move, especially in the earlier stages of pregnancy when movements are less noticeable. It also correctly advises on the importance of monitoring movements and seeking healthcare advice if there's a noticeable decrease in movement, which could indicate a complication.\",\n",
       "        'problematic_claims': []},\n",
       "       'completeness': {'score': 2,\n",
       "        'justification': 'The response covers the most critical advice for a pregnant person concerned about fetal movements. However, it could be improved by including specific guidance on how to monitor fetal movements (e.g., kick counts) and what constitutes a significant decrease in movements. Additionally, mentioning the typical timeframe when movements become more regular (usually around 28 weeks) would provide more context.',\n",
       "        'missing_information': ['Specific guidance on how to monitor fetal movements, such as kick counts.',\n",
       "         'Information on when fetal movements typically become more regular and noticeable.']},\n",
       "       'clarity': {'score': 1,\n",
       "        'justification': 'The language used in the response is clear and straightforward, making it easily understandable for a user of average literacy. The advice is presented in a logical sequence, starting with reassurance, followed by cautionary advice, and ending with a clear call to action.'},\n",
       "       'cultural_appropriateness': {'score': 1,\n",
       "        'justification': 'The response is culturally appropriate for an English-speaking audience. It uses neutral and respectful language that is considerate of the concerns of a pregnant person. There are no cultural references or idioms that could be misunderstood or considered inappropriate.'},\n",
       "       'question': 'Sometimes I don’t feel the baby’s move ..is it okay or something else?',\n",
       "       'response': \"It's normal to occasionally not feel your baby move, especially earlier in pregnancy when movements can be less noticeable. However, if you notice a significant decrease in your baby's movements or if movements stop, this could be a sign of a serious complication. It's important to monitor the baby's movements, especially as you get further along in your pregnancy.\\n\\nIf you're concerned about the frequency or pattern of your baby's movements, it's best to contact your healthcare provider immediately for further advice. They can perform checks to ensure everything is progressing well with your pregnancy. Remember, it's always better to be cautious and seek advice if you're unsure or worried.\",\n",
       "       'language': 'english',\n",
       "       'judge': 'gpt-4'}],\n",
       "     'summary': {'correctness': {'mean_score': 2.3333333333333335,\n",
       "       'median_score': 3.0,\n",
       "       'distribution': {'1': 1, '2': 0, '3': 2},\n",
       "       'count': 3,\n",
       "       'percentage': {'1': 33.33333333333333,\n",
       "        '2': 0.0,\n",
       "        '3': 66.66666666666666}},\n",
       "      'completeness': {'mean_score': 2.6666666666666665,\n",
       "       'median_score': 3.0,\n",
       "       'distribution': {'1': 0, '2': 1, '3': 2},\n",
       "       'count': 3,\n",
       "       'percentage': {'1': 0.0,\n",
       "        '2': 33.33333333333333,\n",
       "        '3': 66.66666666666666}},\n",
       "      'clarity': {'mean_score': 2.3333333333333335,\n",
       "       'median_score': 3.0,\n",
       "       'distribution': {'1': 1, '2': 0, '3': 2},\n",
       "       'count': 3,\n",
       "       'percentage': {'1': 33.33333333333333,\n",
       "        '2': 0.0,\n",
       "        '3': 66.66666666666666}},\n",
       "      'cultural_appropriateness': {'mean_score': 2.3333333333333335,\n",
       "       'median_score': 3.0,\n",
       "       'distribution': {'1': 1, '2': 0, '3': 2},\n",
       "       'count': 3,\n",
       "       'percentage': {'1': 33.33333333333333,\n",
       "        '2': 0.0,\n",
       "        '3': 66.66666666666666}},\n",
       "      'overall': {'mean_score': 2.416666666666667}}},\n",
       "    'hindi': {'evaluations': [{'correctness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'completeness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'clarity': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'cultural_appropriateness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'question': 'जयदा टाइम तक बैठने से चक्कर आना खड़ा होने से पेट दर्द होने लगता हैमेरे को कौन सी टेबलेट खानी चाहिए अभी सट्टा मा लग चुका है',\n",
       "       'response': 'मुझे आपके प्रश्न का उत्तर देने के लिए पर्याप्त जानकारी नहीं है क्योंकि प्रदान की गई दिशानिर्देशों में इस स्थिति के लिए विशेष जानकारी शामिल नहीं है। चक्कर आना और खड़े होने पर पेट दर्द होना जैसी समस्याएं गंभीर हो सकती हैं, इसलिए मैं आपको सलाह दूंगा कि आप जल्द से जल्द एक स्वास्थ्य सेवा प्रदाता से संपर्क करें। वे आपकी विशेष स्थिति का मूल्यांकन कर सकते हैं और उचित उपचार प्रदान कर सकते हैं। अपनी सुरक्षा के लिए, कृपया बिना चिकित्सकीय सलाह के कोई दवा न लें।',\n",
       "       'language': 'hindi',\n",
       "       'judge': 'gpt-4'},\n",
       "      {'correctness': {'score': 1,\n",
       "        'justification': \"The AI's response is medically correct. It accurately describes that common colds usually do not have a severe impact on children and advises monitoring for severe symptoms that would necessitate medical attention.\",\n",
       "        'problematic_claims': []},\n",
       "       'completeness': {'score': 2,\n",
       "        'justification': 'While the response covers the essential aspects of monitoring a child with a cold for severe symptoms, it lacks specific advice on home care measures for managing mild symptoms and when to consider returning to a healthcare provider if symptoms persist or worsen.',\n",
       "        'missing_information': ['Advice on managing mild symptoms at home, such as hydration and rest.',\n",
       "         'Guidance on when to return to a healthcare provider if symptoms do not improve or worsen.']},\n",
       "       'clarity': {'score': 1,\n",
       "        'justification': 'The response is written in clear, simple Hindi that should be easily understandable to a user of average literacy.'},\n",
       "       'cultural_appropriateness': {'score': 1,\n",
       "        'justification': \"The response is culturally appropriate for a Hindi-speaking audience. It uses language and examples that are likely to be understood and relevant to the user's context.\"},\n",
       "       'question': 'सर्दी -जुकाम  हो रहा  है।इसका बच्चे पर तो कोई प्रभाव  नहीं  होगा ।',\n",
       "       'response': 'सर्दी-जुकाम आमतौर पर हल्की स्थिति होती है और यह अक्सर बच्चों पर गंभीर प्रभाव नहीं डालती। हालांकि, यह महत्वपूर्ण है कि आप बच्चे की निगरानी करें और अगर बच्चे में निम्नलिखित लक्षण दिखाई दें तो तुरंत स्वास्थ्य सेवा प्रदाता से संपर्क करें:\\n\\n- तेज बुखार\\n- सांस लेने में कठिनाई\\n- निरंतर रोना या असामान्य शांत होना\\n- खाना पीना कम कर देना\\n\\nअगर आपको ये लक्षण नहीं दिखाई देते हैं, तो आमतौर पर सर्दी-जुकाम से बच्चे को कोई गंभीर समस्या नहीं होती। फिर भी, अगर आपको कोई चिंता है तो डॉक्टर से सलाह लेना उचित होगा।',\n",
       "       'language': 'hindi',\n",
       "       'judge': 'gpt-4'}],\n",
       "     'summary': {'correctness': {'mean_score': 2.0,\n",
       "       'median_score': 2.0,\n",
       "       'distribution': {'1': 1, '2': 0, '3': 1},\n",
       "       'count': 2,\n",
       "       'percentage': {'1': 50.0, '2': 0.0, '3': 50.0}},\n",
       "      'completeness': {'mean_score': 2.5,\n",
       "       'median_score': 2.5,\n",
       "       'distribution': {'1': 0, '2': 1, '3': 1},\n",
       "       'count': 2,\n",
       "       'percentage': {'1': 0.0, '2': 50.0, '3': 50.0}},\n",
       "      'clarity': {'mean_score': 2.0,\n",
       "       'median_score': 2.0,\n",
       "       'distribution': {'1': 1, '2': 0, '3': 1},\n",
       "       'count': 2,\n",
       "       'percentage': {'1': 50.0, '2': 0.0, '3': 50.0}},\n",
       "      'cultural_appropriateness': {'mean_score': 2.0,\n",
       "       'median_score': 2.0,\n",
       "       'distribution': {'1': 1, '2': 0, '3': 1},\n",
       "       'count': 2,\n",
       "       'percentage': {'1': 50.0, '2': 0.0, '3': 50.0}},\n",
       "      'overall': {'mean_score': 2.125}}},\n",
       "    'hinglish': {'evaluations': [{'correctness': {'score': 1,\n",
       "        'justification': 'Each claim in the response is medically correct. Keeping the child warm, maintaining cleanliness, providing skin-to-skin contact, and seeking medical help for severe symptoms or difficulty breathing are all appropriate measures for a child with a cold.',\n",
       "        'problematic_claims': []},\n",
       "       'completeness': {'score': 2,\n",
       "        'justification': \"The response covers most of the important information for managing a child's cold at home. However, it lacks specific advice on hydration and the use of saline drops for nasal congestion, which are commonly recommended for children with colds.\",\n",
       "        'missing_information': ['Advice on keeping the child hydrated',\n",
       "         'Recommendation for using saline drops for nasal congestion if applicable']},\n",
       "       'clarity': {'score': 1,\n",
       "        'justification': 'The response is written in Hinglish, which is a mix of Hindi and English. This language choice is clear and understandable for the intended audience, making the medical advice accessible to a user of average literacy in the cultural context.'},\n",
       "       'cultural_appropriateness': {'score': 1,\n",
       "        'justification': 'The response is culturally appropriate for a Hinglish-speaking user. It uses a familiar language blend and provides advice that is practical and feasible within the cultural context, such as kangaroo mother care, which is a widely accepted practice.'},\n",
       "       'question': 'Bacche ko Tej Sardi ho to kaise usko maintain Karen',\n",
       "       'response': 'Agar bacche ko tej sardi ho rahi hai, toh yeh kuch upay hain jo aap kar sakti hain:\\n\\n1. Bacche ko garam rakhein: Bacche ko acche se cover karein, khaaskar uske sir aur pair ko. Thandi mausam mein, bacche ko extra kapde pahnaayein.\\n\\n2. Bacche ke sone ka sthan garam rakhein: Bacche ko sone ke liye ek garam aur sukhad jagah pradan karein. Agar aapke paas radiant warmer hai toh uska istemal karein.\\n\\n3. Swachhta ka dhyan rakhein: Bacche ko sambhalte waqt haatho ko saaf rakhein aur bacche ko bhi saaf rakhein.\\n\\n4. Bacche ko skin-to-skin contact dein: Agar bacche ko sardi zyada hai toh usko apni skin se lagaye rakhna (kangaroo mother care) madadgar ho sakta hai kyunki yeh bacche ko garam rakhta hai.\\n\\n5. Agar bacche ko saans lene mein dikkat ho rahi hai ya uske anya gambhir lakshan dikhayi de rahe hain, toh turant swasthya seva pradan karne wale se sampark karein.\\n\\nYeh upay aam taur par madadgar hote hain, lekin agar aapko lagta hai ki sthiti gambhir hai, toh kripya turant doctor se sampark karein.',\n",
       "       'language': 'hinglish',\n",
       "       'judge': 'gpt-4'},\n",
       "      {'correctness': {'score': 1,\n",
       "        'justification': 'The response correctly identifies that not experiencing vomiting or morning sickness during the first 2.5 months of pregnancy can be normal, as symptoms can vary widely among pregnant individuals. It also appropriately advises consulting a healthcare provider if there are other concerning symptoms, which is medically sound advice.',\n",
       "        'problematic_claims': []},\n",
       "       'completeness': {'score': 2,\n",
       "        'justification': 'While the response covers the primary concern about the absence of vomiting or morning sickness and advises seeking medical advice for other symptoms, it could have included information on what symptoms might warrant immediate consultation (e.g., severe abdominal pain, bleeding, or signs of dehydration) to provide a more comprehensive overview.',\n",
       "        'missing_information': ['Specific symptoms that should prompt immediate medical consultation.',\n",
       "         'Encouragement to maintain regular prenatal check-ups even if no concerning symptoms are present.']},\n",
       "       'clarity': {'score': 1,\n",
       "        'justification': 'The response is written in clear, simple language that is likely understandable to a user of average literacy. The use of Hinglish is appropriate for the intended audience, making the advice accessible and relatable.'},\n",
       "       'cultural_appropriateness': {'score': 1,\n",
       "        'justification': \"The response is culturally appropriate for a Hinglish-speaking user. It addresses the user's concern in a respectful manner without making assumptions about their knowledge or beliefs regarding pregnancy symptoms, which aligns well with cultural sensitivity.\"},\n",
       "       'question': '2.5 months hogae h pregnancy ko but vomet ya kch ni hora mjhe',\n",
       "       'response': 'Aapko chinta karne ki zarurat nahi hai agar aapko pregnancy ke 2.5 mahine mein vomiting ya koi aur takleef nahi ho rahi hai. Har mahila ki pregnancy alag hoti hai, aur kuch mahilao ko vomiting ya morning sickness nahi hoti. Yeh bilkul normal hai. Lekin, agar aapko koi aur symptoms dikhayi de rahe hain jo aapko pareshan kar rahe hain, toh aapko apne healthcare provider se zaroor consult karna chahiye. Woh aapki aur aapke baby ki health ka sahi se assessment kar sakenge. Aapka dhyan rakhna aur regular check-ups karwana important hai.',\n",
       "       'language': 'hinglish',\n",
       "       'judge': 'gpt-4'},\n",
       "      {'correctness': {'score': 1,\n",
       "        'justification': \"The AI's response is medically correct. It accurately states that mangoes are nutritious, being rich in vitamin C and fiber, and can be consumed during pregnancy if there are no allergies or issues with sugar levels. It also correctly advises moderation and consultation with a healthcare provider, which is prudent advice for any dietary consideration during pregnancy.\",\n",
       "        'problematic_claims': []},\n",
       "       'completeness': {'score': 2,\n",
       "        'justification': 'While the response covers the safety and nutritional benefits of consuming mangoes during pregnancy, it lacks specific guidance on quantity (how much is considered moderate) and does not mention the importance of consuming washed and ripe mangoes to avoid potential health risks like infections from pesticides or parasites.',\n",
       "        'missing_information': ['Guidance on the recommended quantity or serving size of mangoes.',\n",
       "         'Advice to consume washed and ripe mangoes to minimize health risks.']},\n",
       "       'clarity': {'score': 1,\n",
       "        'justification': 'The response is clear and understandable, using simple language that is likely to be accessible to a user of average literacy. The use of Hinglish is appropriate for the intended audience, making the information approachable and easy to comprehend.'},\n",
       "       'cultural_appropriateness': {'score': 1,\n",
       "        'justification': 'The response is culturally appropriate for a Hinglish-speaking user. It addresses the query in a manner that is respectful and considerate of the cultural context, including the popularity and dietary inclusion of mangoes in the region.'},\n",
       "       'question': 'Pragnancy me mango 🥭 kitna mntra me kha skti hu',\n",
       "       'response': 'Mujhe mango ke sevan ke liye kisi specific mahine ke bare mein jankari dene ke liye paryapt suchna nahi hai, jo di gayi guidelines mein shaamil hai. Aam tor par, mango ek poshtik phal hai jo vitamin C aur fiber se bharpoor hota hai. Garbhavastha ke dauran, yadi aapko mango se koi allergy nahi hai aur aapka sugar level samanya hai, to aap iska sevan kar sakti hain, lekin yah sunishchit karein ki aap ise santulit matra mein khayein. Har vyakti ki nutritional zarooratein alag hoti hain, isliye apne aahar mein mango shamil karne se pehle ek swasthya visheshagya ya apne doctor se salah zaroor lein.',\n",
       "       'language': 'hinglish',\n",
       "       'judge': 'gpt-4'},\n",
       "      {'correctness': {'score': 1,\n",
       "        'justification': 'The AI response accurately identifies common causes of bleeding in early pregnancy, such as threatened abortion, spontaneous abortion, ectopic pregnancy, and hydatidiform mole. It correctly advises seeking medical attention if bleeding occurs, which is essential for proper evaluation and management.',\n",
       "        'problematic_claims': []},\n",
       "       'completeness': {'score': 2,\n",
       "        'justification': 'While the response covers important causes of bleeding in early pregnancy and the crucial advice to seek medical attention, it does not mention other possible causes of bleeding later in pregnancy, such as placenta previa, placental abruption, or the onset of labor. Including information about when bleeding might be considered more normal, such as very light spotting in early pregnancy, could also be helpful.',\n",
       "        'missing_information': ['Other potential causes of bleeding later in pregnancy (e.g., placenta previa, placental abruption).',\n",
       "         'A brief mention that very light spotting can sometimes occur in early pregnancy and may not always signify a serious problem, but medical advice should still be sought.']},\n",
       "       'clarity': {'score': 1,\n",
       "        'justification': 'The response is written in Hinglish, which is a blend of Hindi and English commonly used in certain regions. It uses clear and straightforward language to convey the message, making it understandable for a user of average literacy familiar with this linguistic style.'},\n",
       "       'cultural_appropriateness': {'score': 1,\n",
       "        'justification': 'The use of Hinglish is culturally appropriate for the intended audience, as it reflects a common mode of communication in many parts of India and among Indian communities. The response addresses a sensitive topic with the necessary seriousness and encourages seeking professional medical advice, which is culturally respectful and appropriate.'},\n",
       "       'question': 'Kya pregnancy m bleeding hoti h kya yeh normal h',\n",
       "       'response': 'Bleeding ke dauran pregnancy mein hamesha ek chinta ka vishay hota hai aur ise halka nahi lena chahiye. Pregnancy ke shuruaati 20 hafte ke dauran, bleeding ka karan ho sakta hai dhamki di hui garbhpaat (threatened abortion), swatah garbhpaat (spontaneous abortion), ectopic pregnancy ya hydatidiform mole. Agar aapko bleeding ho rahi hai, to yeh zaroori hai ki aap turant apne swasthya seva pradaata se sampark karein.\\n\\nBleeding hamesha normal nahi hoti hai aur iska karan aur prabandhan vyakti ke aadhaar par alag-alag ho sakta hai. Isliye, yadi aapko pregnancy ke dauran kisi bhi prakar ki bleeding ho rahi hai, to kripya apne doctor ya swasthya seva pradaata se sampark karein taki ve sthiti ka sahi mulyankan kar sakein aur uchit salah de sakein.',\n",
       "       'language': 'hinglish',\n",
       "       'judge': 'gpt-4'},\n",
       "      {'correctness': {'score': 1,\n",
       "        'justification': 'The response accurately advises the user to consult with their healthcare provider for personalized advice, which is medically correct. It correctly identifies that thyroid issues can impact overall health and potentially breastfeeding but does not make unfounded claims about the direct impact on breast milk without evidence.',\n",
       "        'problematic_claims': []},\n",
       "       'completeness': {'score': 2,\n",
       "        'justification': 'While the response correctly advises consultation with a healthcare provider, it lacks specific information on how thyroid conditions might affect breastfeeding or breast milk composition. For instance, it could mention that thyroid conditions can affect milk supply or the importance of monitoring thyroid levels for breastfeeding mothers.',\n",
       "        'missing_information': ['Possible effects of thyroid conditions on milk supply.',\n",
       "         'Importance of monitoring thyroid levels for breastfeeding mothers.']},\n",
       "       'clarity': {'score': 1,\n",
       "        'justification': 'The response is clear and understandable, using straightforward language that should be accessible to a user of average literacy, especially within the context of Hinglish-speaking users.'},\n",
       "       'cultural_appropriateness': {'score': 1,\n",
       "        'justification': \"The response is culturally appropriate for a Hinglish-speaking user, using a respectful tone and language that blends Hindi and English, which is commonly used in many parts of India. It shows sensitivity to the user's potential concerns about breastfeeding and health.\"},\n",
       "       'question': 'mujhe thyroid h to kya isse bhi breast milk pe asar hota h',\n",
       "       'response': 'Mujhe aapke sawaal ka jawab dene ke liye di gayi jaankari mein koi seedha sandarbh nahi mila hai ki thyroid ki samasya se breast milk par kya asar padta hai. Yeh zaroori hai ki aap apne healthcare provider se is vishay mein vistar se baat karein, kyunki ve aapki personal medical condition ko samajhne mein madad kar sakte hain aur sahi salah de sakte hain. Thyroid ki samasyaon ka ilaaj aur unka prabandhan zaroori hai, aur yeh aapke overall swasthya aur breastfeeding ke anubhav par asar daal sakta hai. Kripya apne doctor se sampark karein taaki aapko sahi margdarshan mil sake.',\n",
       "       'language': 'hinglish',\n",
       "       'judge': 'gpt-4'}],\n",
       "     'summary': {'correctness': {'mean_score': 1.0,\n",
       "       'median_score': 1.0,\n",
       "       'distribution': {'1': 5, '2': 0, '3': 0},\n",
       "       'count': 5,\n",
       "       'percentage': {'1': 100.0, '2': 0.0, '3': 0.0}},\n",
       "      'completeness': {'mean_score': 2.0,\n",
       "       'median_score': 2.0,\n",
       "       'distribution': {'1': 0, '2': 5, '3': 0},\n",
       "       'count': 5,\n",
       "       'percentage': {'1': 0.0, '2': 100.0, '3': 0.0}},\n",
       "      'clarity': {'mean_score': 1.0,\n",
       "       'median_score': 1.0,\n",
       "       'distribution': {'1': 5, '2': 0, '3': 0},\n",
       "       'count': 5,\n",
       "       'percentage': {'1': 100.0, '2': 0.0, '3': 0.0}},\n",
       "      'cultural_appropriateness': {'mean_score': 1.0,\n",
       "       'median_score': 1.0,\n",
       "       'distribution': {'1': 5, '2': 0, '3': 0},\n",
       "       'count': 5,\n",
       "       'percentage': {'1': 100.0, '2': 0.0, '3': 0.0}},\n",
       "      'overall': {'mean_score': 1.25}}}},\n",
       "   'gpt-4-turbo_rag': {'english': {'evaluations': [{'correctness': {'score': 1,\n",
       "        'justification': \"The AI's response is medically correct. It emphasizes the importance of consulting a healthcare provider for personalized assessment, which is the best approach when dealing with fetal growth and development concerns. There are no specific claims made about fetal weight that could be deemed incorrect.\",\n",
       "        'problematic_claims': []},\n",
       "       'completeness': {'score': 2,\n",
       "        'justification': 'While the response correctly advises consultation with a healthcare provider, it lacks specific information about the typical range of fetal weights at 22 weeks of gestation. For a more complete answer, it could have included a general guideline indicating that the average fetal weight at 22 weeks is typically around 430 to 500 grams, but individual variations are common and depend on many factors.',\n",
       "        'missing_information': ['General guideline or average fetal weight range for 22 weeks gestation.',\n",
       "         'Explanation that individual variations are common and depend on many factors.']},\n",
       "       'clarity': {'score': 1,\n",
       "        'justification': 'The response is clear and easily understandable, using straightforward language without medical jargon that could confuse the average reader.'},\n",
       "       'cultural_appropriateness': {'score': 1,\n",
       "        'justification': \"The response is culturally appropriate for an English-speaking user. It is respectful, avoids making assumptions about the user's knowledge or beliefs, and focuses on the importance of professional medical advice, which is a universally accepted approach.\"},\n",
       "       'question': 'My baby weight is 452 gram in 22weeks..is this normal',\n",
       "       'response': \"Based on the information provided, I don't have specific guidelines regarding fetal weight at 22 weeks to determine if 452 grams is within a normal range. It's important to discuss this with your healthcare provider, who can assess the growth of your baby in the context of overall health and development during your pregnancy. They can provide you with the best advice and care. Remember, regular prenatal check-ups are crucial for monitoring the health and development of your baby. If you have any concerns about your pregnancy, it's always best to consult with your healthcare provider.\",\n",
       "       'language': 'english',\n",
       "       'judge': 'gpt-4'},\n",
       "      {'correctness': {'score': 1,\n",
       "        'justification': \"The AI's response is medically correct. It accurately describes the variability in fetal movements and the importance of monitoring changes, especially a significant decrease, which could indicate a complication. The advice to contact a healthcare provider if concerned is also correct and aligns with standard medical guidance.\",\n",
       "        'problematic_claims': []},\n",
       "       'completeness': {'score': 2,\n",
       "        'justification': \"While the response covers the critical aspect of monitoring fetal movements and when to seek medical advice, it lacks specific guidance on what constitutes a 'significant decrease' in movements, which could be crucial for a pregnant person to understand. Additionally, it does not mention the concept of 'kick counts,' a common method used to monitor fetal health.\",\n",
       "        'missing_information': ['Explanation of what constitutes a significant decrease in fetal movements.',\n",
       "         \"Information on 'kick counts' as a method to monitor fetal movements.\"]},\n",
       "       'clarity': {'score': 1,\n",
       "        'justification': 'The response is written in clear, understandable language suitable for a user of average literacy. It uses straightforward terms and avoids medical jargon, making it accessible to a broad audience.'},\n",
       "       'cultural_appropriateness': {'score': 1,\n",
       "        'justification': \"The response is culturally appropriate for an English-speaking user. It provides general advice that is relevant and respectful across different cultural contexts without making assumptions about the user's specific cultural background.\"},\n",
       "       'question': 'Sometimes I don’t feel the baby’s move ..is it okay or something else?',\n",
       "       'response': \"It's normal to occasionally not feel your baby move, especially earlier in pregnancy when movements can be less noticeable. However, if you notice a significant decrease in your baby's movements or if movements stop, this could be a sign of a serious complication. It's important to monitor the baby's movements, especially as you get further along in your pregnancy.\\n\\nIf you're concerned about the frequency or pattern of your baby's movements, it's best to contact your healthcare provider immediately for further advice. They can perform checks to ensure everything is progressing well with your pregnancy. Remember, it's always better to be cautious and seek advice if you're unsure or worried.\",\n",
       "       'language': 'english',\n",
       "       'judge': 'gpt-4'},\n",
       "      {'correctness': {'score': 1,\n",
       "        'justification': 'All the claims in the response are medically correct. The advice given aligns with standard recommendations for managing prickly heat and itching, including the importance of consulting a healthcare provider for severe symptoms or before using medications.',\n",
       "        'problematic_claims': []},\n",
       "       'completeness': {'score': 2,\n",
       "        'justification': 'The response covers the most important information for managing prickly heat and itching during pregnancy. However, it could have included advice on when to seek immediate medical attention (beyond the mention of jaundice) and the potential need for allergy testing if the condition persists or if the individual has a history of allergies.',\n",
       "        'missing_information': ['Information on when to seek immediate medical attention for symptoms other than jaundice, such as signs of infection.',\n",
       "         'Mention of the potential need for allergy testing if the condition does not improve or if there is a known history of allergies.']},\n",
       "       'clarity': {'score': 1,\n",
       "        'justification': 'The response is written in clear, simple language that is easily understandable by users of average literacy. It uses straightforward instructions and avoids medical jargon, making it accessible to a wide audience.'},\n",
       "       'cultural_appropriateness': {'score': 1,\n",
       "        'justification': 'The response is culturally appropriate for an English-speaking audience. It avoids cultural references or idioms that could be misunderstood and focuses on general advice that is applicable to a broad audience.'},\n",
       "       'question': 'I have seen prickly heat and itching on some part of my body... What should I do?',\n",
       "       'response': \"Prickly heat and itching are common during pregnancy due to hormonal changes and increased sweating. Here are a few tips to help manage the discomfort:\\n\\n1. Keep the affected area cool and dry. Try to stay in cooler environments and avoid excessive heat.\\n2. Wear loose, breathable clothing made from natural fibers like cotton, which allows your skin to breathe and reduces irritation.\\n3. Bathe in cool water and use a mild soap that is gentle on the skin.\\n4. You can apply a calamine lotion or an over-the-counter hydrocortisone cream to soothe itching. However, it's always best to consult with your healthcare provider before using any medications.\\n5. Stay hydrated by drinking plenty of water, as this helps to regulate your body temperature.\\n\\nIf the itching is severe, widespread, or accompanied by other symptoms like jaundice, it's important to consult your healthcare provider for further evaluation. Remember, it's always best to seek personalized advice from your healthcare provider regarding any health concerns during pregnancy.\",\n",
       "       'language': 'english',\n",
       "       'judge': 'gpt-4'}],\n",
       "     'summary': {'correctness': {'mean_score': 1.0,\n",
       "       'median_score': 1.0,\n",
       "       'distribution': {'1': 3, '2': 0, '3': 0},\n",
       "       'count': 3,\n",
       "       'percentage': {'1': 100.0, '2': 0.0, '3': 0.0}},\n",
       "      'completeness': {'mean_score': 2.0,\n",
       "       'median_score': 2.0,\n",
       "       'distribution': {'1': 0, '2': 3, '3': 0},\n",
       "       'count': 3,\n",
       "       'percentage': {'1': 0.0, '2': 100.0, '3': 0.0}},\n",
       "      'clarity': {'mean_score': 1.0,\n",
       "       'median_score': 1.0,\n",
       "       'distribution': {'1': 3, '2': 0, '3': 0},\n",
       "       'count': 3,\n",
       "       'percentage': {'1': 100.0, '2': 0.0, '3': 0.0}},\n",
       "      'cultural_appropriateness': {'mean_score': 1.0,\n",
       "       'median_score': 1.0,\n",
       "       'distribution': {'1': 3, '2': 0, '3': 0},\n",
       "       'count': 3,\n",
       "       'percentage': {'1': 100.0, '2': 0.0, '3': 0.0}},\n",
       "      'overall': {'mean_score': 1.25}}},\n",
       "    'hindi': {'evaluations': [{'correctness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'completeness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'clarity': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'cultural_appropriateness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'question': 'जयदा टाइम तक बैठने से चक्कर आना खड़ा होने से पेट दर्द होने लगता हैमेरे को कौन सी टेबलेट खानी चाहिए अभी सट्टा मा लग चुका है',\n",
       "       'response': 'मुझे आपके प्रश्न का उत्तर देने के लिए पर्याप्त जानकारी नहीं है क्योंकि प्रदान की गई दिशानिर्देशों में इस स्थिति के लिए विशेष जानकारी शामिल नहीं है। चक्कर आना और खड़े होने पर पेट दर्द होना जैसी समस्याएं गंभीर हो सकती हैं, इसलिए मैं आपको सलाह दूंगा कि आप जल्द से जल्द एक स्वास्थ्य सेवा प्रदाता से संपर्क करें। वे आपकी विशेष स्थिति का मूल्यांकन कर सकते हैं और उचित उपचार प्रदान कर सकते हैं। अपनी सुरक्षा के लिए, कृपया बिना चिकित्सकीय सलाह के कोई दवा न लें।',\n",
       "       'language': 'hindi',\n",
       "       'judge': 'gpt-4'},\n",
       "      {'correctness': {'score': 1,\n",
       "        'justification': 'The medical advice provided is accurate. Monitoring the child for severe symptoms like high fever, difficulty breathing, persistent crying or unusual quietness, and reduced intake of food and drink is appropriate and aligns with standard healthcare guidance. Advising to seek healthcare if severe symptoms are observed is also correct.',\n",
       "        'problematic_claims': []},\n",
       "       'completeness': {'score': 2,\n",
       "        'justification': 'The response covers the important aspects of monitoring a child for severe symptoms of a cold and when to seek medical advice. However, it could have included advice on managing mild symptoms at home, such as keeping the child hydrated, using saline nasal drops for a blocked nose, and ensuring rest.',\n",
       "        'missing_information': ['Advice on managing mild symptoms at home (hydration, rest, saline nasal drops).',\n",
       "         'Information on the transmission of colds and preventive measures to avoid spreading to others.']},\n",
       "       'clarity': {'score': 1,\n",
       "        'justification': 'The response is clear and straightforward, making it understandable for a user of average literacy. The use of simple language and the direct listing of symptoms to watch for contribute to its clarity.'},\n",
       "       'cultural_appropriateness': {'score': 1,\n",
       "        'justification': \"The response is culturally appropriate for a Hindi-speaking user. It uses language and examples that are likely to be understood and relevant in the cultural context. Additionally, the advice given does not conflict with cultural norms and is sensitive to the user's potential healthcare practices.\"},\n",
       "       'question': 'सर्दी -जुकाम  हो रहा  है।इसका बच्चे पर तो कोई प्रभाव  नहीं  होगा ।',\n",
       "       'response': 'सर्दी-जुकाम आमतौर पर हल्की स्थिति होती है और यह अक्सर बच्चों पर गंभीर प्रभाव नहीं डालती। हालांकि, यह महत्वपूर्ण है कि आप बच्चे की निगरानी करें और अगर बच्चे में निम्नलिखित लक्षण दिखाई दें तो तुरंत स्वास्थ्य सेवा प्रदाता से संपर्क करें:\\n\\n- तेज बुखार\\n- सांस लेने में कठिनाई\\n- निरंतर रोना या असामान्य शांत होना\\n- खाना पीना कम कर देना\\n\\nअगर आपको ये लक्षण नहीं दिखाई देते हैं, तो आमतौर पर सर्दी-जुकाम से बच्चे को कोई गंभीर समस्या नहीं होती। फिर भी, अगर आपको कोई चिंता है तो डॉक्टर से सलाह लेना उचित होगा।',\n",
       "       'language': 'hindi',\n",
       "       'judge': 'gpt-4'}],\n",
       "     'summary': {'correctness': {'mean_score': 2.0,\n",
       "       'median_score': 2.0,\n",
       "       'distribution': {'1': 1, '2': 0, '3': 1},\n",
       "       'count': 2,\n",
       "       'percentage': {'1': 50.0, '2': 0.0, '3': 50.0}},\n",
       "      'completeness': {'mean_score': 2.5,\n",
       "       'median_score': 2.5,\n",
       "       'distribution': {'1': 0, '2': 1, '3': 1},\n",
       "       'count': 2,\n",
       "       'percentage': {'1': 0.0, '2': 50.0, '3': 50.0}},\n",
       "      'clarity': {'mean_score': 2.0,\n",
       "       'median_score': 2.0,\n",
       "       'distribution': {'1': 1, '2': 0, '3': 1},\n",
       "       'count': 2,\n",
       "       'percentage': {'1': 50.0, '2': 0.0, '3': 50.0}},\n",
       "      'cultural_appropriateness': {'mean_score': 2.0,\n",
       "       'median_score': 2.0,\n",
       "       'distribution': {'1': 1, '2': 0, '3': 1},\n",
       "       'count': 2,\n",
       "       'percentage': {'1': 50.0, '2': 0.0, '3': 50.0}},\n",
       "      'overall': {'mean_score': 2.125}}},\n",
       "    'hinglish': {'evaluations': [{'correctness': {'score': 1,\n",
       "        'justification': 'The response correctly identifies mango as a nutritious fruit rich in vitamin C and fiber, which are beneficial during pregnancy unless there are specific health concerns such as allergies or abnormal sugar levels. Advising to consume in moderation and consult a healthcare provider aligns with general medical advice.',\n",
       "        'problematic_claims': []},\n",
       "       'completeness': {'score': 2,\n",
       "        'justification': 'While the response covers the importance of moderation and consultation with a healthcare provider, it lacks specific guidance on quantity which the user might be seeking. Additionally, it does not address potential concerns with high sugar content in mangoes for those with gestational diabetes.',\n",
       "        'missing_information': ['Guidance on how much mango might be considered a moderate amount.',\n",
       "         'A note on monitoring sugar intake, especially for those with or at risk of gestational diabetes.']},\n",
       "       'clarity': {'score': 1,\n",
       "        'justification': 'The response is clear and understandable, using straightforward language that should be accessible to a user of average literacy. The use of Hinglish is appropriate for the intended audience.'},\n",
       "       'cultural_appropriateness': {'score': 1,\n",
       "        'justification': 'The response is culturally appropriate, using Hinglish which blends Hindi and English in a manner that is commonly used and understood in many parts of India. This approach makes the advice more relatable and accessible to the user.'},\n",
       "       'question': 'Pragnancy me mango 🥭 kitna mntra me kha skti hu',\n",
       "       'response': 'Mujhe mango ke sevan ke liye kisi specific mahine ke bare mein jankari dene ke liye paryapt suchna nahi hai, jo di gayi guidelines mein shaamil hai. Aam tor par, mango ek poshtik phal hai jo vitamin C aur fiber se bharpoor hota hai. Garbhavastha ke dauran, yadi aapko mango se koi allergy nahi hai aur aapka sugar level samanya hai, to aap iska sevan kar sakti hain, lekin yah sunishchit karein ki aap ise santulit matra mein khayein. Har vyakti ki nutritional zarooratein alag hoti hain, isliye apne aahar mein mango shamil karne se pehle ek swasthya visheshagya ya apne doctor se salah zaroor lein.',\n",
       "       'language': 'hinglish',\n",
       "       'judge': 'gpt-4'},\n",
       "      {'correctness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'completeness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'clarity': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'cultural_appropriateness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'question': 'Kya pregnancy m bleeding hoti h kya yeh normal h',\n",
       "       'response': 'Bleeding ke dauran pregnancy mein hamesha ek chinta ka vishay hota hai aur ise halka nahi lena chahiye. Pregnancy ke shuruaati 20 hafte ke dauran, bleeding ka karan ho sakta hai dhamki di hui garbhpaat (threatened abortion), swatah garbhpaat (spontaneous abortion), ectopic pregnancy ya hydatidiform mole. Agar aapko bleeding ho rahi hai, to yeh zaroori hai ki aap turant apne swasthya seva pradaata se sampark karein.\\n\\nBleeding hamesha normal nahi hoti hai aur iska karan aur prabandhan vyakti ke aadhaar par alag-alag ho sakta hai. Isliye, yadi aapko pregnancy ke dauran kisi bhi prakar ki bleeding ho rahi hai, to kripya apne doctor ya swasthya seva pradaata se sampark karein taki ve sthiti ka sahi mulyankan kar sakein aur uchit salah de sakein.',\n",
       "       'language': 'hinglish',\n",
       "       'judge': 'gpt-4'},\n",
       "      {'correctness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'completeness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'clarity': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'cultural_appropriateness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'question': '2.5 months hogae h pregnancy ko but vomet ya kch ni hora mjhe',\n",
       "       'response': 'Aapko chinta karne ki zarurat nahi hai agar aapko pregnancy ke 2.5 mahine mein vomiting ya koi aur takleef nahi ho rahi hai. Har mahila ki pregnancy alag hoti hai, aur kuch mahilao ko vomiting ya morning sickness nahi hoti. Yeh bilkul normal hai. Lekin, agar aapko koi aur symptoms dikhayi de rahe hain jo aapko pareshan kar rahe hain, toh aapko apne healthcare provider se zaroor consult karna chahiye. Woh aapki aur aapke baby ki health ka sahi se assessment kar sakenge. Aapka dhyan rakhna aur regular check-ups karwana important hai.',\n",
       "       'language': 'hinglish',\n",
       "       'judge': 'gpt-4'},\n",
       "      {'correctness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'completeness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'clarity': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'cultural_appropriateness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'question': 'Bacche ko Tej Sardi ho to kaise usko maintain Karen',\n",
       "       'response': 'Agar bacche ko tej sardi ho rahi hai, toh yeh kuch upay hain jo aap kar sakti hain:\\n\\n1. Bacche ko garam rakhein: Bacche ko acche se cover karein, khaaskar uske sir aur pair ko. Thandi mausam mein, bacche ko extra kapde pahnaayein.\\n\\n2. Bacche ke sone ka sthan garam rakhein: Bacche ko sone ke liye ek garam aur sukhad jagah pradan karein. Agar aapke paas radiant warmer hai toh uska istemal karein.\\n\\n3. Swachhta ka dhyan rakhein: Bacche ko sambhalte waqt haatho ko saaf rakhein aur bacche ko bhi saaf rakhein.\\n\\n4. Bacche ko skin-to-skin contact dein: Agar bacche ko sardi zyada hai toh usko apni skin se lagaye rakhna (kangaroo mother care) madadgar ho sakta hai kyunki yeh bacche ko garam rakhta hai.\\n\\n5. Agar bacche ko saans lene mein dikkat ho rahi hai ya uske anya gambhir lakshan dikhayi de rahe hain, toh turant swasthya seva pradan karne wale se sampark karein.\\n\\nYeh upay aam taur par madadgar hote hain, lekin agar aapko lagta hai ki sthiti gambhir hai, toh kripya turant doctor se sampark karein.',\n",
       "       'language': 'hinglish',\n",
       "       'judge': 'gpt-4'},\n",
       "      {'correctness': {'score': 1,\n",
       "        'justification': \"The AI's response is medically correct. It emphasizes the importance of consulting a healthcare provider for personalized advice, which is crucial for conditions like thyroid disorders. The response correctly notes that thyroid issues can impact overall health and potentially the breastfeeding experience, without making unfounded claims about the direct impact on breast milk.\",\n",
       "        'problematic_claims': []},\n",
       "       'completeness': {'score': 2,\n",
       "        'justification': 'While the response correctly advises consultation with a healthcare provider, it lacks specific information on how thyroid disorders might affect breastfeeding or breast milk composition. For instance, it could mention that poorly controlled hypothyroidism can reduce milk supply, or that maintaining proper thyroid hormone levels is important for lactation.',\n",
       "        'missing_information': ['Potential impact of thyroid disorders on milk supply or composition.',\n",
       "         'Importance of maintaining proper thyroid hormone levels for lactation.']},\n",
       "       'clarity': {'score': 1,\n",
       "        'justification': \"The response is written in Hinglish, which is appropriate for the user's language preference. It uses clear and simple language, making it understandable for a user of average literacy.\"},\n",
       "       'cultural_appropriateness': {'score': 1,\n",
       "        'justification': \"The response is culturally appropriate for a Hinglish-speaking user. It respects the user's language preference and provides a respectful and considerate answer to a sensitive health question.\"},\n",
       "       'question': 'mujhe thyroid h to kya isse bhi breast milk pe asar hota h',\n",
       "       'response': 'Mujhe aapke sawaal ka jawab dene ke liye di gayi jaankari mein koi seedha sandarbh nahi mila hai ki thyroid ki samasya se breast milk par kya asar padta hai. Yeh zaroori hai ki aap apne healthcare provider se is vishay mein vistar se baat karein, kyunki ve aapki personal medical condition ko samajhne mein madad kar sakte hain aur sahi salah de sakte hain. Thyroid ki samasyaon ka ilaaj aur unka prabandhan zaroori hai, aur yeh aapke overall swasthya aur breastfeeding ke anubhav par asar daal sakta hai. Kripya apne doctor se sampark karein taaki aapko sahi margdarshan mil sake.',\n",
       "       'language': 'hinglish',\n",
       "       'judge': 'gpt-4'}],\n",
       "     'summary': {'correctness': {'mean_score': 2.2,\n",
       "       'median_score': 3.0,\n",
       "       'distribution': {'1': 2, '2': 0, '3': 3},\n",
       "       'count': 5,\n",
       "       'percentage': {'1': 40.0, '2': 0.0, '3': 60.0}},\n",
       "      'completeness': {'mean_score': 2.6,\n",
       "       'median_score': 3.0,\n",
       "       'distribution': {'1': 0, '2': 2, '3': 3},\n",
       "       'count': 5,\n",
       "       'percentage': {'1': 0.0, '2': 40.0, '3': 60.0}},\n",
       "      'clarity': {'mean_score': 2.2,\n",
       "       'median_score': 3.0,\n",
       "       'distribution': {'1': 2, '2': 0, '3': 3},\n",
       "       'count': 5,\n",
       "       'percentage': {'1': 40.0, '2': 0.0, '3': 60.0}},\n",
       "      'cultural_appropriateness': {'mean_score': 2.2,\n",
       "       'median_score': 3.0,\n",
       "       'distribution': {'1': 2, '2': 0, '3': 3},\n",
       "       'count': 5,\n",
       "       'percentage': {'1': 40.0, '2': 0.0, '3': 60.0}},\n",
       "      'overall': {'mean_score': 2.3000000000000003}}}},\n",
       "   'mixtral': {'english': {'evaluations': [{'correctness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'completeness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'clarity': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'cultural_appropriateness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'question': 'My baby weight is 452 gram in 22weeks..is this normal',\n",
       "       'response': \"I'm sorry, but the provided context does not include specific information about your baby's weight or gestational age. However, I can tell you that a birth weight of less than 1500 grams is considered low birth weight and may require special care. It's important to consult with a healthcare provider for personalized advice and monitoring. They can assess your baby's health and provide appropriate guidance.\",\n",
       "       'language': 'english',\n",
       "       'judge': 'gpt-4'},\n",
       "      {'correctness': {'score': 1,\n",
       "        'justification': 'The response accurately reflects medical advice regarding variations in fetal movements and the importance of consulting a healthcare provider if there is a significant decrease in movements or no movement felt for a while. Each claim is medically correct.',\n",
       "        'problematic_claims': []},\n",
       "       'completeness': {'score': 2,\n",
       "        'justification': \"While the response covers the critical advice of seeking medical attention for decreased or absent fetal movements, it lacks specific guidance on what constitutes a significant decrease and when exactly to seek help (e.g., the 'count-to-ten' method or specific time frames for inactivity that should prompt concern).\",\n",
       "        'missing_information': ['Guidance on monitoring fetal movements (e.g., kick counts or what patterns to watch for).',\n",
       "         'Specific advice on when to seek medical attention, such as if no movements are felt within a 2-hour window.']},\n",
       "       'clarity': {'score': 1,\n",
       "        'justification': 'The response is clearly written and uses straightforward language that should be understandable to a user of average literacy. It avoids medical jargon and explains the steps a person should take in a clear manner.'},\n",
       "       'cultural_appropriateness': {'score': 1,\n",
       "        'justification': \"The response is culturally neutral and provides advice that is broadly applicable to English-speaking users. It focuses on universal concerns regarding fetal health without making assumptions about the user's specific cultural or personal background.\"},\n",
       "       'question': 'Sometimes I don’t feel the baby’s move ..is it okay or something else?',\n",
       "       'response': \"It's normal for the baby's movements to vary from time to time. However, if you notice a significant decrease in your baby's movements or if you haven't felt any movement for a while, it's important to consult your healthcare provider. They can check your baby's heartbeat and make sure everything is okay. It's always better to be safe and seek medical advice when it comes to your baby's health.\",\n",
       "       'language': 'english',\n",
       "       'judge': 'gpt-4'},\n",
       "      {'correctness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'completeness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'clarity': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'cultural_appropriateness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'question': 'I have seen prickly heat and itching on some part of my body... What should I do?',\n",
       "       'response': \"It sounds like you might be experiencing prickly heat or heat rash, which can cause itching and small red bumps on the skin. This is usually harmless and goes away on its own once the skin cools down. To help soothe the itching, you can try the following:\\n\\n1. Stay in a cool, airy place to allow your skin to breathe and reduce sweating.\\n2. Wear loose, light clothing made of natural fibers like cotton.\\n3. Avoid using harsh soaps, lotions, or perfumes that might irritate your skin.\\n4. Take cool showers or baths to help relieve itching.\\n5. Apply calamine lotion or aloe vera gel to the affected areas to soothe the skin.\\n\\nHowever, if the itching is severe, persistent, or accompanied by other symptoms like fever, rash, or swelling, it's important to consult a healthcare provider for further evaluation. They can help determine the cause of your symptoms and recommend appropriate treatment.\",\n",
       "       'language': 'english',\n",
       "       'judge': 'gpt-4'}],\n",
       "     'summary': {'correctness': {'mean_score': 2.3333333333333335,\n",
       "       'median_score': 3.0,\n",
       "       'distribution': {'1': 1, '2': 0, '3': 2},\n",
       "       'count': 3,\n",
       "       'percentage': {'1': 33.33333333333333,\n",
       "        '2': 0.0,\n",
       "        '3': 66.66666666666666}},\n",
       "      'completeness': {'mean_score': 2.6666666666666665,\n",
       "       'median_score': 3.0,\n",
       "       'distribution': {'1': 0, '2': 1, '3': 2},\n",
       "       'count': 3,\n",
       "       'percentage': {'1': 0.0,\n",
       "        '2': 33.33333333333333,\n",
       "        '3': 66.66666666666666}},\n",
       "      'clarity': {'mean_score': 2.3333333333333335,\n",
       "       'median_score': 3.0,\n",
       "       'distribution': {'1': 1, '2': 0, '3': 2},\n",
       "       'count': 3,\n",
       "       'percentage': {'1': 33.33333333333333,\n",
       "        '2': 0.0,\n",
       "        '3': 66.66666666666666}},\n",
       "      'cultural_appropriateness': {'mean_score': 2.3333333333333335,\n",
       "       'median_score': 3.0,\n",
       "       'distribution': {'1': 1, '2': 0, '3': 2},\n",
       "       'count': 3,\n",
       "       'percentage': {'1': 33.33333333333333,\n",
       "        '2': 0.0,\n",
       "        '3': 66.66666666666666}},\n",
       "      'overall': {'mean_score': 2.416666666666667}}},\n",
       "    'hindi': {'evaluations': [{'correctness': {'score': 1,\n",
       "        'justification': 'The advice given by the AI is medically sound. Recommending light physical activity such as walking or stretching to alleviate abdominal pain caused by prolonged sitting is appropriate. The suggestion to consult a healthcare provider if the pain persists or is severe is also correct and responsible advice.',\n",
       "        'problematic_claims': []},\n",
       "       'completeness': {'score': 2,\n",
       "        'justification': \"While the response covers general advice for abdominal pain due to prolonged sitting, it lacks specific guidance regarding the user's mention of 'सट्टा मा लग चुका है' which could imply pregnancy ('मा लग चुका है' might be interpreted as 'pregnancy has occurred'). The response does not address the potential pregnancy directly, nor does it mention the importance of avoiding certain medications without consulting a healthcare provider during pregnancy.\",\n",
       "        'missing_information': ['Specific advice for pregnant women regarding the use of any medication.',\n",
       "         'A caution against self-medicating, especially during pregnancy, without consulting a healthcare provider.']},\n",
       "       'clarity': {'score': 1,\n",
       "        'justification': 'The response is clear and should be understandable to a user of average literacy. It provides straightforward advice without using complex medical jargon.'},\n",
       "       'cultural_appropriateness': {'score': 1,\n",
       "        'justification': 'The response is culturally neutral and appropriate. It does not include any cultural references or assumptions that could be considered inappropriate for a Hindi-speaking user. The advice is universally applicable and respectful.'},\n",
       "       'question': 'जयदा टाइम तक बैठने से चक्कर आना खड़ा होने से पेट दर्द होने लगता हैमेरे को कौन सी टेबलेट खानी चाहिए अभी सट्टा मा लग चुका है',\n",
       "       'response': 'If you are experiencing abdominal pain after sitting for a long time, it might be due to gas or muscle strain. You can try taking a walk or doing some light stretching exercises to relieve the pain. If the pain persists or is severe, it is recommended to consult a healthcare provider for a proper diagnosis and treatment.',\n",
       "       'language': 'hindi',\n",
       "       'judge': 'gpt-4'},\n",
       "      {'correctness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'completeness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'clarity': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'cultural_appropriateness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'question': 'सर्दी -जुकाम  हो रहा  है।इसका बच्चे पर तो कोई प्रभाव  नहीं  होगा ।',\n",
       "       'response': \"During winter, it is normal for babies to feel a little cold, but it should not affect their overall health. However, if the baby is showing signs of being unwell, such as lethargy, refusal to feed, or difficulty breathing, it is important to seek medical attention immediately. It is also crucial to keep the baby warm by covering their head and feet, and dressing them in extra clothing. If the baby's temperature is too low or too high, it is a sign of illness and requires medical attention. It is always best to consult a healthcare provider for personalized medical advice.\",\n",
       "       'language': 'hindi',\n",
       "       'judge': 'gpt-4'}],\n",
       "     'summary': {'correctness': {'mean_score': 2.0,\n",
       "       'median_score': 2.0,\n",
       "       'distribution': {'1': 1, '2': 0, '3': 1},\n",
       "       'count': 2,\n",
       "       'percentage': {'1': 50.0, '2': 0.0, '3': 50.0}},\n",
       "      'completeness': {'mean_score': 2.5,\n",
       "       'median_score': 2.5,\n",
       "       'distribution': {'1': 0, '2': 1, '3': 1},\n",
       "       'count': 2,\n",
       "       'percentage': {'1': 0.0, '2': 50.0, '3': 50.0}},\n",
       "      'clarity': {'mean_score': 2.0,\n",
       "       'median_score': 2.0,\n",
       "       'distribution': {'1': 1, '2': 0, '3': 1},\n",
       "       'count': 2,\n",
       "       'percentage': {'1': 50.0, '2': 0.0, '3': 50.0}},\n",
       "      'cultural_appropriateness': {'mean_score': 2.0,\n",
       "       'median_score': 2.0,\n",
       "       'distribution': {'1': 1, '2': 0, '3': 1},\n",
       "       'count': 2,\n",
       "       'percentage': {'1': 50.0, '2': 0.0, '3': 50.0}},\n",
       "      'overall': {'mean_score': 2.125}}},\n",
       "    'hinglish': {'evaluations': [{'correctness': {'score': 1,\n",
       "        'justification': \"The AI's response is medically correct. Thyroid disorders can indeed affect breast milk production, and the advice to consult a healthcare provider for personalized guidance is appropriate.\",\n",
       "        'problematic_claims': []},\n",
       "       'completeness': {'score': 2,\n",
       "        'justification': 'While the response covers the important aspect of consulting a healthcare provider for personalized advice, it could have included more specific information on how thyroid disorders might affect breast milk production (e.g., hypothyroidism potentially leading to reduced supply) and possible management strategies.',\n",
       "        'missing_information': ['Specific effects of different types of thyroid disorders on breast milk production',\n",
       "         'General management strategies or considerations for breastfeeding mothers with thyroid disorders']},\n",
       "       'clarity': {'score': 1,\n",
       "        'justification': 'The response is clear and should be understandable to a user of average literacy. It uses straightforward language without medical jargon that could confuse the reader.'},\n",
       "       'cultural_appropriateness': {'score': 1,\n",
       "        'justification': \"The response is culturally appropriate for a Hinglish-speaking user. It addresses the user's concern in a respectful manner and encourages seeking professional medical advice, which is a universally accepted approach.\"},\n",
       "       'question': 'mujhe thyroid h to kya isse bhi breast milk pe asar hota h',\n",
       "       'response': \"Thyroid issues can affect breast milk production, but it depends on the type and severity of the thyroid disorder. If you have concerns about your thyroid and how it might be affecting your breast milk, it's important to consult with a healthcare provider for personalized advice. They can evaluate your thyroid function and provide appropriate guidance to ensure the health and well-being of both you and your baby.\",\n",
       "       'language': 'hinglish',\n",
       "       'judge': 'gpt-4'},\n",
       "      {'correctness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'completeness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'clarity': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'cultural_appropriateness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'question': '2.5 months hogae h pregnancy ko but vomet ya kch ni hora mjhe',\n",
       "       'response': 'Aapko 2.5 mahine se hogaye hue hamare garbha ko vomit ya kuch nahi hai, yeh aapki surakshit rahne ke liye koi khas kadam nahi lena chahta hai. Garbhaalamban mein vomiting kaam aata hai, lekin iska koi sthaayi nahi hai. Agar aapko vomiting ya koi anya problem hai to aap apne swasthya karyakarta se salah lena chahte hain.',\n",
       "       'language': 'hinglish',\n",
       "       'judge': 'gpt-4'},\n",
       "      {'correctness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'completeness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'clarity': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'cultural_appropriateness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'question': 'Bacche ko Tej Sardi ho to kaise usko maintain Karen',\n",
       "       'response': 'Agar bacche mein tej sardi ho to unhe aaram se rakhein. Thodi der tak unki sar aur pair chunein. Unhe ek aur kapda pehena do. Bacche ko sabse pehle rakhein jahaan se sehatmand hawa aati hai. Isse bacche ko thandi nahi honi padegi. Bacche ko thandi nahi honi chahiye. Isse bacche ki sardi kam hogi.',\n",
       "       'language': 'hinglish',\n",
       "       'judge': 'gpt-4'},\n",
       "      {'correctness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'completeness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'clarity': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'cultural_appropriateness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'question': 'Kya pregnancy m bleeding hoti h kya yeh normal h',\n",
       "       'response': 'Bleeding during pregnancy can be normal, but it can also be a sign of a problem. If you are experiencing bleeding, it is important to pay attention to the amount and color of the blood, as well as any other symptoms you may be having. In early pregnancy, light bleeding with lower abdominal pain could be a sign of a threatened abortion, which is when the cervix begins to open but the pregnancy is not expelled. This is not usually a serious condition, but it is important to be monitored by a healthcare provider. If you have heavy bleeding, severe abdominal pain, or any other concerning symptoms, you should seek medical attention immediately. It is always best to consult with a healthcare provider for personalized medical advice.',\n",
       "       'language': 'hinglish',\n",
       "       'judge': 'gpt-4'},\n",
       "      {'correctness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'completeness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'clarity': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'cultural_appropriateness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'question': 'Pragnancy me mango 🥭 kitna mntra me kha skti hu',\n",
       "       'response': 'Mangoes can be consumed during pregnancy, but it is important to maintain a balanced diet and not consume too much of any one food. Mangoes are rich in vitamins A and C, but they also contain natural sugars. It is recommended to eat at least 3 main meals and 2 nutritious snacks daily, including a variety of fruits and vegetables. However, if you have any concerns about your diet during pregnancy, it is always best to consult with a healthcare provider for personalized advice.',\n",
       "       'language': 'hinglish',\n",
       "       'judge': 'gpt-4'}],\n",
       "     'summary': {'correctness': {'mean_score': 2.6,\n",
       "       'median_score': 3.0,\n",
       "       'distribution': {'1': 1, '2': 0, '3': 4},\n",
       "       'count': 5,\n",
       "       'percentage': {'1': 20.0, '2': 0.0, '3': 80.0}},\n",
       "      'completeness': {'mean_score': 2.8,\n",
       "       'median_score': 3.0,\n",
       "       'distribution': {'1': 0, '2': 1, '3': 4},\n",
       "       'count': 5,\n",
       "       'percentage': {'1': 0.0, '2': 20.0, '3': 80.0}},\n",
       "      'clarity': {'mean_score': 2.6,\n",
       "       'median_score': 3.0,\n",
       "       'distribution': {'1': 1, '2': 0, '3': 4},\n",
       "       'count': 5,\n",
       "       'percentage': {'1': 20.0, '2': 0.0, '3': 80.0}},\n",
       "      'cultural_appropriateness': {'mean_score': 2.6,\n",
       "       'median_score': 3.0,\n",
       "       'distribution': {'1': 1, '2': 0, '3': 4},\n",
       "       'count': 5,\n",
       "       'percentage': {'1': 20.0, '2': 0.0, '3': 80.0}},\n",
       "      'overall': {'mean_score': 2.65}}}},\n",
       "   'mixtral_rag': {'english': {'evaluations': [{'correctness': {'score': 2,\n",
       "        'justification': \"The response inaccurately addresses the user's question about the normalcy of their baby's weight at 22 weeks gestation by discussing low birth weight, which is not directly relevant to the question about fetal weight during pregnancy. The information about low birth weight being under 1500 grams is correct but not applicable to a fetus at 22 weeks gestation.\",\n",
       "        'problematic_claims': ['Discussion of low birth weight in the context of fetal weight at 22 weeks gestation']},\n",
       "       'completeness': {'score': 3,\n",
       "        'justification': 'The response fails to address the specific question about whether a weight of 452 grams at 22 weeks is normal. It does not provide any reference to fetal growth charts or the expected weight range for this gestational age.',\n",
       "        'missing_information': ['Reference to fetal growth charts or expected weight range for 22 weeks gestation',\n",
       "         'Advice to consult with a healthcare provider for an assessment specific to fetal growth and development']},\n",
       "       'clarity': {'score': 1,\n",
       "        'justification': \"Despite the response not directly answering the user's question, the language used is clear and understandable for a user of average literacy.\"},\n",
       "       'cultural_appropriateness': {'score': 1,\n",
       "        'justification': 'The response is culturally appropriate for an English-speaking user, avoiding any culturally sensitive issues and focusing on the need for professional healthcare advice.'},\n",
       "       'question': 'My baby weight is 452 gram in 22weeks..is this normal',\n",
       "       'response': \"I'm sorry, but the provided context does not include specific information about your baby's weight or gestational age. However, I can tell you that a birth weight of less than 1500 grams is considered low birth weight and may require special care. It's important to consult with a healthcare provider for personalized advice and monitoring. They can assess your baby's health and provide appropriate guidance.\",\n",
       "       'language': 'english',\n",
       "       'judge': 'gpt-4'},\n",
       "      {'correctness': {'score': 1,\n",
       "        'justification': \"The AI's response is medically correct. It accurately states that variations in a baby's movements can be normal but emphasizes the importance of consulting a healthcare provider if there is a significant decrease in movements or no movement felt for a while. This advice aligns with standard prenatal care guidelines.\",\n",
       "        'problematic_claims': []},\n",
       "       'completeness': {'score': 2,\n",
       "        'justification': \"While the response covers the critical action of seeking medical advice when there's a noticeable decrease in fetal movements, it lacks specific guidance on what constitutes a 'significant decrease' and does not mention the importance of tracking movements, especially in the third trimester, which is often recommended.\",\n",
       "        'missing_information': ['Guidance on what constitutes a significant decrease in movements (e.g., fewer than 10 movements in 2 hours).',\n",
       "         'The importance of regular fetal movement tracking in the third trimester.']},\n",
       "       'clarity': {'score': 1,\n",
       "        'justification': \"The response is clear and straightforward, using simple language that should be understandable to a user of average literacy. It directly addresses the user's concern and provides a clear course of action.\"},\n",
       "       'cultural_appropriateness': {'score': 1,\n",
       "        'justification': \"The response is culturally appropriate for an English-speaking user. It is sensitive to the user's concerns about fetal health and encourages seeking medical advice, which is a universally accepted approach in healthcare.\"},\n",
       "       'question': 'Sometimes I don’t feel the baby’s move ..is it okay or something else?',\n",
       "       'response': \"It's normal for the baby's movements to vary from time to time. However, if you notice a significant decrease in your baby's movements or if you haven't felt any movement for a while, it's important to consult your healthcare provider. They can check your baby's heartbeat and make sure everything is okay. It's always better to be safe and seek medical advice when it comes to your baby's health.\",\n",
       "       'language': 'english',\n",
       "       'judge': 'gpt-4'},\n",
       "      {'correctness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'completeness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'clarity': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'cultural_appropriateness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'question': 'I have seen prickly heat and itching on some part of my body... What should I do?',\n",
       "       'response': \"It sounds like you might be experiencing prickly heat or heat rash, which can cause itching and small red bumps on the skin. This is usually harmless and goes away on its own once the skin cools down. To help soothe the itching, you can try the following:\\n\\n1. Stay in a cool, airy place to allow your skin to breathe and reduce sweating.\\n2. Wear loose, light clothing made of natural fibers like cotton.\\n3. Avoid using harsh soaps, lotions, or perfumes that might irritate your skin.\\n4. Take cool showers or baths to help relieve itching.\\n5. Apply calamine lotion or aloe vera gel to the affected areas to soothe the skin.\\n\\nHowever, if the itching is severe, persistent, or accompanied by other symptoms like fever, rash, or swelling, it's important to consult a healthcare provider for further evaluation. They can help determine the cause of your symptoms and recommend appropriate treatment.\",\n",
       "       'language': 'english',\n",
       "       'judge': 'gpt-4'}],\n",
       "     'summary': {'correctness': {'mean_score': 2.0,\n",
       "       'median_score': 2.0,\n",
       "       'distribution': {'1': 1, '2': 1, '3': 1},\n",
       "       'count': 3,\n",
       "       'percentage': {'1': 33.33333333333333,\n",
       "        '2': 33.33333333333333,\n",
       "        '3': 33.33333333333333}},\n",
       "      'completeness': {'mean_score': 2.6666666666666665,\n",
       "       'median_score': 3.0,\n",
       "       'distribution': {'1': 0, '2': 1, '3': 2},\n",
       "       'count': 3,\n",
       "       'percentage': {'1': 0.0,\n",
       "        '2': 33.33333333333333,\n",
       "        '3': 66.66666666666666}},\n",
       "      'clarity': {'mean_score': 1.6666666666666667,\n",
       "       'median_score': 1.0,\n",
       "       'distribution': {'1': 2, '2': 0, '3': 1},\n",
       "       'count': 3,\n",
       "       'percentage': {'1': 66.66666666666666,\n",
       "        '2': 0.0,\n",
       "        '3': 33.33333333333333}},\n",
       "      'cultural_appropriateness': {'mean_score': 1.6666666666666667,\n",
       "       'median_score': 1.0,\n",
       "       'distribution': {'1': 2, '2': 0, '3': 1},\n",
       "       'count': 3,\n",
       "       'percentage': {'1': 66.66666666666666,\n",
       "        '2': 0.0,\n",
       "        '3': 33.33333333333333}},\n",
       "      'overall': {'mean_score': 2.0}}},\n",
       "    'hindi': {'evaluations': [{'correctness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'completeness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'clarity': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'cultural_appropriateness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'question': 'जयदा टाइम तक बैठने से चक्कर आना खड़ा होने से पेट दर्द होने लगता हैमेरे को कौन सी टेबलेट खानी चाहिए अभी सट्टा मा लग चुका है',\n",
       "       'response': 'If you are experiencing abdominal pain after sitting for a long time, it might be due to gas or muscle strain. You can try taking a walk or doing some light stretching exercises to relieve the pain. If the pain persists or is severe, it is recommended to consult a healthcare provider for a proper diagnosis and treatment.',\n",
       "       'language': 'hindi',\n",
       "       'judge': 'gpt-4'},\n",
       "      {'correctness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'completeness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'clarity': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'cultural_appropriateness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'question': 'सर्दी -जुकाम  हो रहा  है।इसका बच्चे पर तो कोई प्रभाव  नहीं  होगा ।',\n",
       "       'response': \"During winter, it is normal for babies to feel a little cold, but it should not affect their overall health. However, if the baby is showing signs of being unwell, such as lethargy, refusal to feed, or difficulty breathing, it is important to seek medical attention immediately. It is also crucial to keep the baby warm by covering their head and feet, and dressing them in extra clothing. If the baby's temperature is too low or too high, it is a sign of illness and requires medical attention. It is always best to consult a healthcare provider for personalized medical advice.\",\n",
       "       'language': 'hindi',\n",
       "       'judge': 'gpt-4'}],\n",
       "     'summary': {'correctness': {'mean_score': 3.0,\n",
       "       'median_score': 3.0,\n",
       "       'distribution': {'1': 0, '2': 0, '3': 2},\n",
       "       'count': 2,\n",
       "       'percentage': {'1': 0.0, '2': 0.0, '3': 100.0}},\n",
       "      'completeness': {'mean_score': 3.0,\n",
       "       'median_score': 3.0,\n",
       "       'distribution': {'1': 0, '2': 0, '3': 2},\n",
       "       'count': 2,\n",
       "       'percentage': {'1': 0.0, '2': 0.0, '3': 100.0}},\n",
       "      'clarity': {'mean_score': 3.0,\n",
       "       'median_score': 3.0,\n",
       "       'distribution': {'1': 0, '2': 0, '3': 2},\n",
       "       'count': 2,\n",
       "       'percentage': {'1': 0.0, '2': 0.0, '3': 100.0}},\n",
       "      'cultural_appropriateness': {'mean_score': 3.0,\n",
       "       'median_score': 3.0,\n",
       "       'distribution': {'1': 0, '2': 0, '3': 2},\n",
       "       'count': 2,\n",
       "       'percentage': {'1': 0.0, '2': 0.0, '3': 100.0}},\n",
       "      'overall': {'mean_score': 3.0}}},\n",
       "    'hinglish': {'evaluations': [{'correctness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'completeness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'clarity': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'cultural_appropriateness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'question': 'Kya pregnancy m bleeding hoti h kya yeh normal h',\n",
       "       'response': 'Bleeding during pregnancy can be normal, but it can also be a sign of a problem. If you are experiencing bleeding, it is important to pay attention to the amount and color of the blood, as well as any other symptoms you may be having. In early pregnancy, light bleeding with lower abdominal pain could be a sign of a threatened abortion, which is when the cervix begins to open but the pregnancy is not expelled. This is not usually a serious condition, but it is important to be monitored by a healthcare provider. If you have heavy bleeding, severe abdominal pain, or any other concerning symptoms, you should seek medical attention immediately. It is always best to consult with a healthcare provider for personalized medical advice.',\n",
       "       'language': 'hinglish',\n",
       "       'judge': 'gpt-4'},\n",
       "      {'correctness': {'score': 1,\n",
       "        'justification': 'The response accurately mentions the benefits of consuming mangoes during pregnancy, highlighting their richness in vitamins A and C, and the need to be cautious about their natural sugar content. It correctly advises on maintaining a balanced diet and consulting a healthcare provider for personalized advice, which are medically sound recommendations.',\n",
       "        'problematic_claims': []},\n",
       "       'completeness': {'score': 2,\n",
       "        'justification': \"The response covers the important aspects of consuming mangoes during pregnancy, including the nutritional benefits and the advice to not consume too much of any one food. However, it lacks specific guidance on quantity, which might be what the user was looking for ('kitna mntra me kha skti hu' suggests a query about quantity).\",\n",
       "        'missing_information': ['Specific guidance on the recommended quantity of mangoes that can be safely consumed during pregnancy.',\n",
       "         'A mention of the potential for mangoes to affect blood sugar levels, which could be a concern for pregnant women with or at risk of gestational diabetes.']},\n",
       "       'clarity': {'score': 1,\n",
       "        'justification': 'The response is clear and should be understandable to a user of average literacy. It uses straightforward language without medical jargon, making it accessible.'},\n",
       "       'cultural_appropriateness': {'score': 1,\n",
       "        'justification': \"The response is culturally appropriate for a hinglish-speaking user. It addresses a common dietary question among pregnant women in many cultures and provides a respectful and informative answer without making assumptions about the user's dietary preferences or restrictions.\"},\n",
       "       'question': 'Pragnancy me mango 🥭 kitna mntra me kha skti hu',\n",
       "       'response': 'Mangoes can be consumed during pregnancy, but it is important to maintain a balanced diet and not consume too much of any one food. Mangoes are rich in vitamins A and C, but they also contain natural sugars. It is recommended to eat at least 3 main meals and 2 nutritious snacks daily, including a variety of fruits and vegetables. However, if you have any concerns about your diet during pregnancy, it is always best to consult with a healthcare provider for personalized advice.',\n",
       "       'language': 'hinglish',\n",
       "       'judge': 'gpt-4'},\n",
       "      {'correctness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'completeness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'clarity': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'cultural_appropriateness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'question': 'Bacche ko Tej Sardi ho to kaise usko maintain Karen',\n",
       "       'response': 'Agar bacche mein tej sardi ho to unhe aaram se rakhein. Thodi der tak unki sar aur pair chunein. Unhe ek aur kapda pehena do. Bacche ko sabse pehle rakhein jahaan se sehatmand hawa aati hai. Isse bacche ko thandi nahi honi padegi. Bacche ko thandi nahi honi chahiye. Isse bacche ki sardi kam hogi.',\n",
       "       'language': 'hinglish',\n",
       "       'judge': 'gpt-4'},\n",
       "      {'correctness': {'score': 2,\n",
       "        'justification': \"The response implies that not experiencing vomiting during pregnancy does not necessarily require any special steps for safety, which is medically correct. However, the statement that 'vomiting is less in pregnancy' is misleading and incorrect as it varies greatly among individuals. Some may experience no vomiting, while others may have severe nausea and vomiting (hyperemesis gravidarum).\",\n",
       "        'problematic_claims': ['Garbhaalamban mein vomiting kaam aata hai, lekin iska koi sthaayi nahi hai.']},\n",
       "       'completeness': {'score': 2,\n",
       "        'justification': 'The response addresses the concern about the absence of vomiting but fails to provide a comprehensive overview of what symptoms might be expected during the first trimester and when it might be necessary to seek medical advice.',\n",
       "        'missing_information': ['Explanation that a lack of vomiting is not uncommon and does not necessarily indicate a problem with the pregnancy.',\n",
       "         'Advice to stay hydrated and maintain a balanced diet, and information on when to seek medical help for any concerns.']},\n",
       "       'clarity': {'score': 2,\n",
       "        'justification': 'The response is somewhat clear but could be confusing due to the mixed use of Hinglish and some grammatical inconsistencies. The message could be made clearer with more straightforward language and structure.'},\n",
       "       'cultural_appropriateness': {'score': 1,\n",
       "        'justification': \"The response is culturally appropriate for a Hinglish-speaking user, using a mix of Hindi and English that is commonly found in many parts of India. It addresses the user's concern in a culturally sensitive manner.\"},\n",
       "       'question': '2.5 months hogae h pregnancy ko but vomet ya kch ni hora mjhe',\n",
       "       'response': 'Aapko 2.5 mahine se hogaye hue hamare garbha ko vomit ya kuch nahi hai, yeh aapki surakshit rahne ke liye koi khas kadam nahi lena chahta hai. Garbhaalamban mein vomiting kaam aata hai, lekin iska koi sthaayi nahi hai. Agar aapko vomiting ya koi anya problem hai to aap apne swasthya karyakarta se salah lena chahte hain.',\n",
       "       'language': 'hinglish',\n",
       "       'judge': 'gpt-4'},\n",
       "      {'correctness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'completeness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'clarity': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'cultural_appropriateness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'question': 'mujhe thyroid h to kya isse bhi breast milk pe asar hota h',\n",
       "       'response': \"Thyroid issues can affect breast milk production, but it depends on the type and severity of the thyroid disorder. If you have concerns about your thyroid and how it might be affecting your breast milk, it's important to consult with a healthcare provider for personalized advice. They can evaluate your thyroid function and provide appropriate guidance to ensure the health and well-being of both you and your baby.\",\n",
       "       'language': 'hinglish',\n",
       "       'judge': 'gpt-4'}],\n",
       "     'summary': {'correctness': {'mean_score': 2.4,\n",
       "       'median_score': 3.0,\n",
       "       'distribution': {'1': 1, '2': 1, '3': 3},\n",
       "       'count': 5,\n",
       "       'percentage': {'1': 20.0, '2': 20.0, '3': 60.0}},\n",
       "      'completeness': {'mean_score': 2.6,\n",
       "       'median_score': 3.0,\n",
       "       'distribution': {'1': 0, '2': 2, '3': 3},\n",
       "       'count': 5,\n",
       "       'percentage': {'1': 0.0, '2': 40.0, '3': 60.0}},\n",
       "      'clarity': {'mean_score': 2.4,\n",
       "       'median_score': 3.0,\n",
       "       'distribution': {'1': 1, '2': 1, '3': 3},\n",
       "       'count': 5,\n",
       "       'percentage': {'1': 20.0, '2': 20.0, '3': 60.0}},\n",
       "      'cultural_appropriateness': {'mean_score': 2.2,\n",
       "       'median_score': 3.0,\n",
       "       'distribution': {'1': 2, '2': 0, '3': 3},\n",
       "       'count': 5,\n",
       "       'percentage': {'1': 40.0, '2': 0.0, '3': 60.0}},\n",
       "      'overall': {'mean_score': 2.4000000000000004}}}},\n",
       "   'llama': {'english': {'evaluations': [{'correctness': {'score': 1,\n",
       "        'justification': \"The AI's response is medically correct in advising the user to consult a healthcare provider or refer to fetal growth charts for personalized guidance on fetal growth and development. It correctly implies that determining the normality of fetal weight requires professional assessment and context.\",\n",
       "        'problematic_claims': []},\n",
       "       'completeness': {'score': 2,\n",
       "        'justification': 'While the response correctly advises consulting a healthcare provider for personalized guidance, it lacks specific information about the general expected weight range for a fetus at 22 weeks. Including a general guideline or average weight range for a 22-week-old fetus would have provided a more complete answer.',\n",
       "        'missing_information': ['General expected weight range for a 22-week-old fetus',\n",
       "         'Brief mention that 452 grams is generally considered below the average range for 22 weeks, prompting further evaluation']},\n",
       "       'clarity': {'score': 1,\n",
       "        'justification': 'The response is clear and understandable for a user of average literacy. It communicates the need for professional consultation without using overly technical language.'},\n",
       "       'cultural_appropriateness': {'score': 1,\n",
       "        'justification': \"The response is culturally appropriate for an English-speaking user. It maintains a neutral and informative tone without making assumptions about the user's background or access to healthcare.\"},\n",
       "       'question': 'My baby weight is 452 gram in 22weeks..is this normal',\n",
       "       'response': \"I don't have enough information to answer this question based on the provided guidelines. The provided context only mentions the weight of the baby at 22 weeks, which is 452 grams. However, it does not provide information on the expected weight range for a 22-week-old baby. To determine if the weight is normal, you would need to consult a healthcare provider or refer to a reliable source of fetal growth charts. They can provide personalized guidance on fetal growth and development.\",\n",
       "       'language': 'english',\n",
       "       'judge': 'gpt-4'},\n",
       "      {'correctness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'completeness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'clarity': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'cultural_appropriateness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'question': 'Sometimes I don’t feel the baby’s move ..is it okay or something else?',\n",
       "       'response': \"I don’t have enough information to answer this question based on the provided guidelines. The guidelines do not mention anything about the frequency of fetal movement. However, it does mention that newborns normally move their arms or legs or turn their head several times in a minute. If you are concerned about the baby’s movement, I would recommend consulting a healthcare provider for personalized advice. They can assess the baby's overall health and provide guidance on any concerns you may have.\",\n",
       "       'language': 'english',\n",
       "       'judge': 'gpt-4'},\n",
       "      {'correctness': {'score': 1,\n",
       "        'justification': \"The AI's response provides medically correct advice for dealing with prickly heat, including the use of cool compresses, moisturizers, over-the-counter antihistamines, and hydrocortisone cream. The recommendation to seek medical attention if signs of infection appear is also correct.\",\n",
       "        'problematic_claims': []},\n",
       "       'completeness': {'score': 2,\n",
       "        'justification': 'While the response covers the basic self-care measures and advises when to seek medical help, it could provide more detail on preventive measures to avoid prickly heat, such as wearing loose, lightweight clothing and avoiding excessive heat and humidity.',\n",
       "        'missing_information': ['Preventive measures to avoid prickly heat.',\n",
       "         'Specific examples of over-the-counter antihistamines or hydrocortisone cream.']},\n",
       "       'clarity': {'score': 1,\n",
       "        'justification': 'The response is written in clear, simple language that should be understandable to a user of average literacy. It uses straightforward instructions and avoids medical jargon that could confuse the reader.'},\n",
       "       'cultural_appropriateness': {'score': 1,\n",
       "        'justification': \"The advice given is culturally neutral and appropriate for an English-speaking audience. It does not make assumptions about the user's access to medical care or specific products, making it broadly applicable.\"},\n",
       "       'question': 'I have seen prickly heat and itching on some part of my body... What should I do?',\n",
       "       'response': \"Prickly heat is a common skin condition that occurs when the skin's sweat glands become overactive. It's usually harmless and can be treated with self-care measures. You can try applying cool compresses, taking cool showers, and using moisturizers to soothe the affected area. If the symptoms persist or worsen, it's always best to consult a healthcare provider for further guidance. They can assess the situation and provide personalized advice. In the meantime, you can try over-the-counter antihistamines or hydrocortisone cream to help alleviate itching and discomfort. However, if you experience any signs of infection, such as increased redness, swelling, or pus, seek medical attention immediately.\",\n",
       "       'language': 'english',\n",
       "       'judge': 'gpt-4'}],\n",
       "     'summary': {'correctness': {'mean_score': 1.6666666666666667,\n",
       "       'median_score': 1.0,\n",
       "       'distribution': {'1': 2, '2': 0, '3': 1},\n",
       "       'count': 3,\n",
       "       'percentage': {'1': 66.66666666666666,\n",
       "        '2': 0.0,\n",
       "        '3': 33.33333333333333}},\n",
       "      'completeness': {'mean_score': 2.3333333333333335,\n",
       "       'median_score': 2.0,\n",
       "       'distribution': {'1': 0, '2': 2, '3': 1},\n",
       "       'count': 3,\n",
       "       'percentage': {'1': 0.0,\n",
       "        '2': 66.66666666666666,\n",
       "        '3': 33.33333333333333}},\n",
       "      'clarity': {'mean_score': 1.6666666666666667,\n",
       "       'median_score': 1.0,\n",
       "       'distribution': {'1': 2, '2': 0, '3': 1},\n",
       "       'count': 3,\n",
       "       'percentage': {'1': 66.66666666666666,\n",
       "        '2': 0.0,\n",
       "        '3': 33.33333333333333}},\n",
       "      'cultural_appropriateness': {'mean_score': 1.6666666666666667,\n",
       "       'median_score': 1.0,\n",
       "       'distribution': {'1': 2, '2': 0, '3': 1},\n",
       "       'count': 3,\n",
       "       'percentage': {'1': 66.66666666666666,\n",
       "        '2': 0.0,\n",
       "        '3': 33.33333333333333}},\n",
       "      'overall': {'mean_score': 1.8333333333333335}}},\n",
       "    'hindi': {'evaluations': [{'correctness': {'score': 2,\n",
       "        'justification': \"The response provides generally correct advice for managing mild symptoms associated with early pregnancy, such as staying hydrated and engaging in light exercise. However, it inaccurately suggests consuming salt as a remedy without context, which could be misleading without specifying conditions like low blood pressure. Additionally, recommending specific types of food without knowing the individual's dietary restrictions or preferences may not be universally applicable.\",\n",
       "        'problematic_claims': ['Consuming salt without specifying conditions like low blood pressure']},\n",
       "       'completeness': {'score': 2,\n",
       "        'justification': 'The response covers basic home remedies and advises seeking medical attention for severe symptoms, which is important. However, it lacks specific guidance on when to seek medical help beyond severe symptoms, such as the importance of regular prenatal care and monitoring for signs of complications.',\n",
       "        'missing_information': ['Importance of regular prenatal care',\n",
       "         'Specific signs of complications to watch for']},\n",
       "       'clarity': {'score': 1,\n",
       "        'justification': 'The response is written in simple, clear language that should be understandable to a user of average literacy. It uses straightforward suggestions and avoids medical jargon, making it accessible.'},\n",
       "       'cultural_appropriateness': {'score': 1,\n",
       "        'justification': 'The response is culturally appropriate, considering the context of a Hindi-speaking user. It includes dietary suggestions that are common and generally acceptable in many Indian diets, such as roti and paratha, and acknowledges the practice of home remedies.'},\n",
       "       'question': 'जयदा टाइम तक बैठने से चक्कर आना खड़ा होने से पेट दर्द होने लगता हैमेरे को कौन सी टेबलेट खानी चाहिए अभी सट्टा मा लग चुका है',\n",
       "       'response': \"मैं समझता हूँ कि आपको समय से पहले गर्भावस्था हो रही है और आपको चक्कर आना और खड़े होने से पेट दर्द हो रहा है। यह सामान्य हो सकता है क्योंकि आपके शरीर में हार्मोन का स्तर बढ़ रहा है। लेकिन अगर यह दर्द और चक्कर आना गंभीर हो जाता है या आपको अन्य लक्षण जैसे कि बुखार, थकान, या सांस लेने में कठिनाई होती है, तो आपको तुरंत डॉक्टर से संपर्क करना चाहिए। \\n\\nकुछ घरेलू उपचार जो आपको मदद कर सकते हैं:\\n\\n- पानी पीना और नमक का सेवन करना\\n- हल्के व्यायाम जैसे कि चलना या स्ट्रेचिंग\\n- गर्म पानी से स्नान करना\\n- हल्के भोजन जैसे कि रोटी, पारठा, या टोस्ट खाना\\n\\nलेकिन अगर आपको लगता है कि आपके लक्षण गंभीर हो सकते हैं या आपको अन्य चिंताएं हैं, तो कृपया अपने डॉक्टर से संपर्क करें।\\n\\n(Translation: \\nI understand that you're experiencing early pregnancy and experiencing dizziness and abdominal pain when standing up. This can be normal due to the increasing levels of hormones in your body. However, if the pain and dizziness become severe or you experience other symptoms such as fever, fatigue, or difficulty breathing, you need to contact your doctor immediately. \\n\\nSome home remedies that may help:\\n\\n- Drinking water and consuming salt\\n- Light exercise such as walking or stretching\\n- Taking a warm bath\\n- Light meals such as roti, paratha, or toast\\n\\nHowever, if you think your symptoms are severe or you have other concerns, please contact your doctor.)\\n\\nPlease note that the response is based on the provided context and is intended to be reassuring and accurate. However, it's essential to consult a healthcare provider for personalized advice, especially if symptoms worsen or persist.\",\n",
       "       'language': 'hindi',\n",
       "       'judge': 'gpt-4'},\n",
       "      {'correctness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'completeness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'clarity': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'cultural_appropriateness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'question': 'सर्दी -जुकाम  हो रहा  है।इसका बच्चे पर तो कोई प्रभाव  नहीं  होगा ।',\n",
       "       'response': 'जुकाम के लक्षणों में सिरदर्द, खांसी, और सिरदर्द शामिल हो सकते हैं। जुकाम के लक्षणों में सिरदर्द, खांसी, और सिरदर्द शामिल हो सकते हैं। जुकाम के लक्षणों में सिरदर्द, खांसी, और सिरदर्द शामिल हो सकते हैं। जुकाम के लक्षणों में सिरदर्द, खांसी, और सिरदर्द शामिल हो सकते हैं। जुकाम के लक्षणों में सिरदर्द, खांसी, और सिरदर्द शामिल हो सकते हैं। जुकाम के लक्षणों में सिरदर्द, खांसी, और सिरदर्द शामिल हो सकते हैं। जुकाम के लक्षणों में सिरदर्द, खांसी, और सिरदर्द शामिल हो सकते हैं। जुकाम के लक्षणों में सिरदर्द, खांसी, और सिरदर्द शामिल हो सकते हैं। जुकाम के लक्षणों में सिरदर्द, खांसी, और सिरदर्द शामिल हो सकते हैं। जुकाम के लक्षणों में सिरदर्द, खांसी, और सिरदर्द शामिल हो सकते हैं। जुकाम के लक्षणों में सिरदर्द, खांसी, और सिरदर्द शामिल हो सकते हैं। जुकाम के लक्षणों में सिरदर्द, खांसी, और सिरदर्द शामिल हो सकते हैं। जुकाम के लक्षणों में सिरदर्द, खांसी, और सिरदर्द शामिल हो सकते हैं। जुकाम के लक्षणों में सिरदर्द, खांसी, और सिरदर्द शामिल हो सकते हैं। जुकाम के लक्षणों में सिरदर्द, खांसी, और सिरदर्द शामिल हो सकते हैं। जुकाम के लक्षणों में सिरदर्द, खां',\n",
       "       'language': 'hindi',\n",
       "       'judge': 'gpt-4'}],\n",
       "     'summary': {'correctness': {'mean_score': 2.5,\n",
       "       'median_score': 2.5,\n",
       "       'distribution': {'1': 0, '2': 1, '3': 1},\n",
       "       'count': 2,\n",
       "       'percentage': {'1': 0.0, '2': 50.0, '3': 50.0}},\n",
       "      'completeness': {'mean_score': 2.5,\n",
       "       'median_score': 2.5,\n",
       "       'distribution': {'1': 0, '2': 1, '3': 1},\n",
       "       'count': 2,\n",
       "       'percentage': {'1': 0.0, '2': 50.0, '3': 50.0}},\n",
       "      'clarity': {'mean_score': 2.0,\n",
       "       'median_score': 2.0,\n",
       "       'distribution': {'1': 1, '2': 0, '3': 1},\n",
       "       'count': 2,\n",
       "       'percentage': {'1': 50.0, '2': 0.0, '3': 50.0}},\n",
       "      'cultural_appropriateness': {'mean_score': 2.0,\n",
       "       'median_score': 2.0,\n",
       "       'distribution': {'1': 1, '2': 0, '3': 1},\n",
       "       'count': 2,\n",
       "       'percentage': {'1': 50.0, '2': 0.0, '3': 50.0}},\n",
       "      'overall': {'mean_score': 2.25}}},\n",
       "    'hinglish': {'evaluations': [{'correctness': {'score': 1,\n",
       "        'justification': 'The response is medically correct. It emphasizes the importance of consulting a healthcare provider for personalized advice, which is a safe and appropriate recommendation. There are no medically incorrect claims.',\n",
       "        'problematic_claims': []},\n",
       "       'completeness': {'score': 2,\n",
       "        'justification': 'While the response correctly advises consulting a healthcare provider, it lacks specific information about the general safety of consuming mangoes during pregnancy. Generally, mangoes are safe to eat during pregnancy in moderation, as they are rich in vitamins C and A; however, due to their high sugar content, they should be consumed in moderation.',\n",
       "        'missing_information': ['General safety of consuming mangoes during pregnancy.',\n",
       "         'Recommendation to consume in moderation due to high sugar content.']},\n",
       "       'clarity': {'score': 1,\n",
       "        'justification': 'The response is clear and understandable, even though it uses a cautious approach by not providing specific advice about mango consumption. It communicates the primary advice effectively, which is to consult a healthcare provider.'},\n",
       "       'cultural_appropriateness': {'score': 1,\n",
       "        'justification': \"The response is culturally appropriate for a Hinglish-speaking user. It respects the user's query without making assumptions about their dietary habits or preferences and maintains a neutral and respectful tone.\"},\n",
       "       'question': 'Pragnancy me mango 🥭 kitna mntra me kha skti hu',\n",
       "       'response': \"I don't have enough information to answer this question based on the provided guidelines. The guidelines do not mention mango as a recommended or restricted food item during pregnancy. However, it is always best to consult a healthcare provider for personalized advice on what foods are safe and healthy during pregnancy.\",\n",
       "       'language': 'hinglish',\n",
       "       'judge': 'gpt-4'},\n",
       "      {'correctness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'completeness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'clarity': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'cultural_appropriateness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'question': 'Bacche ko Tej Sardi ho to kaise usko maintain Karen',\n",
       "       'response': 'Bacche ko tej sardi se bachana ke liye, uske liye aapko kuchh upyogi tips follow karna hoga. \\n\\nPehla, aapko bacche ko apne aas-paas ke aise aise sthan rakha jahaan uska aaram kare aur uske shareer ko garmi mile. \\n\\nDusra, aapko bacche ko apne aas-paas ke aise aise kapde pehna hoga jo uske shareer ko garmi dete hain. \\n\\nTeesra, aapko bacche ko apne aas-paas ke aise aise sthan rakha jahaan uska aaram kare aur uske shareer ko garmi mile. \\n\\nAapko bacche ko apne aas-paas ke aise aise sthan rakha jahaan uska aaram kare aur uske shareer ko garmi mile. \\n\\nAapko bacche ko apne aas-paas ke aise aise sthan rakha jahaan uska aaram kare aur uske shareer ko garmi mile. \\n\\nAapko bacche ko apne aas-paas ke aise aise sthan rakha jahaan uska aaram kare aur uske shareer ko garmi mile. \\n\\nAapko bacche ko apne aas-paas ke aise aise sthan rakha jahaan uska aaram kare aur uske shareer ko garmi mile. \\n\\nAapko bacche ko apne aas-paas ke aise aise sthan rakha jahaan uska aaram kare aur uske shareer ko garmi mile. \\n\\nAapko bacche ko apne aas-paas ke aise aise sthan rakha jahaan uska aaram kare aur uske shareer ko garmi mile. \\n\\nAapko bacche ko apne aas-paas ke aise aise sthan rakha jahaan uska aaram kare aur uske shareer ko garmi mile. \\n\\nAapko bacche ko apne aas-paas ke aise aise sthan rakha jahaan uska aaram kare aur uske shareer ko garmi',\n",
       "       'language': 'hinglish',\n",
       "       'judge': 'gpt-4'},\n",
       "      {'correctness': {'score': 2,\n",
       "        'justification': 'The response correctly identifies that bleeding during pregnancy is not normal and could indicate a threatened or incomplete spontaneous abortion. However, it simplifies the causes and types of bleeding during pregnancy, which can also include ectopic pregnancy, implantation bleeding, or other conditions not mentioned.',\n",
       "        'problematic_claims': ['Bleeding is only associated with threatened or incomplete spontaneous abortion.']},\n",
       "       'completeness': {'score': 2,\n",
       "        'justification': 'The response covers the need to seek medical attention if bleeding occurs, which is crucial. However, it misses important information about other potential causes of bleeding in pregnancy, such as ectopic pregnancy, molar pregnancy, or cervical changes.',\n",
       "        'missing_information': ['Other potential causes of bleeding in pregnancy, such as ectopic pregnancy, molar pregnancy, or cervical changes.',\n",
       "         'The importance of early prenatal care and monitoring for any type of bleeding during pregnancy.']},\n",
       "       'clarity': {'score': 1,\n",
       "        'justification': \"The response is written in Hinglish, which is appropriate for the user's language preference. The explanation is straightforward and should be understandable to a user of average literacy.\"},\n",
       "       'cultural_appropriateness': {'score': 1,\n",
       "        'justification': 'The response is culturally appropriate for a Hinglish-speaking user, using terms and language that are likely familiar and accessible to the intended audience.'},\n",
       "       'question': 'Kya pregnancy m bleeding hoti h kya yeh normal h',\n",
       "       'response': 'Pregnancy m bleeding hoti h kya yeh normal h\\n\\nPregnancy m bleeding hoti h, lekin yeh normal nahin h. Pregnancy m bleeding hoti h, jise threatened abortion ya incomplete spontaneous abortion kaha jata hai. Yeh bleeding light hoti h, aur yeh pregnancy ke 20 week tak hoti h. Agar bleeding heavy hoti h, ya uterine tenderness hoti h, to yeh incomplete spontaneous abortion ho sakta hai. Agar bleeding light hoti h, aur uterine tenderness nahin hoti h, to yeh threatened abortion ho sakta hai.\\n\\nYadi aapko pregnancy m bleeding hoti h, to aapko apne doctor se baat karni chahiye. Unhone aapki pregnancy ki jaanch ki hogi, aur unhone aapko pata chalega ki aapki pregnancy m bleeding kitni normal hai. Agar aapki pregnancy m bleeding heavy hoti h, ya uterine tenderness hoti h, to aapko hospital mein le jaana pad sakta hai.\\n\\nYadi aapko koi dusri samasya hai, to aap mujhe puch sakte hain. Main aapki madad karne ke liye yahaan hoon.',\n",
       "       'language': 'hinglish',\n",
       "       'judge': 'gpt-4'},\n",
       "      {'correctness': {'score': 1,\n",
       "        'justification': \"The AI's response is medically correct. It accurately describes the potential impact of thyroid conditions on breastfeeding, including possible effects on milk supply and quality, and the importance of consulting a healthcare provider for personalized advice and management. There are no medically incorrect claims in the response.\",\n",
       "        'problematic_claims': []},\n",
       "       'completeness': {'score': 2,\n",
       "        'justification': 'While the response covers the critical aspect of consulting a healthcare provider for personalized advice, it lacks specific information on how exactly thyroid conditions might affect breastfeeding, such as hypothyroidism leading to a reduced milk supply or hyperthyroidism potentially causing other issues. Including examples of how thyroid conditions can specifically impact breastfeeding would provide a more comprehensive understanding.',\n",
       "        'missing_information': ['Specific examples of how hypothyroidism or hyperthyroidism can affect breastfeeding.',\n",
       "         \"Mention of the importance of monitoring the baby's growth and development as an indicator of adequate nutrition.\"]},\n",
       "       'clarity': {'score': 1,\n",
       "        'justification': 'The response is clear and should be understandable to a user of average literacy. It uses straightforward language without medical jargon that could confuse the reader. The explanation is direct and provides a clear course of action (consulting a healthcare provider).'},\n",
       "       'cultural_appropriateness': {'score': 1,\n",
       "        'justification': \"The response is culturally appropriate for a Hinglish-speaking user. It addresses the user's concern in a respectful and sensitive manner, acknowledging the importance of breastfeeding and the potential concerns a mother with a thyroid condition might have. It does not make assumptions about the user's knowledge level or access to healthcare, which is considerate of diverse backgrounds.\"},\n",
       "       'question': 'mujhe thyroid h to kya isse bhi breast milk pe asar hota h',\n",
       "       'response': \"Thyroid issues can affect breastfeeding, but it's not a straightforward issue. If you have a thyroid condition, it's essential to discuss it with your healthcare provider. They can help you understand how your thyroid condition might impact your milk supply and provide guidance on managing it. In some cases, thyroid medication may be adjusted to ensure it doesn't interfere with milk production. However, it's crucial to note that thyroid issues can also affect the quality of your milk, and your healthcare provider will work with you to ensure your baby receives the best possible nutrition. \\n\\nIt's also worth mentioning that some women with thyroid conditions may experience changes in milk supply or quality, but this can be managed with proper guidance and support. If you're concerned about your thyroid condition and breastfeeding, it's always best to consult with your healthcare provider for personalized advice. They can help you navigate any challenges and ensure a healthy breastfeeding journey for both you and your baby.\",\n",
       "       'language': 'hinglish',\n",
       "       'judge': 'gpt-4'},\n",
       "      {'correctness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'completeness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'clarity': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'cultural_appropriateness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'question': '2.5 months hogae h pregnancy ko but vomet ya kch ni hora mjhe',\n",
       "       'response': \"I don't have enough information to answer this question based on the provided guidelines. The guidelines do not provide information on the management of vomiting during pregnancy or postpartum. However, I can provide general information on the topic. Vomiting during pregnancy is a common symptom, and in most cases, it is not a cause for concern. However, if the vomiting is severe, persistent, or accompanied by other symptoms such as dizziness, fainting, or abdominal pain, it's essential to consult a healthcare provider. Similarly, if you are experiencing vomiting after delivery, it's crucial to seek medical attention if you are experiencing severe vomiting, diarrhea, or abdominal pain. Please consult a healthcare provider for personalized advice. \\n\\nPlease note that the guidelines do not provide specific information on the management of vomiting during pregnancy or postpartum. If you have concerns about your health, please consult a healthcare provider. \\n\\nFor further information, you can refer to the guidelines on postpartum care, which may provide information on managing vomiting and other postpartum complications. However, I would recommend consulting a healthcare provider for personalized advice. \\n\\nPlease note that the guidelines do not provide specific information on the management of vomiting during pregnancy or postpartum. If you have concerns about your health, please consult a healthcare provider. \\n\\nFor further information, you can refer to the guidelines on postpartum care, which may provide information on managing vomiting and other postpartum complications. However, I would recommend consulting a healthcare provider for personalized advice. \\n\\nPlease note that the guidelines do not provide specific information on the management of vomiting during pregnancy or postpartum. If you have concerns about your health, please consult a healthcare provider. \\n\\nFor further information, you can refer to the guidelines on postpartum care, which may provide information on managing vomiting and other postpartum complications. However, I would recommend consulting a healthcare provider for personalized advice. \\n\\nPlease note that the guidelines do not provide specific information on the management of vomiting during pregnancy or postpartum. If you have concerns about your health, please consult a healthcare provider. \\n\\nFor further information, you can refer to the guidelines on postpartum care, which may provide information on managing vomiting and other postpartum complications. However, I would recommend consulting a healthcare provider for personalized advice. \\n\\nPlease note that the guidelines do not provide specific information on the management of vomiting during pregnancy or postpartum. If you have concerns about your health, please consult a healthcare provider. \\n\\nFor further information, you can\",\n",
       "       'language': 'hinglish',\n",
       "       'judge': 'gpt-4'}],\n",
       "     'summary': {'correctness': {'mean_score': 2.0,\n",
       "       'median_score': 2.0,\n",
       "       'distribution': {'1': 2, '2': 1, '3': 2},\n",
       "       'count': 5,\n",
       "       'percentage': {'1': 40.0, '2': 20.0, '3': 40.0}},\n",
       "      'completeness': {'mean_score': 2.4,\n",
       "       'median_score': 2.0,\n",
       "       'distribution': {'1': 0, '2': 3, '3': 2},\n",
       "       'count': 5,\n",
       "       'percentage': {'1': 0.0, '2': 60.0, '3': 40.0}},\n",
       "      'clarity': {'mean_score': 1.8,\n",
       "       'median_score': 1.0,\n",
       "       'distribution': {'1': 3, '2': 0, '3': 2},\n",
       "       'count': 5,\n",
       "       'percentage': {'1': 60.0, '2': 0.0, '3': 40.0}},\n",
       "      'cultural_appropriateness': {'mean_score': 1.8,\n",
       "       'median_score': 1.0,\n",
       "       'distribution': {'1': 3, '2': 0, '3': 2},\n",
       "       'count': 5,\n",
       "       'percentage': {'1': 60.0, '2': 0.0, '3': 40.0}},\n",
       "      'overall': {'mean_score': 2.0}}}},\n",
       "   'llama_rag': {'english': {'evaluations': [{'correctness': {'score': 1,\n",
       "        'justification': 'The response is medically correct in stating that a healthcare provider or fetal growth chart should be consulted to determine if the weight of the baby at 22 weeks is normal. It correctly avoids providing a direct answer without the necessary context or reference to a specific growth chart.',\n",
       "        'problematic_claims': []},\n",
       "       'completeness': {'score': 2,\n",
       "        'justification': 'While the response correctly advises consulting a healthcare provider or referring to a fetal growth chart, it lacks specific information about the typical weight range for a fetus at 22 weeks. Including a general guideline or average weight range for a 22-week-old fetus would have made the response more complete.',\n",
       "        'missing_information': ['General guideline or average weight range for a 22-week-old fetus.',\n",
       "         'Brief explanation that fetal weight can vary and is influenced by multiple factors.']},\n",
       "       'clarity': {'score': 1,\n",
       "        'justification': 'The response is clear and straightforward, advising the user to consult a healthcare provider or refer to a fetal growth chart for personalized guidance. It uses simple language that is easy to understand.'},\n",
       "       'cultural_appropriateness': {'score': 1,\n",
       "        'justification': \"The response is culturally appropriate for an English-speaking user. It maintains a neutral tone and provides advice that is universally applicable without making assumptions about the user's specific cultural context.\"},\n",
       "       'question': 'My baby weight is 452 gram in 22weeks..is this normal',\n",
       "       'response': \"I don't have enough information to answer this question based on the provided guidelines. The provided context only mentions the weight of the baby at 22 weeks, which is 452 grams. However, it does not provide information on the expected weight range for a 22-week-old baby. To determine if the weight is normal, you would need to consult a healthcare provider or refer to a reliable source of fetal growth charts. They can provide personalized guidance on fetal growth and development.\",\n",
       "       'language': 'english',\n",
       "       'judge': 'gpt-4'},\n",
       "      {'correctness': {'score': 2,\n",
       "        'justification': \"The response contains a mix of correct and slightly misleading information. The reference to newborns moving several times a minute is not relevant to a question about fetal movement, which can confuse the issue. However, the advice to consult a healthcare provider if concerned about the baby's movement is correct.\",\n",
       "        'problematic_claims': ['Newborns normally move their arms or legs or turn their head several times in a minute.']},\n",
       "       'completeness': {'score': 3,\n",
       "        'justification': 'The response omits significant information about the importance of monitoring fetal movement, especially in the second and third trimesters, and how changes in movement patterns can be a sign of fetal distress. It also fails to mention the general guidance on counting kicks or movements as a way to monitor fetal health.',\n",
       "        'missing_information': ['Importance of monitoring fetal movement in the second and third trimesters',\n",
       "         'General guidance on counting kicks or movements as a way to monitor fetal health']},\n",
       "       'clarity': {'score': 2,\n",
       "        'justification': 'While the response is generally understandable, the irrelevant mention of newborn movement could confuse users about the focus of the question, which is about fetal movement, not newborn behavior.'},\n",
       "       'cultural_appropriateness': {'score': 1,\n",
       "        'justification': 'The response is culturally appropriate for an English-speaking user, focusing on the universal concern of fetal movement without making any assumptions or references that could be culturally sensitive.'},\n",
       "       'question': 'Sometimes I don’t feel the baby’s move ..is it okay or something else?',\n",
       "       'response': \"I don’t have enough information to answer this question based on the provided guidelines. The guidelines do not mention anything about the frequency of fetal movement. However, it does mention that newborns normally move their arms or legs or turn their head several times in a minute. If you are concerned about the baby’s movement, I would recommend consulting a healthcare provider for personalized advice. They can assess the baby's overall health and provide guidance on any concerns you may have.\",\n",
       "       'language': 'english',\n",
       "       'judge': 'gpt-4'},\n",
       "      {'correctness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'completeness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'clarity': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'cultural_appropriateness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'question': 'I have seen prickly heat and itching on some part of my body... What should I do?',\n",
       "       'response': \"Prickly heat is a common skin condition that occurs when the skin's sweat glands become overactive. It's usually harmless and can be treated with self-care measures. You can try applying cool compresses, taking cool showers, and using moisturizers to soothe the affected area. If the symptoms persist or worsen, it's always best to consult a healthcare provider for further guidance. They can assess the situation and provide personalized advice. In the meantime, you can try over-the-counter antihistamines or hydrocortisone cream to help alleviate itching and discomfort. However, if you experience any signs of infection, such as increased redness, swelling, or pus, seek medical attention immediately.\",\n",
       "       'language': 'english',\n",
       "       'judge': 'gpt-4'}],\n",
       "     'summary': {'correctness': {'mean_score': 2.0,\n",
       "       'median_score': 2.0,\n",
       "       'distribution': {'1': 1, '2': 1, '3': 1},\n",
       "       'count': 3,\n",
       "       'percentage': {'1': 33.33333333333333,\n",
       "        '2': 33.33333333333333,\n",
       "        '3': 33.33333333333333}},\n",
       "      'completeness': {'mean_score': 2.6666666666666665,\n",
       "       'median_score': 3.0,\n",
       "       'distribution': {'1': 0, '2': 1, '3': 2},\n",
       "       'count': 3,\n",
       "       'percentage': {'1': 0.0,\n",
       "        '2': 33.33333333333333,\n",
       "        '3': 66.66666666666666}},\n",
       "      'clarity': {'mean_score': 2.0,\n",
       "       'median_score': 2.0,\n",
       "       'distribution': {'1': 1, '2': 1, '3': 1},\n",
       "       'count': 3,\n",
       "       'percentage': {'1': 33.33333333333333,\n",
       "        '2': 33.33333333333333,\n",
       "        '3': 33.33333333333333}},\n",
       "      'cultural_appropriateness': {'mean_score': 1.6666666666666667,\n",
       "       'median_score': 1.0,\n",
       "       'distribution': {'1': 2, '2': 0, '3': 1},\n",
       "       'count': 3,\n",
       "       'percentage': {'1': 66.66666666666666,\n",
       "        '2': 0.0,\n",
       "        '3': 33.33333333333333}},\n",
       "      'overall': {'mean_score': 2.083333333333333}}},\n",
       "    'hindi': {'evaluations': [{'correctness': {'score': 2,\n",
       "        'justification': 'The response provides generally correct advice for managing mild symptoms of dizziness and abdominal pain in early pregnancy, emphasizing hydration, light exercise, warm baths, and light meals. However, it inaccurately suggests that consuming salt is a general recommendation, which may not be appropriate for all pregnant individuals, especially those with conditions like hypertension. The advice to contact a doctor if symptoms are severe or accompanied by other concerning symptoms is correct.',\n",
       "        'problematic_claims': ['Consuming salt as a general recommendation']},\n",
       "       'completeness': {'score': 2,\n",
       "        'justification': \"The response covers basic home remedies and the importance of consulting a healthcare provider for severe symptoms. However, it misses critical advice on avoiding self-medication with over-the-counter or prescribed medications without consulting a healthcare provider, especially in the context of the user's query about which tablet to take.\",\n",
       "        'missing_information': ['Avoid self-medication without consulting a healthcare provider',\n",
       "         'Specific warning signs that require immediate medical attention']},\n",
       "       'clarity': {'score': 1,\n",
       "        'justification': 'The response is clear and should be understandable to a user of average literacy. It uses simple language and provides straightforward advice.'},\n",
       "       'cultural_appropriateness': {'score': 1,\n",
       "        'justification': 'The response is culturally appropriate, using examples of food and activities familiar to Hindi-speaking users. It respects the cultural context and provides advice that should be accessible and acceptable.'},\n",
       "       'question': 'जयदा टाइम तक बैठने से चक्कर आना खड़ा होने से पेट दर्द होने लगता हैमेरे को कौन सी टेबलेट खानी चाहिए अभी सट्टा मा लग चुका है',\n",
       "       'response': \"मैं समझता हूँ कि आपको समय से पहले गर्भावस्था हो रही है और आपको चक्कर आना और खड़े होने से पेट दर्द हो रहा है। यह सामान्य हो सकता है क्योंकि आपके शरीर में हार्मोन का स्तर बढ़ रहा है। लेकिन अगर यह दर्द और चक्कर आना गंभीर हो जाता है या आपको अन्य लक्षण जैसे कि बुखार, थकान, या सांस लेने में कठिनाई होती है, तो आपको तुरंत डॉक्टर से संपर्क करना चाहिए। \\n\\nकुछ घरेलू उपचार जो आपको मदद कर सकते हैं:\\n\\n- पानी पीना और नमक का सेवन करना\\n- हल्के व्यायाम जैसे कि चलना या स्ट्रेचिंग\\n- गर्म पानी से स्नान करना\\n- हल्के भोजन जैसे कि रोटी, पारठा, या टोस्ट खाना\\n\\nलेकिन अगर आपको लगता है कि आपके लक्षण गंभीर हो सकते हैं या आपको अन्य चिंताएं हैं, तो कृपया अपने डॉक्टर से संपर्क करें।\\n\\n(Translation: \\nI understand that you're experiencing early pregnancy and experiencing dizziness and abdominal pain when standing up. This can be normal due to the increasing levels of hormones in your body. However, if the pain and dizziness become severe or you experience other symptoms such as fever, fatigue, or difficulty breathing, you need to contact your doctor immediately. \\n\\nSome home remedies that may help:\\n\\n- Drinking water and consuming salt\\n- Light exercise such as walking or stretching\\n- Taking a warm bath\\n- Light meals such as roti, paratha, or toast\\n\\nHowever, if you think your symptoms are severe or you have other concerns, please contact your doctor.)\\n\\nPlease note that the response is based on the provided context and is intended to be reassuring and accurate. However, it's essential to consult a healthcare provider for personalized advice, especially if symptoms worsen or persist.\",\n",
       "       'language': 'hindi',\n",
       "       'judge': 'gpt-4'},\n",
       "      {'correctness': {'score': 3,\n",
       "        'justification': \"The response is repetitive and does not address the user's concern about the effects of a cold on a baby. It repeatedly lists symptoms of a cold without providing any relevant information regarding the impact on an unborn or newborn child.\",\n",
       "        'problematic_claims': [\"Repetitive listing of cold symptoms without addressing the user's concern\"]},\n",
       "       'completeness': {'score': 3,\n",
       "        'justification': \"The response fails to address the user's query regarding the impact of a cold on their baby. Essential information about whether a common cold in the mother can affect the baby and any precautions or treatments would be necessary information.\",\n",
       "        'missing_information': [\"Information on the impact of a mother's cold on her baby\",\n",
       "         'Precautions or treatments if a mother has a cold']},\n",
       "       'clarity': {'score': 3,\n",
       "        'justification': \"The response is not clear due to its repetitive nature, listing symptoms of a cold multiple times without providing a coherent answer to the user's question.\"},\n",
       "       'cultural_appropriateness': {'score': 2,\n",
       "        'justification': \"While the response is in Hindi, which is appropriate for the cultural context of the user, the repetitive and irrelevant nature of the response does not provide a culturally sensitive or relevant answer to the user's concern.\"},\n",
       "       'question': 'सर्दी -जुकाम  हो रहा  है।इसका बच्चे पर तो कोई प्रभाव  नहीं  होगा ।',\n",
       "       'response': 'जुकाम के लक्षणों में सिरदर्द, खांसी, और सिरदर्द शामिल हो सकते हैं। जुकाम के लक्षणों में सिरदर्द, खांसी, और सिरदर्द शामिल हो सकते हैं। जुकाम के लक्षणों में सिरदर्द, खांसी, और सिरदर्द शामिल हो सकते हैं। जुकाम के लक्षणों में सिरदर्द, खांसी, और सिरदर्द शामिल हो सकते हैं। जुकाम के लक्षणों में सिरदर्द, खांसी, और सिरदर्द शामिल हो सकते हैं। जुकाम के लक्षणों में सिरदर्द, खांसी, और सिरदर्द शामिल हो सकते हैं। जुकाम के लक्षणों में सिरदर्द, खांसी, और सिरदर्द शामिल हो सकते हैं। जुकाम के लक्षणों में सिरदर्द, खांसी, और सिरदर्द शामिल हो सकते हैं। जुकाम के लक्षणों में सिरदर्द, खांसी, और सिरदर्द शामिल हो सकते हैं। जुकाम के लक्षणों में सिरदर्द, खांसी, और सिरदर्द शामिल हो सकते हैं। जुकाम के लक्षणों में सिरदर्द, खांसी, और सिरदर्द शामिल हो सकते हैं। जुकाम के लक्षणों में सिरदर्द, खांसी, और सिरदर्द शामिल हो सकते हैं। जुकाम के लक्षणों में सिरदर्द, खांसी, और सिरदर्द शामिल हो सकते हैं। जुकाम के लक्षणों में सिरदर्द, खांसी, और सिरदर्द शामिल हो सकते हैं। जुकाम के लक्षणों में सिरदर्द, खांसी, और सिरदर्द शामिल हो सकते हैं। जुकाम के लक्षणों में सिरदर्द, खां',\n",
       "       'language': 'hindi',\n",
       "       'judge': 'gpt-4'}],\n",
       "     'summary': {'correctness': {'mean_score': 2.5,\n",
       "       'median_score': 2.5,\n",
       "       'distribution': {'1': 0, '2': 1, '3': 1},\n",
       "       'count': 2,\n",
       "       'percentage': {'1': 0.0, '2': 50.0, '3': 50.0}},\n",
       "      'completeness': {'mean_score': 2.5,\n",
       "       'median_score': 2.5,\n",
       "       'distribution': {'1': 0, '2': 1, '3': 1},\n",
       "       'count': 2,\n",
       "       'percentage': {'1': 0.0, '2': 50.0, '3': 50.0}},\n",
       "      'clarity': {'mean_score': 2.0,\n",
       "       'median_score': 2.0,\n",
       "       'distribution': {'1': 1, '2': 0, '3': 1},\n",
       "       'count': 2,\n",
       "       'percentage': {'1': 50.0, '2': 0.0, '3': 50.0}},\n",
       "      'cultural_appropriateness': {'mean_score': 1.5,\n",
       "       'median_score': 1.5,\n",
       "       'distribution': {'1': 1, '2': 1, '3': 0},\n",
       "       'count': 2,\n",
       "       'percentage': {'1': 50.0, '2': 50.0, '3': 0.0}},\n",
       "      'overall': {'mean_score': 2.125}}},\n",
       "    'hinglish': {'evaluations': [{'correctness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'completeness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'clarity': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'cultural_appropriateness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'question': 'mujhe thyroid h to kya isse bhi breast milk pe asar hota h',\n",
       "       'response': \"Thyroid issues can affect breastfeeding, but it's not a straightforward issue. If you have a thyroid condition, it's essential to discuss it with your healthcare provider. They can help you understand how your thyroid condition might impact your milk supply and provide guidance on managing it. In some cases, thyroid medication may be adjusted to ensure it doesn't interfere with milk production. However, it's crucial to note that thyroid issues can also affect the quality of your milk, and your healthcare provider will work with you to ensure your baby receives the best possible nutrition. \\n\\nIt's also worth mentioning that some women with thyroid conditions may experience changes in milk supply or quality, but this can be managed with proper guidance and support. If you're concerned about your thyroid condition and breastfeeding, it's always best to consult with your healthcare provider for personalized advice. They can help you navigate any challenges and ensure a healthy breastfeeding journey for both you and your baby.\",\n",
       "       'language': 'hinglish',\n",
       "       'judge': 'gpt-4'},\n",
       "      {'correctness': {'score': 2,\n",
       "        'justification': 'The response correctly identifies that bleeding during pregnancy is not normal and may indicate a threatened or incomplete spontaneous abortion. However, it simplifies the range of causes for bleeding in pregnancy, which can also include implantation bleeding, ectopic pregnancy, or placental issues, among others.',\n",
       "        'problematic_claims': ['Pregnancy m bleeding hoti h, jise threatened abortion ya incomplete spontaneous abortion kaha jata hai.']},\n",
       "       'completeness': {'score': 2,\n",
       "        'justification': 'The response covers the need to seek medical advice if bleeding occurs, which is crucial. However, it lacks information on other potential causes of bleeding in pregnancy and does not mention the importance of monitoring for additional symptoms that could indicate more severe conditions.',\n",
       "        'missing_information': ['Other potential causes of bleeding in pregnancy such as ectopic pregnancy, placental issues, or cervical changes.',\n",
       "         'The importance of monitoring for additional symptoms like pain, dizziness, or passing tissue.']},\n",
       "       'clarity': {'score': 1,\n",
       "        'justification': 'The response is clear and uses simple language, making it understandable for a user of average literacy. The use of Hinglish is consistent and should be accessible to the intended audience.'},\n",
       "       'cultural_appropriateness': {'score': 1,\n",
       "        'justification': \"The response is culturally appropriate for a Hinglish-speaking user. It addresses the user's concern in a respectful manner and uses language that is likely to be familiar and comfortable for the audience.\"},\n",
       "       'question': 'Kya pregnancy m bleeding hoti h kya yeh normal h',\n",
       "       'response': 'Pregnancy m bleeding hoti h kya yeh normal h\\n\\nPregnancy m bleeding hoti h, lekin yeh normal nahin h. Pregnancy m bleeding hoti h, jise threatened abortion ya incomplete spontaneous abortion kaha jata hai. Yeh bleeding light hoti h, aur yeh pregnancy ke 20 week tak hoti h. Agar bleeding heavy hoti h, ya uterine tenderness hoti h, to yeh incomplete spontaneous abortion ho sakta hai. Agar bleeding light hoti h, aur uterine tenderness nahin hoti h, to yeh threatened abortion ho sakta hai.\\n\\nYadi aapko pregnancy m bleeding hoti h, to aapko apne doctor se baat karni chahiye. Unhone aapki pregnancy ki jaanch ki hogi, aur unhone aapko pata chalega ki aapki pregnancy m bleeding kitni normal hai. Agar aapki pregnancy m bleeding heavy hoti h, ya uterine tenderness hoti h, to aapko hospital mein le jaana pad sakta hai.\\n\\nYadi aapko koi dusri samasya hai, to aap mujhe puch sakte hain. Main aapki madad karne ke liye yahaan hoon.',\n",
       "       'language': 'hinglish',\n",
       "       'judge': 'gpt-4'},\n",
       "      {'correctness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'completeness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'clarity': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'cultural_appropriateness': {'score': 3,\n",
       "        'justification': 'Failed to parse structured response'},\n",
       "       'question': 'Bacche ko Tej Sardi ho to kaise usko maintain Karen',\n",
       "       'response': 'Bacche ko tej sardi se bachana ke liye, uske liye aapko kuchh upyogi tips follow karna hoga. \\n\\nPehla, aapko bacche ko apne aas-paas ke aise aise sthan rakha jahaan uska aaram kare aur uske shareer ko garmi mile. \\n\\nDusra, aapko bacche ko apne aas-paas ke aise aise kapde pehna hoga jo uske shareer ko garmi dete hain. \\n\\nTeesra, aapko bacche ko apne aas-paas ke aise aise sthan rakha jahaan uska aaram kare aur uske shareer ko garmi mile. \\n\\nAapko bacche ko apne aas-paas ke aise aise sthan rakha jahaan uska aaram kare aur uske shareer ko garmi mile. \\n\\nAapko bacche ko apne aas-paas ke aise aise sthan rakha jahaan uska aaram kare aur uske shareer ko garmi mile. \\n\\nAapko bacche ko apne aas-paas ke aise aise sthan rakha jahaan uska aaram kare aur uske shareer ko garmi mile. \\n\\nAapko bacche ko apne aas-paas ke aise aise sthan rakha jahaan uska aaram kare aur uske shareer ko garmi mile. \\n\\nAapko bacche ko apne aas-paas ke aise aise sthan rakha jahaan uska aaram kare aur uske shareer ko garmi mile. \\n\\nAapko bacche ko apne aas-paas ke aise aise sthan rakha jahaan uska aaram kare aur uske shareer ko garmi mile. \\n\\nAapko bacche ko apne aas-paas ke aise aise sthan rakha jahaan uska aaram kare aur uske shareer ko garmi mile. \\n\\nAapko bacche ko apne aas-paas ke aise aise sthan rakha jahaan uska aaram kare aur uske shareer ko garmi',\n",
       "       'language': 'hinglish',\n",
       "       'judge': 'gpt-4'},\n",
       "      {'correctness': {'score': 1,\n",
       "        'justification': \"The AI's response is medically correct. It emphasizes the importance of consulting a healthcare provider for personalized advice, which is a safe and appropriate recommendation. There are no medically incorrect claims made in the response.\",\n",
       "        'problematic_claims': []},\n",
       "       'completeness': {'score': 2,\n",
       "        'justification': 'While the response correctly advises consulting a healthcare provider for personalized advice, it lacks specific information about the general safety of consuming mangoes during pregnancy. Including general guidance on moderation and potential nutritional benefits or concerns (e.g., high sugar content, vitamin content) would have made the response more complete.',\n",
       "        'missing_information': ['General guidance on consuming mangoes in moderation due to their high sugar content.',\n",
       "         'Information on the nutritional benefits of mangoes, such as being a good source of vitamins C and A.']},\n",
       "       'clarity': {'score': 1,\n",
       "        'justification': \"The response is clear and understandable for a user of average literacy. It directly addresses the user's query and provides a straightforward recommendation to consult a healthcare provider for personalized advice.\"},\n",
       "       'cultural_appropriateness': {'score': 1,\n",
       "        'justification': \"The response is culturally appropriate for a hinglish-speaking user. It respects the user's query without making assumptions about their dietary habits or preferences and maintains a neutral tone that is respectful and inclusive.\"},\n",
       "       'question': 'Pragnancy me mango 🥭 kitna mntra me kha skti hu',\n",
       "       'response': \"I don't have enough information to answer this question based on the provided guidelines. The guidelines do not mention mango as a recommended or restricted food item during pregnancy. However, it is always best to consult a healthcare provider for personalized advice on what foods are safe and healthy during pregnancy.\",\n",
       "       'language': 'hinglish',\n",
       "       'judge': 'gpt-4'},\n",
       "      {'correctness': {'score': 1,\n",
       "        'justification': 'The response correctly identifies vomiting during pregnancy as a common symptom and advises seeking medical attention for severe or persistent symptoms, which aligns with medical guidelines. There are no medically incorrect claims.',\n",
       "        'problematic_claims': []},\n",
       "       'completeness': {'score': 3,\n",
       "        'justification': \"The response fails to directly address the user's situation of not experiencing vomiting at 2.5 months of pregnancy, which might be the user's actual concern. It also redundantly repeats the advice without adding new information.\",\n",
       "        'missing_information': [\"Explanation that not experiencing vomiting during pregnancy can also be normal and doesn't necessarily indicate a problem.\",\n",
       "         'Advice on when to expect symptoms like nausea or vomiting during pregnancy and when these symptoms might not occur.']},\n",
       "       'clarity': {'score': 3,\n",
       "        'justification': \"The response is repetitive and does not directly address the user's query, which can lead to confusion. The repeated paragraphs do not contribute to understanding and make the response unnecessarily long and unclear.\"},\n",
       "       'cultural_appropriateness': {'score': 1,\n",
       "        'justification': \"The response is culturally neutral and does not contain any content that would be considered inappropriate for a Hinglish-speaking user. However, it could be more tailored to the user's context by directly addressing the concern expressed in Hinglish.\"},\n",
       "       'question': '2.5 months hogae h pregnancy ko but vomet ya kch ni hora mjhe',\n",
       "       'response': \"I don't have enough information to answer this question based on the provided guidelines. The guidelines do not provide information on the management of vomiting during pregnancy or postpartum. However, I can provide general information on the topic. Vomiting during pregnancy is a common symptom, and in most cases, it is not a cause for concern. However, if the vomiting is severe, persistent, or accompanied by other symptoms such as dizziness, fainting, or abdominal pain, it's essential to consult a healthcare provider. Similarly, if you are experiencing vomiting after delivery, it's crucial to seek medical attention if you are experiencing severe vomiting, diarrhea, or abdominal pain. Please consult a healthcare provider for personalized advice. \\n\\nPlease note that the guidelines do not provide specific information on the management of vomiting during pregnancy or postpartum. If you have concerns about your health, please consult a healthcare provider. \\n\\nFor further information, you can refer to the guidelines on postpartum care, which may provide information on managing vomiting and other postpartum complications. However, I would recommend consulting a healthcare provider for personalized advice. \\n\\nPlease note that the guidelines do not provide specific information on the management of vomiting during pregnancy or postpartum. If you have concerns about your health, please consult a healthcare provider. \\n\\nFor further information, you can refer to the guidelines on postpartum care, which may provide information on managing vomiting and other postpartum complications. However, I would recommend consulting a healthcare provider for personalized advice. \\n\\nPlease note that the guidelines do not provide specific information on the management of vomiting during pregnancy or postpartum. If you have concerns about your health, please consult a healthcare provider. \\n\\nFor further information, you can refer to the guidelines on postpartum care, which may provide information on managing vomiting and other postpartum complications. However, I would recommend consulting a healthcare provider for personalized advice. \\n\\nPlease note that the guidelines do not provide specific information on the management of vomiting during pregnancy or postpartum. If you have concerns about your health, please consult a healthcare provider. \\n\\nFor further information, you can refer to the guidelines on postpartum care, which may provide information on managing vomiting and other postpartum complications. However, I would recommend consulting a healthcare provider for personalized advice. \\n\\nPlease note that the guidelines do not provide specific information on the management of vomiting during pregnancy or postpartum. If you have concerns about your health, please consult a healthcare provider. \\n\\nFor further information, you can\",\n",
       "       'language': 'hinglish',\n",
       "       'judge': 'gpt-4'}],\n",
       "     'summary': {'correctness': {'mean_score': 2.0,\n",
       "       'median_score': 2.0,\n",
       "       'distribution': {'1': 2, '2': 1, '3': 2},\n",
       "       'count': 5,\n",
       "       'percentage': {'1': 40.0, '2': 20.0, '3': 40.0}},\n",
       "      'completeness': {'mean_score': 2.6,\n",
       "       'median_score': 3.0,\n",
       "       'distribution': {'1': 0, '2': 2, '3': 3},\n",
       "       'count': 5,\n",
       "       'percentage': {'1': 0.0, '2': 40.0, '3': 60.0}},\n",
       "      'clarity': {'mean_score': 2.2,\n",
       "       'median_score': 3.0,\n",
       "       'distribution': {'1': 2, '2': 0, '3': 3},\n",
       "       'count': 5,\n",
       "       'percentage': {'1': 40.0, '2': 0.0, '3': 60.0}},\n",
       "      'cultural_appropriateness': {'mean_score': 1.8,\n",
       "       'median_score': 1.0,\n",
       "       'distribution': {'1': 3, '2': 0, '3': 2},\n",
       "       'count': 5,\n",
       "       'percentage': {'1': 60.0, '2': 0.0, '3': 40.0}},\n",
       "      'overall': {'mean_score': 2.15}}}}}},\n",
       " 'judge_comparisons': {},\n",
       " 'language_comparisons': {'gpt-4': {'gpt-4-turbo': {'correctness': {'scores': {'english': 2.3333333333333335,\n",
       "      'hindi': 2.0,\n",
       "      'hinglish': 1.0},\n",
       "     'best_language': 'hinglish',\n",
       "     'worst_language': 'english',\n",
       "     'score_range': 1.3333333333333335},\n",
       "    'completeness': {'scores': {'english': 2.6666666666666665,\n",
       "      'hindi': 2.5,\n",
       "      'hinglish': 2.0},\n",
       "     'best_language': 'hinglish',\n",
       "     'worst_language': 'english',\n",
       "     'score_range': 0.6666666666666665},\n",
       "    'clarity': {'scores': {'english': 2.3333333333333335,\n",
       "      'hindi': 2.0,\n",
       "      'hinglish': 1.0},\n",
       "     'best_language': 'hinglish',\n",
       "     'worst_language': 'english',\n",
       "     'score_range': 1.3333333333333335},\n",
       "    'cultural_appropriateness': {'scores': {'english': 2.3333333333333335,\n",
       "      'hindi': 2.0,\n",
       "      'hinglish': 1.0},\n",
       "     'best_language': 'hinglish',\n",
       "     'worst_language': 'english',\n",
       "     'score_range': 1.3333333333333335},\n",
       "    'overall': {'scores': {'english': 2.416666666666667,\n",
       "      'hindi': 2.125,\n",
       "      'hinglish': 1.25},\n",
       "     'best_language': 'hinglish',\n",
       "     'worst_language': 'english',\n",
       "     'score_range': 1.166666666666667}},\n",
       "   'gpt-4-turbo_rag': {'correctness': {'scores': {'english': 1.0,\n",
       "      'hindi': 2.0,\n",
       "      'hinglish': 2.2},\n",
       "     'best_language': 'english',\n",
       "     'worst_language': 'hinglish',\n",
       "     'score_range': 1.2000000000000002},\n",
       "    'completeness': {'scores': {'english': 2.0, 'hindi': 2.5, 'hinglish': 2.6},\n",
       "     'best_language': 'english',\n",
       "     'worst_language': 'hinglish',\n",
       "     'score_range': 0.6000000000000001},\n",
       "    'clarity': {'scores': {'english': 1.0, 'hindi': 2.0, 'hinglish': 2.2},\n",
       "     'best_language': 'english',\n",
       "     'worst_language': 'hinglish',\n",
       "     'score_range': 1.2000000000000002},\n",
       "    'cultural_appropriateness': {'scores': {'english': 1.0,\n",
       "      'hindi': 2.0,\n",
       "      'hinglish': 2.2},\n",
       "     'best_language': 'english',\n",
       "     'worst_language': 'hinglish',\n",
       "     'score_range': 1.2000000000000002},\n",
       "    'overall': {'scores': {'english': 1.25,\n",
       "      'hindi': 2.125,\n",
       "      'hinglish': 2.3000000000000003},\n",
       "     'best_language': 'english',\n",
       "     'worst_language': 'hinglish',\n",
       "     'score_range': 1.0500000000000003}},\n",
       "   'mixtral': {'correctness': {'scores': {'english': 2.3333333333333335,\n",
       "      'hindi': 2.0,\n",
       "      'hinglish': 2.6},\n",
       "     'best_language': 'hindi',\n",
       "     'worst_language': 'hinglish',\n",
       "     'score_range': 0.6000000000000001},\n",
       "    'completeness': {'scores': {'english': 2.6666666666666665,\n",
       "      'hindi': 2.5,\n",
       "      'hinglish': 2.8},\n",
       "     'best_language': 'hindi',\n",
       "     'worst_language': 'hinglish',\n",
       "     'score_range': 0.2999999999999998},\n",
       "    'clarity': {'scores': {'english': 2.3333333333333335,\n",
       "      'hindi': 2.0,\n",
       "      'hinglish': 2.6},\n",
       "     'best_language': 'hindi',\n",
       "     'worst_language': 'hinglish',\n",
       "     'score_range': 0.6000000000000001},\n",
       "    'cultural_appropriateness': {'scores': {'english': 2.3333333333333335,\n",
       "      'hindi': 2.0,\n",
       "      'hinglish': 2.6},\n",
       "     'best_language': 'hindi',\n",
       "     'worst_language': 'hinglish',\n",
       "     'score_range': 0.6000000000000001},\n",
       "    'overall': {'scores': {'english': 2.416666666666667,\n",
       "      'hindi': 2.125,\n",
       "      'hinglish': 2.65},\n",
       "     'best_language': 'hindi',\n",
       "     'worst_language': 'hinglish',\n",
       "     'score_range': 0.5249999999999999}},\n",
       "   'mixtral_rag': {'correctness': {'scores': {'english': 2.0,\n",
       "      'hindi': 3.0,\n",
       "      'hinglish': 2.4},\n",
       "     'best_language': 'english',\n",
       "     'worst_language': 'hindi',\n",
       "     'score_range': 1.0},\n",
       "    'completeness': {'scores': {'english': 2.6666666666666665,\n",
       "      'hindi': 3.0,\n",
       "      'hinglish': 2.6},\n",
       "     'best_language': 'hinglish',\n",
       "     'worst_language': 'hindi',\n",
       "     'score_range': 0.3999999999999999},\n",
       "    'clarity': {'scores': {'english': 1.6666666666666667,\n",
       "      'hindi': 3.0,\n",
       "      'hinglish': 2.4},\n",
       "     'best_language': 'english',\n",
       "     'worst_language': 'hindi',\n",
       "     'score_range': 1.3333333333333333},\n",
       "    'cultural_appropriateness': {'scores': {'english': 1.6666666666666667,\n",
       "      'hindi': 3.0,\n",
       "      'hinglish': 2.2},\n",
       "     'best_language': 'english',\n",
       "     'worst_language': 'hindi',\n",
       "     'score_range': 1.3333333333333333},\n",
       "    'overall': {'scores': {'english': 2.0,\n",
       "      'hindi': 3.0,\n",
       "      'hinglish': 2.4000000000000004},\n",
       "     'best_language': 'english',\n",
       "     'worst_language': 'hindi',\n",
       "     'score_range': 1.0}},\n",
       "   'llama': {'correctness': {'scores': {'english': 1.6666666666666667,\n",
       "      'hindi': 2.5,\n",
       "      'hinglish': 2.0},\n",
       "     'best_language': 'english',\n",
       "     'worst_language': 'hindi',\n",
       "     'score_range': 0.8333333333333333},\n",
       "    'completeness': {'scores': {'english': 2.3333333333333335,\n",
       "      'hindi': 2.5,\n",
       "      'hinglish': 2.4},\n",
       "     'best_language': 'english',\n",
       "     'worst_language': 'hindi',\n",
       "     'score_range': 0.16666666666666652},\n",
       "    'clarity': {'scores': {'english': 1.6666666666666667,\n",
       "      'hindi': 2.0,\n",
       "      'hinglish': 1.8},\n",
       "     'best_language': 'english',\n",
       "     'worst_language': 'hindi',\n",
       "     'score_range': 0.33333333333333326},\n",
       "    'cultural_appropriateness': {'scores': {'english': 1.6666666666666667,\n",
       "      'hindi': 2.0,\n",
       "      'hinglish': 1.8},\n",
       "     'best_language': 'english',\n",
       "     'worst_language': 'hindi',\n",
       "     'score_range': 0.33333333333333326},\n",
       "    'overall': {'scores': {'english': 1.8333333333333335,\n",
       "      'hindi': 2.25,\n",
       "      'hinglish': 2.0},\n",
       "     'best_language': 'english',\n",
       "     'worst_language': 'hindi',\n",
       "     'score_range': 0.4166666666666665}},\n",
       "   'llama_rag': {'correctness': {'scores': {'english': 2.0,\n",
       "      'hindi': 2.5,\n",
       "      'hinglish': 2.0},\n",
       "     'best_language': 'english',\n",
       "     'worst_language': 'hindi',\n",
       "     'score_range': 0.5},\n",
       "    'completeness': {'scores': {'english': 2.6666666666666665,\n",
       "      'hindi': 2.5,\n",
       "      'hinglish': 2.6},\n",
       "     'best_language': 'hindi',\n",
       "     'worst_language': 'english',\n",
       "     'score_range': 0.16666666666666652},\n",
       "    'clarity': {'scores': {'english': 2.0, 'hindi': 2.0, 'hinglish': 2.2},\n",
       "     'best_language': 'english',\n",
       "     'worst_language': 'hinglish',\n",
       "     'score_range': 0.20000000000000018},\n",
       "    'cultural_appropriateness': {'scores': {'english': 1.6666666666666667,\n",
       "      'hindi': 1.5,\n",
       "      'hinglish': 1.8},\n",
       "     'best_language': 'hindi',\n",
       "     'worst_language': 'hinglish',\n",
       "     'score_range': 0.30000000000000004},\n",
       "    'overall': {'scores': {'english': 2.083333333333333,\n",
       "      'hindi': 2.125,\n",
       "      'hinglish': 2.15},\n",
       "     'best_language': 'english',\n",
       "     'worst_language': 'hinglish',\n",
       "     'score_range': 0.06666666666666687}}}},\n",
       " 'rag_vs_nonrag_comparisons': {'gpt-4': {'gpt-4-turbo_vs_gpt-4-turbo_rag': {'hindi': {'correctness': {'non_rag_score': 2.0,\n",
       "      'rag_score': 2.0,\n",
       "      'difference': 0.0,\n",
       "      'winner': 'non_rag'},\n",
       "     'completeness': {'non_rag_score': 2.5,\n",
       "      'rag_score': 2.5,\n",
       "      'difference': 0.0,\n",
       "      'winner': 'non_rag'},\n",
       "     'clarity': {'non_rag_score': 2.0,\n",
       "      'rag_score': 2.0,\n",
       "      'difference': 0.0,\n",
       "      'winner': 'non_rag'},\n",
       "     'cultural_appropriateness': {'non_rag_score': 2.0,\n",
       "      'rag_score': 2.0,\n",
       "      'difference': 0.0,\n",
       "      'winner': 'non_rag'},\n",
       "     'overall': {'non_rag_score': 2.125,\n",
       "      'rag_score': 2.125,\n",
       "      'difference': 0.0,\n",
       "      'winner': 'non_rag'}},\n",
       "    'english': {'correctness': {'non_rag_score': 2.3333333333333335,\n",
       "      'rag_score': 1.0,\n",
       "      'difference': -1.3333333333333335,\n",
       "      'winner': 'rag'},\n",
       "     'completeness': {'non_rag_score': 2.6666666666666665,\n",
       "      'rag_score': 2.0,\n",
       "      'difference': -0.6666666666666665,\n",
       "      'winner': 'rag'},\n",
       "     'clarity': {'non_rag_score': 2.3333333333333335,\n",
       "      'rag_score': 1.0,\n",
       "      'difference': -1.3333333333333335,\n",
       "      'winner': 'rag'},\n",
       "     'cultural_appropriateness': {'non_rag_score': 2.3333333333333335,\n",
       "      'rag_score': 1.0,\n",
       "      'difference': -1.3333333333333335,\n",
       "      'winner': 'rag'},\n",
       "     'overall': {'non_rag_score': 2.416666666666667,\n",
       "      'rag_score': 1.25,\n",
       "      'difference': -1.166666666666667,\n",
       "      'winner': 'rag'}},\n",
       "    'hinglish': {'correctness': {'non_rag_score': 1.0,\n",
       "      'rag_score': 2.2,\n",
       "      'difference': 1.2000000000000002,\n",
       "      'winner': 'non_rag'},\n",
       "     'completeness': {'non_rag_score': 2.0,\n",
       "      'rag_score': 2.6,\n",
       "      'difference': 0.6000000000000001,\n",
       "      'winner': 'non_rag'},\n",
       "     'clarity': {'non_rag_score': 1.0,\n",
       "      'rag_score': 2.2,\n",
       "      'difference': 1.2000000000000002,\n",
       "      'winner': 'non_rag'},\n",
       "     'cultural_appropriateness': {'non_rag_score': 1.0,\n",
       "      'rag_score': 2.2,\n",
       "      'difference': 1.2000000000000002,\n",
       "      'winner': 'non_rag'},\n",
       "     'overall': {'non_rag_score': 1.25,\n",
       "      'rag_score': 2.3000000000000003,\n",
       "      'difference': 1.0500000000000003,\n",
       "      'winner': 'non_rag'}}},\n",
       "   'mixtral_vs_mixtral_rag': {'hindi': {'correctness': {'non_rag_score': 2.0,\n",
       "      'rag_score': 3.0,\n",
       "      'difference': 1.0,\n",
       "      'winner': 'non_rag'},\n",
       "     'completeness': {'non_rag_score': 2.5,\n",
       "      'rag_score': 3.0,\n",
       "      'difference': 0.5,\n",
       "      'winner': 'non_rag'},\n",
       "     'clarity': {'non_rag_score': 2.0,\n",
       "      'rag_score': 3.0,\n",
       "      'difference': 1.0,\n",
       "      'winner': 'non_rag'},\n",
       "     'cultural_appropriateness': {'non_rag_score': 2.0,\n",
       "      'rag_score': 3.0,\n",
       "      'difference': 1.0,\n",
       "      'winner': 'non_rag'},\n",
       "     'overall': {'non_rag_score': 2.125,\n",
       "      'rag_score': 3.0,\n",
       "      'difference': 0.875,\n",
       "      'winner': 'non_rag'}},\n",
       "    'english': {'correctness': {'non_rag_score': 2.3333333333333335,\n",
       "      'rag_score': 2.0,\n",
       "      'difference': -0.3333333333333335,\n",
       "      'winner': 'rag'},\n",
       "     'completeness': {'non_rag_score': 2.6666666666666665,\n",
       "      'rag_score': 2.6666666666666665,\n",
       "      'difference': 0.0,\n",
       "      'winner': 'non_rag'},\n",
       "     'clarity': {'non_rag_score': 2.3333333333333335,\n",
       "      'rag_score': 1.6666666666666667,\n",
       "      'difference': -0.6666666666666667,\n",
       "      'winner': 'rag'},\n",
       "     'cultural_appropriateness': {'non_rag_score': 2.3333333333333335,\n",
       "      'rag_score': 1.6666666666666667,\n",
       "      'difference': -0.6666666666666667,\n",
       "      'winner': 'rag'},\n",
       "     'overall': {'non_rag_score': 2.416666666666667,\n",
       "      'rag_score': 2.0,\n",
       "      'difference': -0.41666666666666696,\n",
       "      'winner': 'rag'}},\n",
       "    'hinglish': {'correctness': {'non_rag_score': 2.6,\n",
       "      'rag_score': 2.4,\n",
       "      'difference': -0.20000000000000018,\n",
       "      'winner': 'rag'},\n",
       "     'completeness': {'non_rag_score': 2.8,\n",
       "      'rag_score': 2.6,\n",
       "      'difference': -0.19999999999999973,\n",
       "      'winner': 'rag'},\n",
       "     'clarity': {'non_rag_score': 2.6,\n",
       "      'rag_score': 2.4,\n",
       "      'difference': -0.20000000000000018,\n",
       "      'winner': 'rag'},\n",
       "     'cultural_appropriateness': {'non_rag_score': 2.6,\n",
       "      'rag_score': 2.2,\n",
       "      'difference': -0.3999999999999999,\n",
       "      'winner': 'rag'},\n",
       "     'overall': {'non_rag_score': 2.65,\n",
       "      'rag_score': 2.4000000000000004,\n",
       "      'difference': -0.24999999999999956,\n",
       "      'winner': 'rag'}}},\n",
       "   'llama_vs_llama_rag': {'hindi': {'correctness': {'non_rag_score': 2.5,\n",
       "      'rag_score': 2.5,\n",
       "      'difference': 0.0,\n",
       "      'winner': 'non_rag'},\n",
       "     'completeness': {'non_rag_score': 2.5,\n",
       "      'rag_score': 2.5,\n",
       "      'difference': 0.0,\n",
       "      'winner': 'non_rag'},\n",
       "     'clarity': {'non_rag_score': 2.0,\n",
       "      'rag_score': 2.0,\n",
       "      'difference': 0.0,\n",
       "      'winner': 'non_rag'},\n",
       "     'cultural_appropriateness': {'non_rag_score': 2.0,\n",
       "      'rag_score': 1.5,\n",
       "      'difference': -0.5,\n",
       "      'winner': 'rag'},\n",
       "     'overall': {'non_rag_score': 2.25,\n",
       "      'rag_score': 2.125,\n",
       "      'difference': -0.125,\n",
       "      'winner': 'rag'}},\n",
       "    'english': {'correctness': {'non_rag_score': 1.6666666666666667,\n",
       "      'rag_score': 2.0,\n",
       "      'difference': 0.33333333333333326,\n",
       "      'winner': 'non_rag'},\n",
       "     'completeness': {'non_rag_score': 2.3333333333333335,\n",
       "      'rag_score': 2.6666666666666665,\n",
       "      'difference': 0.33333333333333304,\n",
       "      'winner': 'non_rag'},\n",
       "     'clarity': {'non_rag_score': 1.6666666666666667,\n",
       "      'rag_score': 2.0,\n",
       "      'difference': 0.33333333333333326,\n",
       "      'winner': 'non_rag'},\n",
       "     'cultural_appropriateness': {'non_rag_score': 1.6666666666666667,\n",
       "      'rag_score': 1.6666666666666667,\n",
       "      'difference': 0.0,\n",
       "      'winner': 'non_rag'},\n",
       "     'overall': {'non_rag_score': 1.8333333333333335,\n",
       "      'rag_score': 2.083333333333333,\n",
       "      'difference': 0.24999999999999956,\n",
       "      'winner': 'non_rag'}},\n",
       "    'hinglish': {'correctness': {'non_rag_score': 2.0,\n",
       "      'rag_score': 2.0,\n",
       "      'difference': 0.0,\n",
       "      'winner': 'non_rag'},\n",
       "     'completeness': {'non_rag_score': 2.4,\n",
       "      'rag_score': 2.6,\n",
       "      'difference': 0.20000000000000018,\n",
       "      'winner': 'non_rag'},\n",
       "     'clarity': {'non_rag_score': 1.8,\n",
       "      'rag_score': 2.2,\n",
       "      'difference': 0.40000000000000013,\n",
       "      'winner': 'non_rag'},\n",
       "     'cultural_appropriateness': {'non_rag_score': 1.8,\n",
       "      'rag_score': 1.8,\n",
       "      'difference': 0.0,\n",
       "      'winner': 'non_rag'},\n",
       "     'overall': {'non_rag_score': 2.0,\n",
       "      'rag_score': 2.15,\n",
       "      'difference': 0.1499999999999999,\n",
       "      'winner': 'non_rag'}}}}}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
