{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2725100/2819816299.py:6: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index loaded successfully with 3020 vvectors.\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "import os\n",
    "\n",
    "# Load embeddings model (use OpenAI or HuggingFace embeddings)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "# Path where FAISS index is stored\n",
    "faiss_index_path = \"/data/user_data/vidhij2/medical_db/heirchical_chunking_rag\"\n",
    "\n",
    "# Load FAISS vector store\n",
    "vector_store = FAISS.load_local(folder_path=faiss_index_path, embeddings=embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "print(f\"FAISS index loaded successfully with {vector_store.index.ntotal} vvectors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3910775/3314463875.py:7: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retrieved_chunks = retriever.get_relevant_documents(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Chunk 1: Pregnant now or within the last year?\n",
      "Get medical care right away if you experience any of the following symptoms:\n",
      "These could be signs of very serious complications. If you can’t reach a healthcare provider, go to the\n",
      "emergency room. Be sure to tell them you are pregnant or were pregnant within the...\n",
      "\n",
      "Retrieved Chunk 2: Normal symptoms during pregnancy – Nausea and vomiting, heartburn, constipation, and increased\n",
      "frequency of urination. These symptoms may cause discomfort to the woman.\n",
      "Examine symptoms of complications – Fever, persistent vomiting with dehydration, palpitations,\n",
      "tiredness, breathlessness at rest/on...\n",
      "\n",
      "Retrieved Chunk 3: medical facility when\n",
      "labour starts or in case\n",
      "of a complication.\n",
      "• Adequate finance and\n",
      "transport should be\n",
      "arranged beforehand.\n",
      "• A blood donor should\n",
      "be identified for any\n",
      "unforeseen emergencies.\n",
      "Care and support by husband\n",
      "and mother-in-law gives\n",
      "emotional support and confidence\n",
      "in child bearing...\n",
      "\n",
      "Retrieved Chunk 4: 5.  Consult to a doctor as soon as you experience any of the nine danger signs – 1) Excessive swelling in the feet and puf ﬁ ness on face\n",
      "2) Difﬁ culty in breathing 3) Severe pain and burning sensation during urination 4) Repeated fever during pregnancy or within one month\n",
      "of delivery 5) Severe stom...\n",
      "\n",
      "Retrieved Chunk 5: by the government\n",
      "2. Early identification of complications – recognising danger signs during pregnancy, labour and after delivery/\n",
      "abortion...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n",
    "\n",
    "# Example medical query\n",
    "query = \"What are the early signs of pregnancy complications?\"\n",
    "\n",
    "# Retrieve relevant document chunks\n",
    "retrieved_chunks = retriever.get_relevant_documents(query)\n",
    "\n",
    "# Print retrieved content\n",
    "for i, chunk in enumerate(retrieved_chunks):\n",
    "    print(f\"Retrieved Chunk {i+1}: {chunk.page_content[:300]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 retrieved chunks using MMR:\n",
      "\n",
      "page_content='likely to express her particular needs and wants while planning for the delivery.  \n",
      "Antenatal Care and Skilled Attendance at Birth by ANMs/LHVs/SNsGUIDELINES\n",
      "10\n",
      "/circle6 All women in the reproductive age group should be advised to have folic acid for 2–3\n",
      "months pre-conception and continue with it during the fi rst 12 weeks of pregnancy.\n",
      "Th  is remarkably reduces the incidence of neural tube defects in the foetus. A daily\n",
      "dose of 400 μg folic acid taken orally is the recommended daily dose.\n",
      "/circle6 Low iodine levels during pregnancy can cause cretinism, which can lead to mental/\n",
      "physical retardation of the baby. So regular consumption of iodated salts is advised, as\n",
      "a prophylactic measures.\n",
      "Detection of pregnancy\n",
      "Th e simplest way for you to confi  rm pregnancy in the fi rst trimester is to conduct a urine\n",
      "examination using a pregnancy test kit. Th e pregnancy test should be off ered to any woman\n",
      "in the reproductive age group who comes to you with a history of amenorrhoea or symptoms' metadata={'subsection': 'Counselling', 'section_path': ['Counselling'], 'document_title': '', 'document_type': '', 'chunk_type': 'text', 'chunk_index': 5, 'total_chunks': 19}\n",
      "Chunk 1: likely to express her particular needs and wants while planning for the delivery.  \n",
      "Antenatal Care and Skilled Attendance at Birth by ANMs/LHVs/SNsGUIDELINES\n",
      "10\n",
      "/circle6 All women in the reproductive age group should be advised to have folic acid for 2–3\n",
      "months pre-conception and continue with it du...\n",
      "\n",
      "Chunk 2: During Pregnancy...\n",
      "\n",
      "Chunk 3: Pregnancy Working Group. Consensus\n",
      "statement on folic acid supplementation during\n",
      "pregnancy. Geneva; 2015 (https:/ /www.pmi.gov/\n",
      "docs/ default-source/ default-document-library/\n",
      "tools-curricula/ consensus-statement-folic-\n",
      "acid-supplementation-during-pregnancy.pdf,\n",
      "accessed 26 September 2016).\n",
      "156. Ei...\n",
      "\n",
      "Chunk 4: One tablet of IFA is to be taken daily starting from the fourth month\n",
      "of pregnancy. If you are anaemic, you will be advised to take two IFA\n",
      "tablets daily, one tablet in the morning and one in the evening.\n",
      "Taking one IFA tablet a day keeps anaemia away in mothers\n",
      "and ensures delivery of a healthy bab...\n",
      "\n",
      "Chunk 5: of elemental iron a and 2800 µg (2.8mg) of folic acid once weekly is recommended for pregnant\n",
      "women to improve maternal and neonatal outcomes if daily iron is not acceptable due to side-\n",
      "effects, and in populations with an anaemia prevalence among pregnant women of less than\n",
      "20%. (Context-specific r...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "retriever = vector_store.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 5, \"fetch_k\": 10})\n",
    "\n",
    "query = \"How does folic acid help during pregnancy?\"\n",
    "retrieved_chunks = retriever.get_relevant_documents(query)\n",
    "\n",
    "print(f\"Top {len(retrieved_chunks)} retrieved chunks using MMR:\\n\")\n",
    "print(retrieved_chunks[0])\n",
    "for i, chunk in enumerate(retrieved_chunks):\n",
    "    print(f\"Chunk {i+1}: {chunk.page_content[:300]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x7f4d8e5bde20>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from typing import List, Dict, Any, Optional, Tuple, Union\n",
    "class MedicalRAG:\n",
    "    \"\"\"RAG system for medical documents with advanced filtering and retrieval.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        vector_store: FAISS,\n",
    "        model_name: str = \"gpt-4-turbo\",\n",
    "        temperature: float = 0.0,\n",
    "    ):\n",
    "        self.vector_store = vector_store\n",
    "        self.llm = ChatOpenAI(model_name=model_name, temperature=temperature)\n",
    "        \n",
    "        # Create RAG prompt template\n",
    "        self.rag_prompt = PromptTemplate(\n",
    "            template=\"\"\"You are a medical information assistant that helps healthcare professionals by providing evidence-based information from medical guidelines and literature.\n",
    "\n",
    "Context information from medical documents:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Instructions:\n",
    "1. Answer based only on the provided context. If the information isn't in the context, say \"I don't have enough information to answer this question based on the provided medical guidelines.\"\n",
    "2. Cite specific recommendations, evidence levels, and document sources when available.\n",
    "3. Be concise but comprehensive.\n",
    "4. If multiple recommendations or conflicting guidance exists in the context, present all perspectives.\n",
    "5. When quoting recommendations, preserve their exact wording.\n",
    "\n",
    "Answer:\"\"\",\n",
    "            input_variables=[\"context\", \"question\"]\n",
    "        )\n",
    "        \n",
    "        # Setup retrieval chain\n",
    "        self.retriever = vector_store.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\"k\": 6}\n",
    "        )\n",
    "        \n",
    "        self.qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm=self.llm,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=self.retriever,\n",
    "            return_source_documents=True,\n",
    "            chain_type_kwargs={\"prompt\": self.rag_prompt}\n",
    "        )\n",
    "    \n",
    "    def query(self, question: str, filters: Dict[str, Any] = None) -> Dict[str, Any]:\n",
    "        \"\"\"Query the RAG system with optional metadata filtering.\"\"\"\n",
    "        if filters:\n",
    "            # Apply metadata filters to the retriever\n",
    "            self.retriever.search_kwargs[\"filter\"] = filters\n",
    "        \n",
    "        # Get answer\n",
    "        result = self.qa_chain({\"query\": question})\n",
    "        \n",
    "        # Format response\n",
    "        sources = []\n",
    "        for doc in result.get(\"source_documents\", []):\n",
    "            # Add source information\n",
    "            source_info = {\n",
    "                \"content\": doc.page_content[:200] + \"...\" if len(doc.page_content) > 200 else doc.page_content,\n",
    "                \"metadata\": doc.metadata\n",
    "            }\n",
    "            sources.append(source_info)\n",
    "        \n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"answer\": result[\"result\"],\n",
    "            \"sources\": sources\n",
    "        }\n",
    "    \n",
    "    def query_recommendations(self, question: str, evidence_level: str = None) -> Dict[str, Any]:\n",
    "        \"\"\"Query specifically for medical recommendations with optional evidence filtering.\"\"\"\n",
    "        filters = {\"chunk_type\": \"recommendation\"}\n",
    "        \n",
    "        if evidence_level:\n",
    "            # Filter by evidence level (e.g., \"high\", \"moderate\", \"low\")\n",
    "            filters[\"evidence_level\"] = {\"$regex\": f\"{evidence_level.lower()}.*evidence\"}\n",
    "        \n",
    "        return self.query(question, filters)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3919087/195178156.py:15: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  self.llm = ChatOpenAI(model_name=model_name, temperature=temperature)\n",
      "/tmp/ipykernel_3919087/195178156.py:58: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = self.qa_chain({\"query\": question})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "QUESTION:\n",
      "How does folic acid help during pregnancy?\n",
      "\n",
      "ANSWER:\n",
      "Folic acid supplementation during pregnancy is crucial for preventing neural tube defects in the fetus. According to the guidelines provided:\n",
      "\n",
      "1. **Pre-conception and Early Pregnancy**: It is recommended that all women in the reproductive age group should take folic acid for 2–3 months pre-conception and continue during the first 12 weeks of pregnancy. The recommended daily dose is 400 μg taken orally. This practice remarkably reduces the incidence of neural tube defects in the fetus (Antenatal Care and Skilled Attendance at Birth by ANMs/LHVs/SNsGUIDELINES).\n",
      "\n",
      "2. **Evidence Supporting Folic Acid Supplementation**: The recommendation to commence folic acid as early as possible, ideally before conception, is based on evidence that it prevents neural tube defects. This recommendation supersedes the previous WHO recommendation found in the 2012 Guideline: daily iron and folic acid supplementation in pregnant women (WHO recommendations on antenatal care for a positive pregnancy experience).\n",
      "\n",
      "These guidelines emphasize the importance of folic acid in early pregnancy to support the normal development of the baby's brain and spinal cord, highlighting its role in significantly reducing the risk of neural tube defects.\n",
      "\n",
      "SOURCES:\n",
      "\n",
      "1. \n",
      "   Section: Counselling\n",
      "   Type: text\n",
      "\n",
      "2. antenatal care \n",
      "for a positive pregnancy experience\n",
      "\n",
      "WHO Library Cataloguing-in-Publication Data\n",
      "WHO recommendations on antenatal care for a positive pregnancy experience\n",
      "   Section: 3 Evidence and recommendations\n",
      "   Type: text\n",
      "\n",
      "3. \n",
      "   Section: These cards are designed for individual counselling but may be used for group counselling if it is possible to form a homogenous group of\n",
      "   Type: text\n",
      "\n",
      "4. antenatal care \n",
      "for a positive pregnancy experience\n",
      "\n",
      "WHO Library Cataloguing-in-Publication Data\n",
      "WHO recommendations on antenatal care for a positive pregnancy experience\n",
      "   Section: 9 Blood and Nutrition. In: British > Roll Back Malaria Partnership Malaria in\n",
      "   Type: text\n",
      "\n",
      "5. antenatal care \n",
      "for a positive pregnancy experience\n",
      "\n",
      "WHO Library Cataloguing-in-Publication Data\n",
      "WHO recommendations on antenatal care for a positive pregnancy experience\n",
      "   Section: 4 Implementation of the ANC guideline and recommendations\n",
      "   Type: text\n",
      "\n",
      "6. \n",
      "   Section: 18 Screening for Syphilis\n",
      "   Type: text\n"
     ]
    }
   ],
   "source": [
    "rag = MedicalRAG(vector_store)\n",
    "    \n",
    "    # # Run query if provided\n",
    "    # if args.query:\n",
    "    #     if args.filter_recommendations:\n",
    "    #         result = rag.query_recommendations(args.query, args.evidence_level)\n",
    "    #     else:\n",
    "query=\"How does folic acid help during pregnancy?\"\n",
    "result = rag.query(query)\n",
    "        \n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"QUESTION:\")\n",
    "print(result[\"question\"])\n",
    "print(\"\\nANSWER:\")\n",
    "print(result[\"answer\"])\n",
    "print(\"\\nSOURCES:\")\n",
    "for i, source in enumerate(result[\"sources\"]):\n",
    "    print(f\"\\n{i+1}. {source['metadata'].get('document_title', 'Unknown document')}\")\n",
    "    if \"section_path\" in source[\"metadata\"]:\n",
    "        print(f\"   Section: {' > '.join(source['metadata']['section_path'])}\")\n",
    "    if \"chunk_type\" in source[\"metadata\"]:\n",
    "        print(f\"   Type: {source['metadata']['chunk_type']}\")\n",
    "    if \"evidence_level\" in source[\"metadata\"]:\n",
    "        print(f\"   Evidence level: {source['metadata']['evidence_level']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from typing import List, Dict, Any, Optional, Tuple, Union\n",
    "class MedicalRAG:\n",
    "    \"\"\"RAG system for medical documents with advanced filtering and retrieval.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        vector_store: FAISS,\n",
    "        model_name: str = \"gpt-4-turbo\",\n",
    "        temperature: float = 0.0,\n",
    "    ):\n",
    "        self.vector_store = vector_store\n",
    "        self.llm = ChatOpenAI(model_name=model_name, temperature=temperature)\n",
    "        \n",
    "        # Create RAG prompt template\n",
    "        self.rag_prompt = PromptTemplate(\n",
    "            template=\"\"\"You are a medical information assistant that helps healthcare professionals by providing evidence-based information from medical guidelines and literature.\n",
    "\n",
    "Context information from medical documents:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Instructions:\n",
    "1. Answer based only on the provided context. If the information isn't in the context, say \"I don't have enough information to answer this question based on the provided medical guidelines.\"\n",
    "2. Cite specific recommendations, evidence levels, and document sources when available.\n",
    "3. Be concise but comprehensive.\n",
    "4. If multiple recommendations or conflicting guidance exists in the context, present all perspectives.\n",
    "5. When quoting recommendations, preserve their exact wording.\n",
    "\n",
    "Answer:\"\"\",\n",
    "            input_variables=[\"context\", \"question\"]\n",
    "        )\n",
    "        \n",
    "        # Setup retrieval chain\n",
    "        self.retriever = vector_store.as_retriever(\n",
    "            search_type=\"mmr\",\n",
    "            search_kwargs={\"k\": 6}\n",
    "        )\n",
    "        \n",
    "        self.qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm=self.llm,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=self.retriever,\n",
    "            return_source_documents=True,\n",
    "            chain_type_kwargs={\"prompt\": self.rag_prompt}\n",
    "        )\n",
    "    \n",
    "    def query(self, question: str, filters: Dict[str, Any] = None) -> Dict[str, Any]:\n",
    "        \"\"\"Query the RAG system with optional metadata filtering.\"\"\"\n",
    "        if filters:\n",
    "            # Apply metadata filters to the retriever\n",
    "            self.retriever.search_kwargs[\"filter\"] = filters\n",
    "        \n",
    "        # Get answer\n",
    "        result = self.qa_chain({\"query\": question})\n",
    "        \n",
    "        # Format response\n",
    "        sources = []\n",
    "        for doc in result.get(\"source_documents\", []):\n",
    "            # Add source information\n",
    "            source_info = {\n",
    "                \"content\": doc.page_content[:200] + \"...\" if len(doc.page_content) > 200 else doc.page_content,\n",
    "                \"metadata\": doc.metadata\n",
    "            }\n",
    "            sources.append(source_info)\n",
    "        \n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"answer\": result[\"result\"],\n",
    "            \"sources\": sources\n",
    "        }\n",
    "    \n",
    "    def query_recommendations(self, question: str, evidence_level: str = None) -> Dict[str, Any]:\n",
    "        \"\"\"Query specifically for medical recommendations with optional evidence filtering.\"\"\"\n",
    "        filters = {\"chunk_type\": \"recommendation\"}\n",
    "        \n",
    "        if evidence_level:\n",
    "            # Filter by evidence level (e.g., \"high\", \"moderate\", \"low\")\n",
    "            filters[\"evidence_level\"] = {\"$regex\": f\"{evidence_level.lower()}.*evidence\"}\n",
    "        \n",
    "        return self.query(question, filters)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "QUESTION:\n",
      "How does folic acid help during pregnancy?\n",
      "\n",
      "ANSWER:\n",
      "Folic acid supplementation during pregnancy is recommended to significantly reduce the incidence of neural tube defects in the fetus. According to the guidelines, a daily dose of 400 μg of folic acid taken orally is advised for all women in the reproductive age group for 2–3 months pre-conception and during the first 12 weeks of pregnancy (Antenatal Care and Skilled Attendance at Birth by ANMs/LHVs/SNsGUIDELINES). This recommendation is supported by the consensus statement from the Pregnancy Working Group, Geneva, 2015, which underscores the importance of folic acid supplementation during pregnancy to prevent neural tube defects in the developing fetus (Pregnancy Working Group. Consensus statement on folic acid supplementation during pregnancy. Geneva; 2015).\n",
      "\n",
      "SOURCES:\n",
      "\n",
      "1. \n",
      "   Section: Counselling\n",
      "   Type: text\n",
      "\n",
      "2. \n",
      "   Section: 18 Screening for Syphilis\n",
      "   Type: text\n",
      "\n",
      "3. antenatal care \n",
      "for a positive pregnancy experience\n",
      "\n",
      "WHO Library Cataloguing-in-Publication Data\n",
      "WHO recommendations on antenatal care for a positive pregnancy experience\n",
      "   Section: 9 Blood and Nutrition. In: British > Roll Back Malaria Partnership Malaria in\n",
      "   Type: text\n",
      "\n",
      "4. antenatal care \n",
      "for a positive pregnancy experience\n",
      "\n",
      "WHO Library Cataloguing-in-Publication Data\n",
      "WHO recommendations on antenatal care for a positive pregnancy experience\n",
      "   Section: 3 Evidence and recommendations\n",
      "   Type: text\n",
      "\n",
      "5. \n",
      "   Section: \n",
      "   Type: text\n",
      "\n",
      "6. antenatal care \n",
      "for a positive pregnancy experience\n",
      "\n",
      "WHO Library Cataloguing-in-Publication Data\n",
      "WHO recommendations on antenatal care for a positive pregnancy experience\n",
      "   Section: 4 Implementation of the ANC guideline and recommendations > Nutritional interventions\n",
      "   Type: text\n"
     ]
    }
   ],
   "source": [
    "rag = MedicalRAG(vector_store)\n",
    "    \n",
    "    # # Run query if provided\n",
    "    # if args.query:\n",
    "    #     if args.filter_recommendations:\n",
    "    #         result = rag.query_recommendations(args.query, args.evidence_level)\n",
    "    #     else:\n",
    "query=\"How does folic acid help during pregnancy?\"\n",
    "result = rag.query(query)\n",
    "        \n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"QUESTION:\")\n",
    "print(result[\"question\"])\n",
    "print(\"\\nANSWER:\")\n",
    "print(result[\"answer\"])\n",
    "print(\"\\nSOURCES:\")\n",
    "for i, source in enumerate(result[\"sources\"]):\n",
    "    print(f\"\\n{i+1}. {source['metadata'].get('document_title', 'Unknown document')}\")\n",
    "    if \"section_path\" in source[\"metadata\"]:\n",
    "        print(f\"   Section: {' > '.join(source['metadata']['section_path'])}\")\n",
    "    if \"chunk_type\" in source[\"metadata\"]:\n",
    "        print(f\"   Type: {source['metadata']['chunk_type']}\")\n",
    "    if \"evidence_level\" in source[\"metadata\"]:\n",
    "        print(f\"   Evidence level: {source['metadata']['evidence_level']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from typing import List, Dict, Any, Optional, Tuple, Union\n",
    "class MedicalRAG:\n",
    "    \"\"\"RAG system for medical documents with advanced filtering and retrieval.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        vector_store: FAISS,\n",
    "        model_name: str = \"gpt-4-turbo\",\n",
    "        temperature: float = 0.0,\n",
    "    ):\n",
    "        self.vector_store = vector_store\n",
    "        self.llm = ChatOpenAI(model_name=model_name, temperature=temperature)\n",
    "        \n",
    "        # Create RAG prompt template\n",
    "        self.rag_prompt = PromptTemplate(\n",
    "            template=\"\"\" You are a medical AI assistant. Answer the medical question based only on the following context.\n",
    "        If you don't know the answer based on the context, admit that you don't know rather than making up information.\n",
    "        Always maintain patient confidentiality and provide evidence-based answers when possible.\n",
    "        \n",
    "        Context:\n",
    "        {context}\n",
    "        \n",
    "        Medical Question:\n",
    "        {question}\n",
    "        \n",
    "        Answer:\n",
    "        \"\"\",\n",
    "            input_variables=[\"context\", \"question\"]\n",
    "        )\n",
    "        \n",
    "        # Setup retrieval chain\n",
    "        self.retriever = vector_store.as_retriever(\n",
    "            search_type=\"mmr\",\n",
    "            search_kwargs={\"k\": 6}\n",
    "        )\n",
    "        \n",
    "        self.qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm=self.llm,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=self.retriever,\n",
    "            return_source_documents=True,\n",
    "            chain_type_kwargs={\"prompt\": self.rag_prompt}\n",
    "        )\n",
    "    \n",
    "    def query(self, question: str, filters: Dict[str, Any] = None) -> Dict[str, Any]:\n",
    "        \"\"\"Query the RAG system with optional metadata filtering.\"\"\"\n",
    "        if filters:\n",
    "            # Apply metadata filters to the retriever\n",
    "            self.retriever.search_kwargs[\"filter\"] = filters\n",
    "        \n",
    "        # Get answer\n",
    "        result = self.qa_chain({\"query\": question})\n",
    "        \n",
    "        # Format response\n",
    "        sources = []\n",
    "        for doc in result.get(\"source_documents\", []):\n",
    "            # Add source information\n",
    "            source_info = {\n",
    "                \"content\": doc.page_content[:200] + \"...\" if len(doc.page_content) > 200 else doc.page_content,\n",
    "                \"metadata\": doc.metadata\n",
    "            }\n",
    "            sources.append(source_info)\n",
    "        \n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"answer\": result[\"result\"],\n",
    "            \"sources\": sources\n",
    "        }\n",
    "    \n",
    "    def query_recommendations(self, question: str, evidence_level: str = None) -> Dict[str, Any]:\n",
    "        \"\"\"Query specifically for medical recommendations with optional evidence filtering.\"\"\"\n",
    "        filters = {\"chunk_type\": \"recommendation\"}\n",
    "        \n",
    "        if evidence_level:\n",
    "            # Filter by evidence level (e.g., \"high\", \"moderate\", \"low\")\n",
    "            filters[\"evidence_level\"] = {\"$regex\": f\"{evidence_level.lower()}.*evidence\"}\n",
    "        \n",
    "        return self.query(question, filters)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "QUESTION:\n",
      "How does folic acid help during pregnancy?\n",
      "\n",
      "ANSWER:\n",
      "Folic acid is crucial during pregnancy as it significantly reduces the incidence of neural tube defects in the fetus. Neural tube defects are serious birth defects of the brain and spine, such as spina bifida and anencephaly. The recommended daily dose of folic acid for women in the reproductive age group is 400 μg taken orally. It is advised to start taking folic acid for 2–3 months pre-conception and continue during the first 12 weeks of pregnancy to achieve its protective effects against neural tube defects.\n",
      "\n",
      "SOURCES:\n",
      "\n",
      "1. \n",
      "   Section: Counselling\n",
      "   Type: text\n",
      "\n",
      "2. \n",
      "   Section: 18 Screening for Syphilis\n",
      "   Type: text\n",
      "\n",
      "3. antenatal care \n",
      "for a positive pregnancy experience\n",
      "\n",
      "WHO Library Cataloguing-in-Publication Data\n",
      "WHO recommendations on antenatal care for a positive pregnancy experience\n",
      "   Section: 9 Blood and Nutrition. In: British > Roll Back Malaria Partnership Malaria in\n",
      "   Type: text\n",
      "\n",
      "4. antenatal care \n",
      "for a positive pregnancy experience\n",
      "\n",
      "WHO Library Cataloguing-in-Publication Data\n",
      "WHO recommendations on antenatal care for a positive pregnancy experience\n",
      "   Section: 3 Evidence and recommendations\n",
      "   Type: text\n",
      "\n",
      "5. \n",
      "   Section: \n",
      "   Type: text\n",
      "\n",
      "6. antenatal care \n",
      "for a positive pregnancy experience\n",
      "\n",
      "WHO Library Cataloguing-in-Publication Data\n",
      "WHO recommendations on antenatal care for a positive pregnancy experience\n",
      "   Section: 4 Implementation of the ANC guideline and recommendations > Nutritional interventions\n",
      "   Type: text\n"
     ]
    }
   ],
   "source": [
    "rag = MedicalRAG(vector_store)\n",
    "    \n",
    "    # # Run query if provided\n",
    "    # if args.query:\n",
    "    #     if args.filter_recommendations:\n",
    "    #         result = rag.query_recommendations(args.query, args.evidence_level)\n",
    "    #     else:\n",
    "query=\"How does folic acid help during pregnancy?\"\n",
    "result = rag.query(query)\n",
    "        \n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"QUESTION:\")\n",
    "print(result[\"question\"])\n",
    "print(\"\\nANSWER:\")\n",
    "print(result[\"answer\"])\n",
    "print(\"\\nSOURCES:\")\n",
    "for i, source in enumerate(result[\"sources\"]):\n",
    "    print(f\"\\n{i+1}. {source['metadata'].get('document_title', 'Unknown document')}\")\n",
    "    if \"section_path\" in source[\"metadata\"]:\n",
    "        print(f\"   Section: {' > '.join(source['metadata']['section_path'])}\")\n",
    "    if \"chunk_type\" in source[\"metadata\"]:\n",
    "        print(f\"   Type: {source['metadata']['chunk_type']}\")\n",
    "    if \"evidence_level\" in source[\"metadata\"]:\n",
    "        print(f\"   Evidence level: {source['metadata']['evidence_level']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "QUESTION:\n",
      "2.5 months hogae h pregnancy ko but vomet ya kch ni hora mjhe bss upt positive h\n",
      "\n",
      "ANSWER:\n",
      "If you have a positive pregnancy test but are not experiencing typical pregnancy symptoms like nausea or vomiting, it is still likely that you are pregnant. Some women do not experience these symptoms, especially early in pregnancy. Each pregnancy is unique, and the presence or absence of certain symptoms does not necessarily indicate a problem.\n",
      "\n",
      "However, it is important to have regular antenatal check-ups to monitor the health of both you and your baby. Since you are around 2.5 months pregnant, you should schedule an appointment with your healthcare provider if you haven't already done so. During this visit, your healthcare provider can confirm the pregnancy through a clinical assessment and possibly an ultrasound, discuss what to expect in the coming months, and address any questions or concerns you might have.\n",
      "\n",
      "Remember, regular antenatal care is crucial for ensuring a healthy pregnancy and addressing any issues early on.\n",
      "\n",
      "SOURCES:\n",
      "\n",
      "1. \n",
      "   Section: Counselling\n",
      "   Type: text\n",
      "\n",
      "2. \n",
      "   Section: These cards are designed for individual counselling but may be used for group counselling if it is possible to form a homogenous group of\n",
      "   Type: text\n",
      "\n",
      "3. \n",
      "   Section: The presence of meconium\n",
      "   Type: text\n",
      "\n",
      "4. \n",
      "   Section: Confirm\n",
      "   Type: text\n",
      "\n",
      "5. \n",
      "   Section: Metronidazole\n",
      "   Type: text\n",
      "\n",
      "6. \n",
      "   Section: \n",
      "   Type: text\n"
     ]
    }
   ],
   "source": [
    "rag = MedicalRAG(vector_store)\n",
    "    \n",
    "    # # Run query if provided\n",
    "    # if args.query:\n",
    "    #     if args.filter_recommendations:\n",
    "    #         result = rag.query_recommendations(args.query, args.evidence_level)\n",
    "    #     else:\n",
    "query=\"2.5 months hogae h pregnancy ko but vomet ya kch ni hora mjhe bss upt positive h\"\n",
    "result = rag.query(query)\n",
    "        \n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"QUESTION:\")\n",
    "print(result[\"question\"])\n",
    "print(\"\\nANSWER:\")\n",
    "print(result[\"answer\"])\n",
    "print(\"\\nSOURCES:\")\n",
    "for i, source in enumerate(result[\"sources\"]):\n",
    "    print(f\"\\n{i+1}. {source['metadata'].get('document_title', 'Unknown document')}\")\n",
    "    if \"section_path\" in source[\"metadata\"]:\n",
    "        print(f\"   Section: {' > '.join(source['metadata']['section_path'])}\")\n",
    "    if \"chunk_type\" in source[\"metadata\"]:\n",
    "        print(f\"   Type: {source['metadata']['chunk_type']}\")\n",
    "    if \"evidence_level\" in source[\"metadata\"]:\n",
    "        print(f\"   Evidence level: {source['metadata']['evidence_level']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from typing import List, Dict, Any, Optional, Tuple, Union\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "class MedicalRAG:\n",
    "    \"\"\"RAG system for medical documents with advanced filtering and retrieval.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        vector_store: FAISS,\n",
    "        model_name: str =  \"/data/models/huggingface/meta-llama/Llama-3.2-3B-Instruct\",\n",
    "        temperature: float = 0.2,\n",
    "        model=model,\n",
    "        tokenizer=tokenizer\n",
    "\n",
    "    ):\n",
    "        self.vector_store = vector_store\n",
    "        \n",
    "        \n",
    "        # Create text generation pipeline\n",
    "        pipe = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            max_new_tokens=512,\n",
    "            temperature=temperature,  # Lower temperature for medical accuracy\n",
    "            top_p=0.95\n",
    "        )\n",
    "        self.device = \"cuda\"\n",
    "        # Create LangChain wrapper\n",
    "        self.llm = HuggingFacePipeline(pipeline=pipe)\n",
    "        \n",
    "        # Create RAG prompt template\n",
    "        self.template=\"\"\" You are a medical AI assistant. Answer the medical question based only on the following context.\n",
    "        If you don't know the answer based on the context, admit that you don't know rather than making up information.\n",
    "        Always maintain patient confidentiality and provide evidence-based answers when possible.\n",
    "        \n",
    "        Context:\n",
    "        {context}\n",
    "        \n",
    "        Medical Question:\n",
    "        {question}\n",
    "        \n",
    "        Answer:\n",
    "        \n",
    "        \"\"\"\n",
    "        self.rag_prompt = PromptTemplate(\n",
    "            template=self.template,\n",
    "            input_variables=[\"context\", \"question\"]\n",
    "        )\n",
    "        \n",
    "        # Setup retrieval chain\n",
    "        self.retriever = vector_store.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\"k\": 6}\n",
    "        )\n",
    "        \n",
    "        self.qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm=self.llm,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=self.retriever,\n",
    "            return_source_documents=True,\n",
    "            chain_type_kwargs={\"prompt\": self.rag_prompt}\n",
    "        )\n",
    "    \n",
    "    def query(self, question: str, filters: Dict[str, Any] = None) -> Dict[str, Any]:\n",
    "        \"\"\"Query the RAG system with optional metadata filtering.\"\"\"\n",
    "        if filters:\n",
    "            # Apply metadata filters to the retriever\n",
    "            self.retriever.search_kwargs[\"filter\"] = filters\n",
    "        \n",
    "        # Get answer\n",
    "        result = self.qa_chain({\"query\": question})\n",
    "        answer = result[\"result\"]\n",
    "\n",
    "    # Find \"Answer:\" and return everything after it\n",
    "        answer_start = answer.lower().find(\"answer:\")\n",
    "        if answer_start != -1:\n",
    "            answer = answer[answer_start:].strip()\n",
    "        # Format response\n",
    "        sources = []\n",
    "        for doc in result.get(\"source_documents\", []):\n",
    "            # Add source information\n",
    "            source_info = {\n",
    "                \"content\": doc.page_content[:200] + \"...\" if len(doc.page_content) > 200 else doc.page_content,\n",
    "                \"metadata\": doc.metadata\n",
    "            }\n",
    "            sources.append(source_info)\n",
    "        start=len(result[\"result\"])\n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"answer\": answer,\n",
    "            \"sources\": sources\n",
    "        }\n",
    "    \n",
    "    def query_recommendations(self, question: str, evidence_level: str = None) -> Dict[str, Any]:\n",
    "        \"\"\"Query specifically for medical recommendations with optional evidence filtering.\"\"\"\n",
    "        filters = {\"chunk_type\": \"recommendation\"}\n",
    "        \n",
    "        if evidence_level:\n",
    "            # Filter by evidence level (e.g., \"high\", \"moderate\", \"low\")\n",
    "            filters[\"evidence_level\"] = {\"$regex\": f\"{evidence_level.lower()}.*evidence\"}\n",
    "        \n",
    "        return self.query(question, filters)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MedicalRAG' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m rag \u001b[38;5;241m=\u001b[39m \u001b[43mMedicalRAG\u001b[49m(vector_store,model\u001b[38;5;241m=\u001b[39mmodel, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer)\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# # Run query if provided\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# if args.query:\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m#     if args.filter_recommendations:\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m#         result = rag.query_recommendations(args.query, args.evidence_level)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m#     else:\u001b[39;00m\n\u001b[1;32m      8\u001b[0m query\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mजयदा टाइम तक बैठने से चक्कर आना खड़ा होने से पेट दर्द होने लगता है\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MedicalRAG' is not defined"
     ]
    }
   ],
   "source": [
    "rag = MedicalRAG(vector_store,model=model, tokenizer=tokenizer)\n",
    "    \n",
    "    # # Run query if provided\n",
    "    # if args.query:\n",
    "    #     if args.filter_recommendations:\n",
    "    #         result = rag.query_recommendations(args.query, args.evidence_level)\n",
    "    #     else:\n",
    "query=\"जयदा टाइम तक बैठने से चक्कर आना खड़ा होने से पेट दर्द होने लगता है\"\n",
    "result = rag.query(query)\n",
    "        \n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"QUESTION:\")\n",
    "print(result[\"question\"])\n",
    "print(\"\\nANSWER:\")\n",
    "print(result[\"answer\"])\n",
    "print(\"\\nSOURCES:\")\n",
    "for i, source in enumerate(result[\"sources\"]):\n",
    "    print(f\"\\n{i+1}. {source['metadata'].get('document_title', 'Unknown document')}\")\n",
    "    if \"section_path\" in source[\"metadata\"]:\n",
    "        print(f\"   Section: {' > '.join(source['metadata']['section_path'])}\")\n",
    "    if \"chunk_type\" in source[\"metadata\"]:\n",
    "        print(f\"   Type: {source['metadata']['chunk_type']}\")\n",
    "    if \"evidence_level\" in source[\"metadata\"]:\n",
    "        print(f\"   Evidence level: {source['metadata']['evidence_level']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [01:12<00:00, 36.01s/it]\n"
     ]
    }
   ],
   "source": [
    "model_name =  \"/data/models/huggingface/meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name\n",
    "        ).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "QUESTION:\n",
      "How does folic acid help during pregnancy?\n",
      "\n",
      "ANSWER:\n",
      "Answer:\n",
      "        \n",
      "         Folic acid plays an essential role in preventing congenital anomalies such as spina bifida and anencephaly. It also helps reduce the risk of miscarriage and stillbirth. The recommended daily intake of folic acid during pregnancy is 400 μg, which can be obtained through oral supplements or fortified foods. Taking folic acid before conception and throughout the first 12 weeks of pregnancy significantly reduces the incidence of neural tube defects in the fetus. Therefore, all women of childbearing age should be advised to consume folic acid for 2-3 months prior to conception and continue taking it during the first 12 weeks of pregnancy.\n",
      "\n",
      "SOURCES:\n",
      "\n",
      "1. \n",
      "   Section: Counselling\n",
      "   Type: text\n",
      "\n",
      "2. \n",
      "   Section: 18 Screening for Syphilis\n",
      "   Type: text\n",
      "\n",
      "3. antenatal care \n",
      "for a positive pregnancy experience\n",
      "\n",
      "WHO Library Cataloguing-in-Publication Data\n",
      "WHO recommendations on antenatal care for a positive pregnancy experience\n",
      "   Section: 9 Blood and Nutrition. In: British > Roll Back Malaria Partnership Malaria in\n",
      "   Type: text\n",
      "\n",
      "4. antenatal care \n",
      "for a positive pregnancy experience\n",
      "\n",
      "WHO Library Cataloguing-in-Publication Data\n",
      "WHO recommendations on antenatal care for a positive pregnancy experience\n",
      "   Section: 3 Evidence and recommendations\n",
      "   Type: text\n",
      "\n",
      "5. \n",
      "   Section: \n",
      "   Type: text\n",
      "\n",
      "6. antenatal care \n",
      "for a positive pregnancy experience\n",
      "\n",
      "WHO Library Cataloguing-in-Publication Data\n",
      "WHO recommendations on antenatal care for a positive pregnancy experience\n",
      "   Section: 4 Implementation of the ANC guideline and recommendations > Nutritional interventions\n",
      "   Type: text\n"
     ]
    }
   ],
   "source": [
    "rag = MedicalRAG(vector_store)\n",
    "    \n",
    "    # # Run query if provided\n",
    "    # if args.query:\n",
    "    #     if args.filter_recommendations:\n",
    "    #         result = rag.query_recommendations(args.query, args.evidence_level)\n",
    "    #     else:\n",
    "query=\"How does folic acid help during pregnancy?\"\n",
    "result = rag.query(query)\n",
    "        \n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"QUESTION:\")\n",
    "print(result[\"question\"])\n",
    "print(\"\\nANSWER:\")\n",
    "print(result[\"answer\"])\n",
    "print(\"\\nSOURCES:\")\n",
    "for i, source in enumerate(result[\"sources\"]):\n",
    "    print(f\"\\n{i+1}. {source['metadata'].get('document_title', 'Unknown document')}\")\n",
    "    if \"section_path\" in source[\"metadata\"]:\n",
    "        print(f\"   Section: {' > '.join(source['metadata']['section_path'])}\")\n",
    "    if \"chunk_type\" in source[\"metadata\"]:\n",
    "        print(f\"   Type: {source['metadata']['chunk_type']}\")\n",
    "    if \"evidence_level\" in source[\"metadata\"]:\n",
    "        print(f\"   Evidence level: {source['metadata']['evidence_level']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "QUESTION:\n",
      "What is the remedy for nausea or vomiting in 1 month pregnancy?\n",
      "\n",
      "ANSWER:\n",
      "Answer:\n",
      "        \n",
      "         Based on the provided context, there is no specific recommended treatment mentioned for nausea or vomiting during the first month of pregnancy. However, the guidelines suggest that pharmacological treatments like doxylamine and metoclopramide should be reserved for those pregnant women experiencing distressing symptoms that are not relieved by non-pharmacological options, under the supervision of a medical doctor.\n",
      "\n",
      "The most commonly recommended non-pharmacological option for relieving nausea and vomiting in early pregnancy is ginger, although this is not explicitly mentioned in the provided text. It is essential to note that individual experiences and responses to different remedies can vary greatly, so it's crucial to consult a healthcare provider for personalized guidance.\n",
      "\n",
      "In general, the American College of Obstetricians and Gynecologists (ACOG) recommends the following non-pharmacological measures for managing nausea and vomiting in early pregnancy:\n",
      "\n",
      "* Keeping a food diary to track eating patterns and identify potential triggers\n",
      "* Eating smaller, more frequent meals throughout the day\n",
      "* Avoiding strong-smelling foods and spicy dishes\n",
      "* Staying hydrated by drinking plenty of water\n",
      "* Getting regular exercise, such as walking or prenatal yoga\n",
      "* Practicing relaxation techniques, such as deep breathing or meditation\n",
      "\n",
      "It's always best to consult a healthcare provider for personalized advice on managing nausea and vomiting during pregnancy. They can help determine the underlying cause of symptoms and recommend the most effective treatment plan.\n",
      "\n",
      "SOURCES:\n",
      "\n",
      "1. antenatal care \n",
      "for a positive pregnancy experience\n",
      "\n",
      "WHO Library Cataloguing-in-Publication Data\n",
      "WHO recommendations on antenatal care for a positive pregnancy experience\n",
      "   Section: 3 Evidence and recommendations > Interventions for common\n",
      "   Type: text\n",
      "\n",
      "2. \n",
      "   Section: These cards are designed for individual counselling but may be used for group counselling if it is possible to form a homogenous group of\n",
      "   Type: text\n",
      "\n",
      "3. \n",
      "   Section: Confirm\n",
      "   Type: text\n",
      "\n",
      "4. \n",
      "   Section: The presence of meconium\n",
      "   Type: text\n",
      "\n",
      "5. antenatal care \n",
      "for a positive pregnancy experience\n",
      "\n",
      "WHO Library Cataloguing-in-Publication Data\n",
      "WHO recommendations on antenatal care for a positive pregnancy experience\n",
      "   Section: 3 Evidence and recommendations\n",
      "   Type: text\n",
      "\n",
      "6. antenatal care \n",
      "for a positive pregnancy experience\n",
      "\n",
      "WHO Library Cataloguing-in-Publication Data\n",
      "WHO recommendations on antenatal care for a positive pregnancy experience\n",
      "   Section: 3 Evidence and recommendations\n",
      "   Type: text\n"
     ]
    }
   ],
   "source": [
    "rag = MedicalRAG(vector_store,model=model, tokenizer=tokenizer)\n",
    "    \n",
    "    # # Run query if provided\n",
    "    # if args.query:\n",
    "    #     if args.filter_recommendations:\n",
    "    #         result = rag.query_recommendations(args.query, args.evidence_level)\n",
    "    #     else:\n",
    "query=\"What is the remedy for nausea or vomiting in 1 month pregnancy?\"\n",
    "result = rag.query(query)\n",
    "        \n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"QUESTION:\")\n",
    "print(result[\"question\"])\n",
    "print(\"\\nANSWER:\")\n",
    "print(result[\"answer\"])\n",
    "print(\"\\nSOURCES:\")\n",
    "for i, source in enumerate(result[\"sources\"]):\n",
    "    print(f\"\\n{i+1}. {source['metadata'].get('document_title', 'Unknown document')}\")\n",
    "    if \"section_path\" in source[\"metadata\"]:\n",
    "        print(f\"   Section: {' > '.join(source['metadata']['section_path'])}\")\n",
    "    if \"chunk_type\" in source[\"metadata\"]:\n",
    "        print(f\"   Type: {source['metadata']['chunk_type']}\")\n",
    "    if \"evidence_level\" in source[\"metadata\"]:\n",
    "        print(f\"   Evidence level: {source['metadata']['evidence_level']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vidhij2/.local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-03-05 20:52:37.979456: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1741225957.993906 4005322 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1741225957.998167 4005322 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-05 20:52:38.014567: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Loading checkpoint shards:  11%|█         | 2/19 [00:07<01:02,  3.68s/it]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "model_name =  \"/data/models/huggingface/mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name\n",
    "        ).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "Total GPU Memory: 50.896961536 GB\n",
      "Memory Allocated: 0.0 GB\n",
      "Memory Reserved: 0.0 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"Total GPU Memory:\", torch.cuda.get_device_properties(0).total_memory / 1e9, \"GB\")\n",
    "print(\"Memory Allocated:\", torch.cuda.memory_allocated() / 1e9, \"GB\")\n",
    "print(\"Memory Reserved:\", torch.cuda.memory_reserved() / 1e9, \"GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Mar  5 20:41:30 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX 6000 Ada Gene...    On  |   00000000:01:00.0 Off |                  Off |\n",
      "| 30%   36C    P8             26W /  270W |       2MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA RTX 6000 Ada Gene...    On  |   00000000:23:00.0 Off |                  Off |\n",
      "| 30%   33C    P8             26W /  270W |       2MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/vidhij2/.local/lib/python3.9/site-packages (4.47.1)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.0 MB 3.2 MB/s eta 0:00:01|██████                          | 1.9 MB 3.2 MB/s eta 0:00:03\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /home/vidhij2/.local/lib/python3.9/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/vidhij2/.local/lib/python3.9/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /home/vidhij2/.local/lib/python3.9/site-packages (from transformers) (0.27.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/vidhij2/.local/lib/python3.9/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: filelock in /home/vidhij2/.local/lib/python3.9/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: requests in /home/vidhij2/.local/lib/python3.9/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/vidhij2/.local/lib/python3.9/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/vidhij2/.local/lib/python3.9/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/vidhij2/.local/lib/python3.9/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/vidhij2/.local/lib/python3.9/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/vidhij2/.local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/vidhij2/.local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/vidhij2/.local/lib/python3.9/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3.9/site-packages (from requests->transformers) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/vidhij2/.local/lib/python3.9/site-packages (from requests->transformers) (2024.12.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3.9/site-packages (from requests->transformers) (2.10)\n",
      "Installing collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.47.1\n",
      "    Uninstalling transformers-4.47.1:\n",
      "      Successfully uninstalled transformers-4.47.1\n",
      "\u001b[33m  WARNING: Value for scheme.platlib does not match. Please report this to <https://github.com/pypa/pip/issues/10151>\n",
      "  distutils: /home/vidhij2/.local/lib/python3.9/site-packages\n",
      "  sysconfig: /home/vidhij2/.local/lib64/python3.9/site-packages\u001b[0m\n",
      "\u001b[33m  WARNING: Additional context:\n",
      "  user = True\n",
      "  home = None\n",
      "  root = None\n",
      "  prefix = None\u001b[0m\n",
      "Successfully installed transformers-4.49.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 76.1 MB 33.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: accelerate in /home/vidhij2/.local/lib/python3.9/site-packages (1.3.0)\n",
      "Requirement already satisfied: transformers in /home/vidhij2/.local/lib/python3.9/site-packages (4.49.0)\n",
      "Requirement already satisfied: torch in /home/vidhij2/.local/lib/python3.9/site-packages (2.5.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/vidhij2/.local/lib/python3.9/site-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: pyyaml in /home/vidhij2/.local/lib/python3.9/site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib64/python3.9/site-packages (from accelerate) (6.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/vidhij2/.local/lib/python3.9/site-packages (from accelerate) (24.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/vidhij2/.local/lib/python3.9/site-packages (from accelerate) (0.4.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /home/vidhij2/.local/lib/python3.9/site-packages (from accelerate) (0.27.0)\n",
      "Requirement already satisfied: filelock in /home/vidhij2/.local/lib/python3.9/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: requests in /home/vidhij2/.local/lib/python3.9/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/vidhij2/.local/lib/python3.9/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/vidhij2/.local/lib/python3.9/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/vidhij2/.local/lib/python3.9/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/vidhij2/.local/lib/python3.9/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/vidhij2/.local/lib/python3.9/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/vidhij2/.local/lib/python3.9/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/vidhij2/.local/lib/python3.9/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/vidhij2/.local/lib/python3.9/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/vidhij2/.local/lib/python3.9/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: fsspec in /home/vidhij2/.local/lib/python3.9/site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/vidhij2/.local/lib/python3.9/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/vidhij2/.local/lib/python3.9/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/vidhij2/.local/lib/python3.9/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/vidhij2/.local/lib/python3.9/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/vidhij2/.local/lib/python3.9/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/vidhij2/.local/lib/python3.9/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/vidhij2/.local/lib/python3.9/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/vidhij2/.local/lib/python3.9/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/vidhij2/.local/lib/python3.9/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/vidhij2/.local/lib/python3.9/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/vidhij2/.local/lib/python3.9/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib64/python3.9/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3.9/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3.9/site-packages (from requests->transformers) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/vidhij2/.local/lib/python3.9/site-packages (from requests->transformers) (2024.12.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/vidhij2/.local/lib/python3.9/site-packages (from requests->transformers) (3.4.1)\n",
      "Installing collected packages: bitsandbytes\n",
      "\u001b[33m  WARNING: Value for scheme.platlib does not match. Please report this to <https://github.com/pypa/pip/issues/10151>\n",
      "  distutils: /home/vidhij2/.local/lib/python3.9/site-packages\n",
      "  sysconfig: /home/vidhij2/.local/lib64/python3.9/site-packages\u001b[0m\n",
      "\u001b[33m  WARNING: Additional context:\n",
      "  user = True\n",
      "  home = None\n",
      "  root = None\n",
      "  prefix = None\u001b[0m\n",
      "Successfully installed bitsandbytes-0.45.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install bitsandbytes accelerate transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vidhij2/.local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-03-05 20:56:32.504016: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1741226192.518052 4009762 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1741226192.522254 4009762 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-05 20:56:32.538413: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Loading checkpoint shards: 100%|██████████| 19/19 [09:10<00:00, 28.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Mixtral loaded successfully in 8-bit mode.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "model_name = \"/data/models/huggingface/mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "\n",
    "# Enable 8-bit quantization to reduce VRAM usage\n",
    "bnb_config = BitsAndBytesConfig(load_in_8bit=True)  # Use load_in_4bit=True if needed\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load model in optimized mode\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(\"✅ Mixtral loaded successfully in 8-bit mode.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "/home/vidhij2/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/vidhij2/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "QUESTION:\n",
      "2.5 months hogae h pregnancy ko but vomet ya kch ni hora mjhe bss upt positive h\n",
      "\n",
      "ANSWER:\n",
      "Answer:\n",
      "        \n",
      "        2.5 months ho gai hogae h pregnancy ko par vomiting ya kuch nahi hota hai, lekin agar\n",
      "        vomiting ho raha hai to doctor se consult kar lijiye. Urine mein Human Chorionic\n",
      "        Gonadotrophin (hCG) hormone detect ki ja sakti hai jo pratyek maheene ke doosre din se shuru\n",
      "        hota hai aur apni madad kar sakta hai pregnancy test kit ka istemaal kar pregnancy confirm\n",
      "        karne ke liye.\n",
      "\n",
      "SOURCES:\n",
      "\n",
      "1. \n",
      "   Section: Counselling\n",
      "   Type: text\n",
      "\n",
      "2. \n",
      "   Section: These cards are designed for individual counselling but may be used for group counselling if it is possible to form a homogenous group of\n",
      "   Type: text\n",
      "\n",
      "3. \n",
      "   Section: 8 Schedule for MMA Trainings > MMA is a safe and effective method to terminate early\n",
      "   Type: text\n",
      "\n",
      "4. \n",
      "   Section: The presence of meconium\n",
      "   Type: text\n",
      "\n",
      "5. \n",
      "   Section: Confirm\n",
      "   Type: text\n",
      "\n",
      "6. \n",
      "   Section: Metronidazole\n",
      "   Type: text\n"
     ]
    }
   ],
   "source": [
    "rag = MedicalRAG(vector_store,model=model, tokenizer=tokenizer)\n",
    "    \n",
    "query=\"2.5 months hogae h pregnancy ko but vomet ya kch ni hora mjhe bss upt positive h\"\n",
    "result = rag.query(query)\n",
    "        \n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"QUESTION:\")\n",
    "print(result[\"question\"])\n",
    "print(\"\\nANSWER:\")\n",
    "print(result[\"answer\"])\n",
    "print(\"\\nSOURCES:\")\n",
    "for i, source in enumerate(result[\"sources\"]):\n",
    "    print(f\"\\n{i+1}. {source['metadata'].get('document_title', 'Unknown document')}\")\n",
    "    if \"section_path\" in source[\"metadata\"]:\n",
    "        print(f\"   Section: {' > '.join(source['metadata']['section_path'])}\")\n",
    "    if \"chunk_type\" in source[\"metadata\"]:\n",
    "        print(f\"   Type: {source['metadata']['chunk_type']}\")\n",
    "    if \"evidence_level\" in source[\"metadata\"]:\n",
    "        print(f\"   Evidence level: {source['metadata']['evidence_level']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vidhij2/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/vidhij2/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "QUESTION:\n",
      "What is the remedy for nausea or vomiting in 1 month pregnancy?\n",
      "\n",
      "ANSWER:\n",
      "Answer:\n",
      "        \n",
      "        \n",
      "        Based on the provided context, for the relief of nausea in early pregnancy, non-pharmacological options are recommended. These options include eating three main meals and one nutritious snack, including calcium-rich foods like milk, curd, paneer, ragi, sesame seeds, bathua leaves, methi leaves, etc., and continuing to consume one Folic Acid tablet (400 mg) daily and dietary sources of folate such as catla fish, moth beans, kidney beans, soybean, spinach, sem ki phali, sarson ka saag, chicken liver. Lemon oil, mint oil, and chamomile have low-certainty evidence suggesting they may make little or no difference in reducing nausea and vomiting symptoms. However, moderate-certainty evidence shows that vitamin B6 (pyridoxine) probably reduces nausea symptoms scores during pregnancy. It is essential to consult with a healthcare provider before starting any new treatment regimen.\n",
      "\n",
      "SOURCES:\n",
      "\n",
      "1. antenatal care \n",
      "for a positive pregnancy experience\n",
      "\n",
      "WHO Library Cataloguing-in-Publication Data\n",
      "WHO recommendations on antenatal care for a positive pregnancy experience\n",
      "   Section: 3 Evidence and recommendations > Interventions for common\n",
      "   Type: text\n",
      "\n",
      "2. \n",
      "   Section: These cards are designed for individual counselling but may be used for group counselling if it is possible to form a homogenous group of\n",
      "   Type: text\n",
      "\n",
      "3. \n",
      "   Section: Confirm\n",
      "   Type: text\n",
      "\n",
      "4. \n",
      "   Section: The presence of meconium\n",
      "   Type: text\n",
      "\n",
      "5. antenatal care \n",
      "for a positive pregnancy experience\n",
      "\n",
      "WHO Library Cataloguing-in-Publication Data\n",
      "WHO recommendations on antenatal care for a positive pregnancy experience\n",
      "   Section: 3 Evidence and recommendations\n",
      "   Type: text\n",
      "\n",
      "6. antenatal care \n",
      "for a positive pregnancy experience\n",
      "\n",
      "WHO Library Cataloguing-in-Publication Data\n",
      "WHO recommendations on antenatal care for a positive pregnancy experience\n",
      "   Section: 3 Evidence and recommendations\n",
      "   Type: text\n"
     ]
    }
   ],
   "source": [
    "query=\"What is the remedy for nausea or vomiting in 1 month pregnancy?\"\n",
    "result = rag.query(query)\n",
    "        \n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"QUESTION:\")\n",
    "print(result[\"question\"])\n",
    "print(\"\\nANSWER:\")\n",
    "print(result[\"answer\"])\n",
    "print(\"\\nSOURCES:\")\n",
    "for i, source in enumerate(result[\"sources\"]):\n",
    "    print(f\"\\n{i+1}. {source['metadata'].get('document_title', 'Unknown document')}\")\n",
    "    if \"section_path\" in source[\"metadata\"]:\n",
    "        print(f\"   Section: {' > '.join(source['metadata']['section_path'])}\")\n",
    "    if \"chunk_type\" in source[\"metadata\"]:\n",
    "        print(f\"   Type: {source['metadata']['chunk_type']}\")\n",
    "    if \"evidence_level\" in source[\"metadata\"]:\n",
    "        print(f\"   Evidence level: {source['metadata']['evidence_level']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vidhij2/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/vidhij2/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "QUESTION:\n",
      "How does folic acid help during pregnancy?\n",
      "\n",
      "ANSWER:\n",
      "Answer:\n",
      "        \n",
      "        \n",
      "        Folic acid helps reduce the incidence of neural tube defects in the fetus when taken before conception and during the first 12 weeks of pregnancy. The recommended daily dose is 400 μg taken orally.\n",
      "\n",
      "SOURCES:\n",
      "\n",
      "1. \n",
      "   Section: Counselling\n",
      "   Type: text\n",
      "\n",
      "2. \n",
      "   Section: 18 Screening for Syphilis\n",
      "   Type: text\n",
      "\n",
      "3. antenatal care \n",
      "for a positive pregnancy experience\n",
      "\n",
      "WHO Library Cataloguing-in-Publication Data\n",
      "WHO recommendations on antenatal care for a positive pregnancy experience\n",
      "   Section: 9 Blood and Nutrition. In: British > Roll Back Malaria Partnership Malaria in\n",
      "   Type: text\n",
      "\n",
      "4. antenatal care \n",
      "for a positive pregnancy experience\n",
      "\n",
      "WHO Library Cataloguing-in-Publication Data\n",
      "WHO recommendations on antenatal care for a positive pregnancy experience\n",
      "   Section: 3 Evidence and recommendations\n",
      "   Type: text\n",
      "\n",
      "5. \n",
      "   Section: \n",
      "   Type: text\n",
      "\n",
      "6. antenatal care \n",
      "for a positive pregnancy experience\n",
      "\n",
      "WHO Library Cataloguing-in-Publication Data\n",
      "WHO recommendations on antenatal care for a positive pregnancy experience\n",
      "   Section: 4 Implementation of the ANC guideline and recommendations > Nutritional interventions\n",
      "   Type: text\n"
     ]
    }
   ],
   "source": [
    "query=\"How does folic acid help during pregnancy?\"\n",
    "result = rag.query(query)\n",
    "        \n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"QUESTION:\")\n",
    "print(result[\"question\"])\n",
    "print(\"\\nANSWER:\")\n",
    "print(result[\"answer\"])\n",
    "print(\"\\nSOURCES:\")\n",
    "for i, source in enumerate(result[\"sources\"]):\n",
    "    print(f\"\\n{i+1}. {source['metadata'].get('document_title', 'Unknown document')}\")\n",
    "    if \"section_path\" in source[\"metadata\"]:\n",
    "        print(f\"   Section: {' > '.join(source['metadata']['section_path'])}\")\n",
    "    if \"chunk_type\" in source[\"metadata\"]:\n",
    "        print(f\"   Type: {source['metadata']['chunk_type']}\")\n",
    "    if \"evidence_level\" in source[\"metadata\"]:\n",
    "        print(f\"   Evidence level: {source['metadata']['evidence_level']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vidhij2/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/vidhij2/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "QUESTION:\n",
      "what should be my diet and what precautions should I keep?\n",
      "\n",
      "ANSWER:\n",
      "Answer:\n",
      "        \n",
      "        \n",
      "        Based on the provided context, here are some recommendations for your diet and precautions you should keep:\n",
      "\n",
      "        Diet:\n",
      "        - Your diet should consist of a balanced plate where half of it is filled with various vegetables like beans, tomatoes, celery, cabbage, mushrooms, etc. These vegetables provide fiber which helps in controlling postprandial glucose level.\n",
      "        - Another quarter of your plate should contain protein-rich food such as dal, soy nuggets, tofu, eggs, paneer, chicken, or fish.\n",
      "        - The last quarter of your plate should include carbohydrates like chapati, brown rice, bread, or cereals.\n",
      "        - Ensure you have at least one serving of low-fat, sugar-free yogurt, curd, or milk daily.\n",
      "        - Include at least one serving of fruits like guava, apple, berries, or any citrus fruits in your diet.\n",
      "        - Try to limit your carbohydrate servings in lunch and dinner to 2-3. Avoid consuming heavy meals.\n",
      "        - Make sure not to skip lunch and dinner and take meals at regular intervals.\n",
      "        - As a mid-day snack, consume 2-3 healthy options, preferably with one or two carbohydrate servings.\n",
      "\n",
      "        Precautions:\n",
      "        - Reduce your salt intake to less than 6 grams per day (around one teaspoon full).\n",
      "        - Decrease consumption of caffeine products like tea, coffee, etc.\n",
      "        - Engage in regular physical activities such as yoga or walking, ideally around 30 minutes, twice a day.\n",
      "        - Follow proper food safety guidelines, including washing hands with soap before and after handling food and after using the toilet, coughing, or sneezing.\n",
      "        - Use potable or clean drinking water for food preparation.\n",
      "        - Keep kitchen equipment and containers clean, dry, and free from mold and fungi.\n",
      "        - Store double fortified salt in an airtight container, away from heat and humidity.\n",
      "        - Register your name under government schemes such as Anganwadi services, JSSK, JSY, PMMVY, PMSMA, PDS, or NRLM for proper nutrition during pregnancy and after delivery.\n",
      "\n",
      "SOURCES:\n",
      "\n",
      "1. \n",
      "   Section: Special considerations for high-risk pregnancies:\n",
      "   Type: text\n",
      "\n",
      "2. \n",
      "   Section: Hypertension:\n",
      "   Type: text\n",
      "\n",
      "3. \n",
      "   Section: High risk pregnancies: A high risk pregnancy is the one where the\n",
      "   Type: text\n",
      "\n",
      "4. \n",
      "   Section: 6 Recording and Reporting for > Viewing test results in meter memory\n",
      "   Type: text\n",
      "\n",
      "5. \n",
      "   Section: These cards are designed for individual counselling but may be used for group counselling if it is possible to form a homogenous group of\n",
      "   Type: text\n",
      "\n",
      "6. antenatal care \n",
      "for a positive pregnancy experience\n",
      "\n",
      "WHO Library Cataloguing-in-Publication Data\n",
      "WHO recommendations on antenatal care for a positive pregnancy experience\n",
      "   Section: 3 Evidence and recommendations\n",
      "   Type: text\n"
     ]
    }
   ],
   "source": [
    "query=\"what should be my diet and what precautions should I keep?\"\n",
    "result = rag.query(query)\n",
    "        \n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"QUESTION:\")\n",
    "print(result[\"question\"])\n",
    "print(\"\\nANSWER:\")\n",
    "print(result[\"answer\"])\n",
    "print(\"\\nSOURCES:\")\n",
    "for i, source in enumerate(result[\"sources\"]):\n",
    "    print(f\"\\n{i+1}. {source['metadata'].get('document_title', 'Unknown document')}\")\n",
    "    if \"section_path\" in source[\"metadata\"]:\n",
    "        print(f\"   Section: {' > '.join(source['metadata']['section_path'])}\")\n",
    "    if \"chunk_type\" in source[\"metadata\"]:\n",
    "        print(f\"   Type: {source['metadata']['chunk_type']}\")\n",
    "    if \"evidence_level\" in source[\"metadata\"]:\n",
    "        print(f\"   Evidence level: {source['metadata']['evidence_level']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vidhij2/.local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-03-06 01:06:52.572425: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1741241212.587841 4088178 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1741241212.592333 4088178 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-06 01:06:52.609327: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [01:43<00:00, 51.91s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "model_name =  \"/data/models/huggingface/qwen/Qwen1.5-4B-Chat\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name\n",
    "        ).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vidhij2/.local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-03-06 00:04:29.005186: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1741237469.028514 4077900 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1741237469.035355 4077900 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-06 00:04:29.062210: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "model_name =  \"/data/models/huggingface/tiiuae/falcon-7b\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name\n",
    "        ).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('unable to open database file')).History will not be written to the database.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 122] Disk quota exceeded",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmedalpaca/medalpaca-7b\u001b[39m\u001b[38;5;124m\"\u001b[39m, legacy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmedalpaca/medalpaca-7b\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/modeling_utils.py:262\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 262\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    264\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/modeling_utils.py:4022\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4019\u001b[0m \u001b[38;5;66;03m# We'll need to download and cache each checkpoint shard if the checkpoint is sharded.\u001b[39;00m\n\u001b[1;32m   4020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_sharded:\n\u001b[1;32m   4021\u001b[0m     \u001b[38;5;66;03m# resolved_archive_file becomes a list of files that point to the different checkpoint shards in this case.\u001b[39;00m\n\u001b[0;32m-> 4022\u001b[0m     resolved_archive_file, sharded_metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_checkpoint_shard_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4024\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4025\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4026\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4027\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4028\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4029\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4030\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4031\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4032\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4033\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4034\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4035\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4037\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   4038\u001b[0m     is_safetensors_available()\n\u001b[1;32m   4039\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resolved_archive_file, \u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m   4040\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m resolved_archive_file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.safetensors\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   4041\u001b[0m ):\n\u001b[1;32m   4042\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m safe_open(resolved_archive_file, framework\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/utils/hub.py:1037\u001b[0m, in \u001b[0;36mget_checkpoint_shard_files\u001b[0;34m(pretrained_model_name_or_path, index_filename, cache_dir, force_download, proxies, resume_download, local_files_only, token, user_agent, revision, subfolder, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m shard_filename \u001b[38;5;129;01min\u001b[39;00m tqdm(shard_filenames, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading shards\u001b[39m\u001b[38;5;124m\"\u001b[39m, disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m show_progress_bar):\n\u001b[1;32m   1035\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1036\u001b[0m         \u001b[38;5;66;03m# Load from URL\u001b[39;00m\n\u001b[0;32m-> 1037\u001b[0m         cached_filename \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1038\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1039\u001b[0m \u001b[43m            \u001b[49m\u001b[43mshard_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1040\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1041\u001b[0m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1043\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1044\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1045\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1046\u001b[0m \u001b[43m            \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1049\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_commit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1051\u001b[0m     \u001b[38;5;66;03m# We have already dealt with RepositoryNotFoundError and RevisionNotFoundError when getting the index, so\u001b[39;00m\n\u001b[1;32m   1052\u001b[0m     \u001b[38;5;66;03m# we don't have to catch them here.\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/utils/hub.py:342\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    339\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 342\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    357\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m _get_cache_file_to_return(path_or_repo_id, full_filename, cache_dir, revision)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/huggingface_hub/file_download.py:860\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[1;32m    841\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[1;32m    842\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    857\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m    858\u001b[0m     )\n\u001b[1;32m    859\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 860\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m    864\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m    875\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/huggingface_hub/file_download.py:1008\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1005\u001b[0m     blob_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m?\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(blob_path)\n\u001b[1;32m   1007\u001b[0m Path(lock_path)\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m-> 1008\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[1;32m   1009\u001b[0m     _download_to_tmp_and_move(\n\u001b[1;32m   1010\u001b[0m         incomplete_path\u001b[38;5;241m=\u001b[39mPath(blob_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.incomplete\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1011\u001b[0m         destination_path\u001b[38;5;241m=\u001b[39mPath(blob_path),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1017\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[1;32m   1018\u001b[0m     )\n\u001b[1;32m   1019\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(pointer_path):\n",
      "File \u001b[0;32m/usr/lib64/python3.9/contextlib.py:119\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/huggingface_hub/utils/_fixes.py:98\u001b[0m, in \u001b[0;36mWeakFileLock\u001b[0;34m(lock_file)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 98\u001b[0m         \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Timeout:\n\u001b[1;32m    100\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstill waiting to acquire lock on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, lock_file)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/filelock/_api.py:332\u001b[0m, in \u001b[0;36mBaseFileLock.acquire\u001b[0;34m(self, timeout, poll_interval, poll_intervall, blocking)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_locked:\n\u001b[1;32m    331\u001b[0m     _LOGGER\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to acquire lock \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, lock_id, lock_filename)\n\u001b[0;32m--> 332\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_acquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_locked:\n\u001b[1;32m    334\u001b[0m     _LOGGER\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLock \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m acquired on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, lock_id, lock_filename)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/filelock/_unix.py:44\u001b[0m, in \u001b[0;36mUnixFileLock._acquire\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     42\u001b[0m fd \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlock_file, open_flags, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_context\u001b[38;5;241m.\u001b[39mmode)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m suppress(\u001b[38;5;167;01mPermissionError\u001b[39;00m):  \u001b[38;5;66;03m# This locked is not owned by this UID\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfchmod\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m     fcntl\u001b[38;5;241m.\u001b[39mflock(fd, fcntl\u001b[38;5;241m.\u001b[39mLOCK_EX \u001b[38;5;241m|\u001b[39m fcntl\u001b[38;5;241m.\u001b[39mLOCK_NB)\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 122] Disk quota exceeded"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"medalpaca/medalpaca-7b\", legacy=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "            \"medalpaca/medalpaca-7b\"\n",
    "        ).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/vidhij2/.local/lib/python3.9/site-packages (4.49.0)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 4.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tiktoken in /home/vidhij2/.local/lib/python3.9/site-packages (0.9.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/vidhij2/.local/lib/python3.9/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: requests in /home/vidhij2/.local/lib/python3.9/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /home/vidhij2/.local/lib/python3.9/site-packages (from transformers) (0.27.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/vidhij2/.local/lib/python3.9/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/vidhij2/.local/lib/python3.9/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/vidhij2/.local/lib/python3.9/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/vidhij2/.local/lib/python3.9/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/vidhij2/.local/lib/python3.9/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/vidhij2/.local/lib/python3.9/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: filelock in /home/vidhij2/.local/lib/python3.9/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/vidhij2/.local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/vidhij2/.local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.9.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/vidhij2/.local/lib/python3.9/site-packages (from requests->transformers) (2024.12.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3.9/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3.9/site-packages (from requests->transformers) (1.26.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/vidhij2/.local/lib/python3.9/site-packages (from requests->transformers) (3.4.1)\n",
      "Installing collected packages: sentencepiece\n",
      "\u001b[33m  WARNING: Value for scheme.platlib does not match. Please report this to <https://github.com/pypa/pip/issues/10151>\n",
      "  distutils: /home/vidhij2/.local/lib/python3.9/site-packages\n",
      "  sysconfig: /home/vidhij2/.local/lib64/python3.9/site-packages\u001b[0m\n",
      "\u001b[33m  WARNING: Additional context:\n",
      "  user = True\n",
      "  home = None\n",
      "  root = None\n",
      "  prefix = None\u001b[0m\n",
      "Successfully installed sentencepiece-0.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade transformers sentencepiece tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sentencepiece in /home/vidhij2/.local/lib/python3.9/site-packages (0.2.0)\n",
      "\u001b[33mWARNING: Value for scheme.platlib does not match. Please report this to <https://github.com/pypa/pip/issues/10151>\n",
      "distutils: /home/vidhij2/.local/lib/python3.9/site-packages\n",
      "sysconfig: /home/vidhij2/.local/lib64/python3.9/site-packages\u001b[0m\n",
      "\u001b[33mWARNING: Additional context:\n",
      "user = True\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/vidhij2/.local/lib/python3.9/site-packages (4.49.0)\n",
      "Requirement already satisfied: torch in /home/vidhij2/.local/lib/python3.9/site-packages (2.5.1)\n",
      "Requirement already satisfied: accelerate in /home/vidhij2/.local/lib/python3.9/site-packages (1.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/vidhij2/.local/lib/python3.9/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/vidhij2/.local/lib/python3.9/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/vidhij2/.local/lib/python3.9/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /home/vidhij2/.local/lib/python3.9/site-packages (from transformers) (0.27.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/vidhij2/.local/lib/python3.9/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: requests in /home/vidhij2/.local/lib/python3.9/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/vidhij2/.local/lib/python3.9/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/vidhij2/.local/lib/python3.9/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: filelock in /home/vidhij2/.local/lib/python3.9/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/vidhij2/.local/lib/python3.9/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/vidhij2/.local/lib/python3.9/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/vidhij2/.local/lib/python3.9/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/vidhij2/.local/lib/python3.9/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/vidhij2/.local/lib/python3.9/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: networkx in /home/vidhij2/.local/lib/python3.9/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/vidhij2/.local/lib/python3.9/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/vidhij2/.local/lib/python3.9/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/vidhij2/.local/lib/python3.9/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/vidhij2/.local/lib/python3.9/site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/vidhij2/.local/lib/python3.9/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/vidhij2/.local/lib/python3.9/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/vidhij2/.local/lib/python3.9/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/vidhij2/.local/lib/python3.9/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/vidhij2/.local/lib/python3.9/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/vidhij2/.local/lib/python3.9/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/vidhij2/.local/lib/python3.9/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/vidhij2/.local/lib/python3.9/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/vidhij2/.local/lib/python3.9/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib64/python3.9/site-packages (from accelerate) (6.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib64/python3.9/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3.9/site-packages (from requests->transformers) (1.26.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/vidhij2/.local/lib/python3.9/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3.9/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/vidhij2/.local/lib/python3.9/site-packages (from requests->transformers) (2024.12.14)\n",
      "\u001b[33mWARNING: Value for scheme.platlib does not match. Please report this to <https://github.com/pypa/pip/issues/10151>\n",
      "distutils: /home/vidhij2/.local/lib/python3.9/site-packages\n",
      "sysconfig: /home/vidhij2/.local/lib64/python3.9/site-packages\u001b[0m\n",
      "\u001b[33mWARNING: Additional context:\n",
      "user = True\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers torch accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "QUESTION:\n",
      "2.5 months hogae h pregnancy ko but vomet ya kch ni hora mjhe bss upt positive h\n",
      "\n",
      "ANSWER:\n",
      "Answer:\n",
      "\n",
      "SOURCES:\n",
      "\n",
      "1. \n",
      "   Section: Counselling\n",
      "   Type: text\n",
      "\n",
      "2. \n",
      "   Section: These cards are designed for individual counselling but may be used for group counselling if it is possible to form a homogenous group of\n",
      "   Type: text\n",
      "\n",
      "3. \n",
      "   Section: Confirm\n",
      "   Type: text\n",
      "\n",
      "4. \n",
      "   Section: Confirm\n",
      "   Type: text\n",
      "\n",
      "5. \n",
      "   Section: Metronidazole\n",
      "   Type: text\n",
      "\n",
      "6. \n",
      "   Section: These cards are designed for individual counselling but may be used for group counselling if it is possible to form a homogenous group of\n",
      "   Type: text\n"
     ]
    }
   ],
   "source": [
    "rag = MedicalRAG(vector_store,model=model, tokenizer=tokenizer)\n",
    "    \n",
    "query=\"2.5 months hogae h pregnancy ko but vomet ya kch ni hora mjhe bss upt positive h\"\n",
    "result = rag.query(query)\n",
    "        \n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"QUESTION:\")\n",
    "print(result[\"question\"])\n",
    "print(\"\\nANSWER:\")\n",
    "print(result[\"answer\"])\n",
    "print(\"\\nSOURCES:\")\n",
    "for i, source in enumerate(result[\"sources\"]):\n",
    "    print(f\"\\n{i+1}. {source['metadata'].get('document_title', 'Unknown document')}\")\n",
    "    if \"section_path\" in source[\"metadata\"]:\n",
    "        print(f\"   Section: {' > '.join(source['metadata']['section_path'])}\")\n",
    "    if \"chunk_type\" in source[\"metadata\"]:\n",
    "        print(f\"   Type: {source['metadata']['chunk_type']}\")\n",
    "    if \"evidence_level\" in source[\"metadata\"]:\n",
    "        print(f\"   Evidence level: {source['metadata']['evidence_level']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "QUESTION:\n",
      "How does folic acid help during pregnancy?\n",
      "\n",
      "ANSWER:\n",
      "Answer:\n",
      "\n",
      "SOURCES:\n",
      "\n",
      "1. \n",
      "   Section: Counselling\n",
      "   Type: text\n",
      "\n",
      "2. antenatal care \n",
      "for a positive pregnancy experience\n",
      "\n",
      "WHO Library Cataloguing-in-Publication Data\n",
      "WHO recommendations on antenatal care for a positive pregnancy experience\n",
      "   Section: 3 Evidence and recommendations\n",
      "   Type: text\n",
      "\n",
      "3. \n",
      "   Section: These cards are designed for individual counselling but may be used for group counselling if it is possible to form a homogenous group of\n",
      "   Type: text\n",
      "\n",
      "4. antenatal care \n",
      "for a positive pregnancy experience\n",
      "\n",
      "WHO Library Cataloguing-in-Publication Data\n",
      "WHO recommendations on antenatal care for a positive pregnancy experience\n",
      "   Section: 9 Blood and Nutrition. In: British > Roll Back Malaria Partnership Malaria in\n",
      "   Type: text\n",
      "\n",
      "5. antenatal care \n",
      "for a positive pregnancy experience\n",
      "\n",
      "WHO Library Cataloguing-in-Publication Data\n",
      "WHO recommendations on antenatal care for a positive pregnancy experience\n",
      "   Section: 4 Implementation of the ANC guideline and recommendations\n",
      "   Type: text\n",
      "\n",
      "6. \n",
      "   Section: 18 Screening for Syphilis\n",
      "   Type: text\n"
     ]
    }
   ],
   "source": [
    "query=\"How does folic acid help during pregnancy?\"\n",
    "result = rag.query(query)\n",
    "        \n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"QUESTION:\")\n",
    "print(result[\"question\"])\n",
    "print(\"\\nANSWER:\")\n",
    "print(result[\"answer\"])\n",
    "print(\"\\nSOURCES:\")\n",
    "for i, source in enumerate(result[\"sources\"]):\n",
    "    print(f\"\\n{i+1}. {source['metadata'].get('document_title', 'Unknown document')}\")\n",
    "    if \"section_path\" in source[\"metadata\"]:\n",
    "        print(f\"   Section: {' > '.join(source['metadata']['section_path'])}\")\n",
    "    if \"chunk_type\" in source[\"metadata\"]:\n",
    "        print(f\"   Type: {source['metadata']['chunk_type']}\")\n",
    "    if \"evidence_level\" in source[\"metadata\"]:\n",
    "        print(f\"   Evidence level: {source['metadata']['evidence_level']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
